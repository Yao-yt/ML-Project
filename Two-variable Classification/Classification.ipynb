{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80060c4c-f5d0-467f-a392-7aebf1644bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d046c98b-f2d2-4682-a62a-6f8940449dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = np.loadtxt(\"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\实验三\\\\data1.txt\",delimiter=',')\n",
    "features = data.shape[1] - 1  # the number of features\n",
    "X = data[:, 0:features]\n",
    "y = data[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3ac220-f4fe-4d1b-97ff-991e5ca5035c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot(X,y):\n",
    "    pos = np.where(y==1)\n",
    "    neg = np.where(y==0)\n",
    "    plt.scatter(X[pos[0],0],X[pos[0],1],marker='x')\n",
    "    plt.scatter(X[neg[0], 0], X[neg[0], 1], marker='o')\n",
    "    plt.xlabel('Exam 1 score')\n",
    "    plt.ylabel('Exam 2 score')\n",
    "plt.show()\n",
    "plot(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769d700a-6366-4413-9050-1fb7616b90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    r = 1/(1+np.exp(-z))\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f428a2-47fb-4edb-b829-4aa81f75bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X,theta):\n",
    "    z= np.dot(X,theta)\n",
    "    return sigmoid(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a83494-70b5-4eb8-a8e0-7c21c5f3f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X,y,theta):\n",
    "    m = X.shape[0]  # the number of the samples\n",
    "    \n",
    "    result = -np.sum(y *  np.log(hypothesis(X, theta)) + (1 - y) * np.log(1 - (hypothesis(X, theta))))/m\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f19e0ec-7095-4539-854c-a5443a1a5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,iterations,alpha):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    theta_temp = theta\n",
    "    loss = np.zeros(iterations)   #initialize the loss array\n",
    "    for i in range(iterations):\n",
    "        for j in range(n):\n",
    "            theta_temp[j] = theta_temp[j] - alpha * np.sum(((hypothesis(X, theta))- y)*X[:,j].reshape(-1,1))\n",
    "        theta = theta_temp\n",
    "        #print the loss every 100 iterations\n",
    "        loss[i] = computeCost(X,y,theta)\n",
    "        if(i%100==0):\n",
    "            print('the iterations of ',i, 'the loss is',loss[i],'theta=',theta)\n",
    "    return theta,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f2623-75d0-46a3-8c81-68785612eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the iterations of  0 the loss is 0.6300104234070943 theta= [[ 0.0001    ]\n",
      " [ 0.01200758]\n",
      " [-0.000991  ]]\n",
      "the iterations of  100 the loss is 0.6293347267067181 theta= [[-0.00689228]\n",
      " [ 0.01047628]\n",
      " [ 0.00050271]]\n",
      "the iterations of  200 the loss is 0.6288482385110523 theta= [[-0.01386734]\n",
      " [ 0.01052467]\n",
      " [ 0.00055692]]\n",
      "the iterations of  300 the loss is 0.6283625884813296 theta= [[-0.0208364 ]\n",
      " [ 0.01057301]\n",
      " [ 0.00061108]]\n",
      "the iterations of  400 the loss is 0.6278777752243103 theta= [[-0.02779944]\n",
      " [ 0.01062132]\n",
      " [ 0.00066519]]\n",
      "the iterations of  500 the loss is 0.6273937973472077 theta= [[-0.03475649]\n",
      " [ 0.0106696 ]\n",
      " [ 0.00071927]]\n",
      "the iterations of  600 the loss is 0.6269106534576303 theta= [[-0.04170754]\n",
      " [ 0.01071783]\n",
      " [ 0.0007733 ]]\n",
      "the iterations of  700 the loss is 0.6264283421635933 theta= [[-0.0486526 ]\n",
      " [ 0.01076603]\n",
      " [ 0.00082728]]\n",
      "the iterations of  800 the loss is 0.6259468620735325 theta= [[-0.05559167]\n",
      " [ 0.0108142 ]\n",
      " [ 0.00088123]]\n",
      "the iterations of  900 the loss is 0.6254662117963165 theta= [[-0.06252476]\n",
      " [ 0.01086233]\n",
      " [ 0.00093512]]\n",
      "the iterations of  1000 the loss is 0.6249863899412604 theta= [[-0.06945187]\n",
      " [ 0.01091042]\n",
      " [ 0.00098898]]\n",
      "the iterations of  1100 the loss is 0.6245073951181386 theta= [[-0.07637301]\n",
      " [ 0.01095848]\n",
      " [ 0.00104279]]\n",
      "the iterations of  1200 the loss is 0.624029225937196 theta= [[-0.08328818]\n",
      " [ 0.0110065 ]\n",
      " [ 0.00109656]]\n",
      "the iterations of  1300 the loss is 0.623551881009163 theta= [[-0.09019739]\n",
      " [ 0.01105449]\n",
      " [ 0.00115029]]\n",
      "the iterations of  1400 the loss is 0.6230753589452668 theta= [[-0.09710064]\n",
      " [ 0.01110244]\n",
      " [ 0.00120397]]\n",
      "the iterations of  1500 the loss is 0.6225996583572437 theta= [[-0.10399794]\n",
      " [ 0.01115035]\n",
      " [ 0.00125761]]\n",
      "the iterations of  1600 the loss is 0.6221247778573523 theta= [[-0.11088929]\n",
      " [ 0.01119823]\n",
      " [ 0.0013112 ]]\n",
      "the iterations of  1700 the loss is 0.6216507160583861 theta= [[-0.11777469]\n",
      " [ 0.01124608]\n",
      " [ 0.00136475]]\n",
      "the iterations of  1800 the loss is 0.621177471573685 theta= [[-0.12465416]\n",
      " [ 0.01129388]\n",
      " [ 0.00141826]]\n",
      "the iterations of  1900 the loss is 0.620705043017148 theta= [[-0.1315277 ]\n",
      " [ 0.01134166]\n",
      " [ 0.00147172]]\n",
      "the iterations of  2000 the loss is 0.6202334290032457 theta= [[-0.1383953 ]\n",
      " [ 0.01138939]\n",
      " [ 0.00152515]]\n",
      "the iterations of  2100 the loss is 0.6197626281470326 theta= [[-0.14525699]\n",
      " [ 0.01143709]\n",
      " [ 0.00157852]]\n",
      "the iterations of  2200 the loss is 0.6192926390641582 theta= [[-0.15211275]\n",
      " [ 0.01148476]\n",
      " [ 0.00163186]]\n",
      "the iterations of  2300 the loss is 0.6188234603708801 theta= [[-0.15896261]\n",
      " [ 0.01153239]\n",
      " [ 0.00168515]]\n",
      "the iterations of  2400 the loss is 0.6183550906840756 theta= [[-0.16580655]\n",
      " [ 0.01157998]\n",
      " [ 0.0017384 ]]\n",
      "the iterations of  2500 the loss is 0.6178875286212532 theta= [[-0.17264459]\n",
      " [ 0.01162754]\n",
      " [ 0.00179161]]\n",
      "the iterations of  2600 the loss is 0.6174207728005656 theta= [[-0.17947674]\n",
      " [ 0.01167506]\n",
      " [ 0.00184477]]\n",
      "the iterations of  2700 the loss is 0.6169548218408198 theta= [[-0.18630299]\n",
      " [ 0.01172255]\n",
      " [ 0.00189789]]\n",
      "the iterations of  2800 the loss is 0.6164896743614904 theta= [[-0.19312335]\n",
      " [ 0.01177   ]\n",
      " [ 0.00195096]]\n",
      "the iterations of  2900 the loss is 0.6160253289827302 theta= [[-0.19993783]\n",
      " [ 0.01181741]\n",
      " [ 0.002004  ]]\n",
      "the iterations of  3000 the loss is 0.6155617843253824 theta= [[-0.20674643]\n",
      " [ 0.01186479]\n",
      " [ 0.00205699]]\n",
      "the iterations of  3100 the loss is 0.6150990390109916 theta= [[-0.21354915]\n",
      " [ 0.01191214]\n",
      " [ 0.00210993]]\n",
      "the iterations of  3200 the loss is 0.6146370916618159 theta= [[-0.22034601]\n",
      " [ 0.01195945]\n",
      " [ 0.00216284]]\n",
      "the iterations of  3300 the loss is 0.6141759409008375 theta= [[-0.22713701]\n",
      " [ 0.01200672]\n",
      " [ 0.0022157 ]]\n",
      "the iterations of  3400 the loss is 0.6137155853517744 theta= [[-0.23392215]\n",
      " [ 0.01205396]\n",
      " [ 0.00226852]]\n",
      "the iterations of  3500 the loss is 0.6132560236390922 theta= [[-0.24070143]\n",
      " [ 0.01210117]\n",
      " [ 0.00232129]]\n",
      "the iterations of  3600 the loss is 0.612797254388014 theta= [[-0.24747487]\n",
      " [ 0.01214833]\n",
      " [ 0.00237402]]\n",
      "the iterations of  3700 the loss is 0.6123392762245331 theta= [[-0.25424247]\n",
      " [ 0.01219547]\n",
      " [ 0.00242671]]\n",
      "the iterations of  3800 the loss is 0.6118820877754225 theta= [[-0.26100423]\n",
      " [ 0.01224257]\n",
      " [ 0.00247936]]\n",
      "the iterations of  3900 the loss is 0.6114256876682471 theta= [[-0.26776015]\n",
      " [ 0.01228963]\n",
      " [ 0.00253196]]\n",
      "the iterations of  4000 the loss is 0.610970074531374 theta= [[-0.27451025]\n",
      " [ 0.01233666]\n",
      " [ 0.00258452]]\n",
      "the iterations of  4100 the loss is 0.6105152469939834 theta= [[-0.28125453]\n",
      " [ 0.01238365]\n",
      " [ 0.00263704]]\n",
      "the iterations of  4200 the loss is 0.6100612036860799 theta= [[-0.28799299]\n",
      " [ 0.01243061]\n",
      " [ 0.00268952]]\n",
      "the iterations of  4300 the loss is 0.6096079432385021 theta= [[-0.29472564]\n",
      " [ 0.01247753]\n",
      " [ 0.00274195]]\n",
      "the iterations of  4400 the loss is 0.6091554642829349 theta= [[-0.30145248]\n",
      " [ 0.01252442]\n",
      " [ 0.00279434]]\n",
      "the iterations of  4500 the loss is 0.6087037654519185 theta= [[-0.30817352]\n",
      " [ 0.01257127]\n",
      " [ 0.00284669]]\n",
      "the iterations of  4600 the loss is 0.60825284537886 theta= [[-0.31488876]\n",
      " [ 0.01261809]\n",
      " [ 0.00289899]]\n",
      "the iterations of  4700 the loss is 0.6078027026980435 theta= [[-0.32159821]\n",
      " [ 0.01266487]\n",
      " [ 0.00295126]]\n",
      "the iterations of  4800 the loss is 0.6073533360446405 theta= [[-0.32830188]\n",
      " [ 0.01271162]\n",
      " [ 0.00300347]]\n",
      "the iterations of  4900 the loss is 0.6069047440547197 theta= [[-0.33499977]\n",
      " [ 0.01275833]\n",
      " [ 0.00305565]]\n",
      "the iterations of  5000 the loss is 0.606456925365259 theta= [[-0.34169188]\n",
      " [ 0.01280501]\n",
      " [ 0.00310779]]\n",
      "the iterations of  5100 the loss is 0.6060098786141531 theta= [[-0.34837822]\n",
      " [ 0.01285165]\n",
      " [ 0.00315988]]\n",
      "the iterations of  5200 the loss is 0.6055636024402262 theta= [[-0.35505879]\n",
      " [ 0.01289826]\n",
      " [ 0.00321193]]\n",
      "the iterations of  5300 the loss is 0.6051180954832406 theta= [[-0.36173361]\n",
      " [ 0.01294483]\n",
      " [ 0.00326394]]\n",
      "the iterations of  5400 the loss is 0.6046733563839067 theta= [[-0.36840267]\n",
      " [ 0.01299137]\n",
      " [ 0.0033159 ]]\n",
      "the iterations of  5500 the loss is 0.604229383783894 theta= [[-0.37506598]\n",
      " [ 0.01303787]\n",
      " [ 0.00336782]]\n",
      "the iterations of  5600 the loss is 0.6037861763258396 theta= [[-0.38172354]\n",
      " [ 0.01308434]\n",
      " [ 0.0034197 ]]\n",
      "the iterations of  5700 the loss is 0.603343732653359 theta= [[-0.38837537]\n",
      " [ 0.01313077]\n",
      " [ 0.00347154]]\n",
      "the iterations of  5800 the loss is 0.6029020514110559 theta= [[-0.39502146]\n",
      " [ 0.01317717]\n",
      " [ 0.00352333]]\n",
      "the iterations of  5900 the loss is 0.6024611312445309 theta= [[-0.40166183]\n",
      " [ 0.01322354]\n",
      " [ 0.00357509]]\n",
      "the iterations of  6000 the loss is 0.6020209708003922 theta= [[-0.40829647]\n",
      " [ 0.01326987]\n",
      " [ 0.0036268 ]]\n",
      "the iterations of  6100 the loss is 0.6015815687262649 theta= [[-0.41492539]\n",
      " [ 0.01331616]\n",
      " [ 0.00367847]]\n",
      "the iterations of  6200 the loss is 0.6011429236708 theta= [[-0.4215486 ]\n",
      " [ 0.01336242]\n",
      " [ 0.00373009]]\n",
      "the iterations of  6300 the loss is 0.6007050342836839 theta= [[-0.4281661 ]\n",
      " [ 0.01340865]\n",
      " [ 0.00378168]]\n",
      "the iterations of  6400 the loss is 0.600267899215649 theta= [[-0.4347779 ]\n",
      " [ 0.01345484]\n",
      " [ 0.00383322]]\n",
      "the iterations of  6500 the loss is 0.5998315171184813 theta= [[-0.44138401]\n",
      " [ 0.013501  ]\n",
      " [ 0.00388472]]\n",
      "the iterations of  6600 the loss is 0.5993958866450302 theta= [[-0.44798442]\n",
      " [ 0.01354712]\n",
      " [ 0.00393618]]\n",
      "the iterations of  6700 the loss is 0.5989610064492185 theta= [[-0.45457914]\n",
      " [ 0.01359321]\n",
      " [ 0.00398759]]\n",
      "the iterations of  6800 the loss is 0.5985268751860506 theta= [[-0.46116819]\n",
      " [ 0.01363927]\n",
      " [ 0.00403897]]\n",
      "the iterations of  6900 the loss is 0.5980934915116215 theta= [[-0.46775155]\n",
      " [ 0.01368528]\n",
      " [ 0.0040903 ]]\n",
      "the iterations of  7000 the loss is 0.5976608540831269 theta= [[-0.47432925]\n",
      " [ 0.01373127]\n",
      " [ 0.00414159]]\n",
      "the iterations of  7100 the loss is 0.5972289615588702 theta= [[-0.48090129]\n",
      " [ 0.01377722]\n",
      " [ 0.00419284]]\n",
      "the iterations of  7200 the loss is 0.5967978125982735 theta= [[-0.48746766]\n",
      " [ 0.01382314]\n",
      " [ 0.00424404]]\n",
      "the iterations of  7300 the loss is 0.596367405861885 theta= [[-0.49402838]\n",
      " [ 0.01386902]\n",
      " [ 0.00429521]]\n",
      "the iterations of  7400 the loss is 0.5959377400113879 theta= [[-0.50058344]\n",
      " [ 0.01391487]\n",
      " [ 0.00434633]]\n",
      "the iterations of  7500 the loss is 0.5955088137096096 theta= [[-0.50713287]\n",
      " [ 0.01396068]\n",
      " [ 0.00439741]]\n",
      "the iterations of  7600 the loss is 0.5950806256205295 theta= [[-0.51367666]\n",
      " [ 0.01400646]\n",
      " [ 0.00444845]]\n",
      "the iterations of  7700 the loss is 0.5946531744092887 theta= [[-0.52021481]\n",
      " [ 0.01405221]\n",
      " [ 0.00449944]]\n",
      "the iterations of  7800 the loss is 0.5942264587421972 theta= [[-0.52674734]\n",
      " [ 0.01409792]\n",
      " [ 0.0045504 ]]\n",
      "the iterations of  7900 the loss is 0.5938004772867433 theta= [[-0.53327424]\n",
      " [ 0.0141436 ]\n",
      " [ 0.00460131]]\n",
      "the iterations of  8000 the loss is 0.5933752287116014 theta= [[-0.53979552]\n",
      " [ 0.01418924]\n",
      " [ 0.00465219]]\n",
      "the iterations of  8100 the loss is 0.5929507116866405 theta= [[-0.5463112 ]\n",
      " [ 0.01423485]\n",
      " [ 0.00470302]]\n",
      "the iterations of  8200 the loss is 0.5925269248829325 theta= [[-0.55282127]\n",
      " [ 0.01428042]\n",
      " [ 0.0047538 ]]\n",
      "the iterations of  8300 the loss is 0.5921038669727599 theta= [[-0.55932573]\n",
      " [ 0.01432596]\n",
      " [ 0.00480455]]\n",
      "the iterations of  8400 the loss is 0.5916815366296244 theta= [[-0.56582461]\n",
      " [ 0.01437147]\n",
      " [ 0.00485526]]\n",
      "the iterations of  8500 the loss is 0.5912599325282553 theta= [[-0.57231789]\n",
      " [ 0.01441695]\n",
      " [ 0.00490592]]\n",
      "the iterations of  8600 the loss is 0.5908390533446161 theta= [[-0.57880558]\n",
      " [ 0.01446238]\n",
      " [ 0.00495654]]\n",
      "the iterations of  8700 the loss is 0.590418897755914 theta= [[-0.5852877 ]\n",
      " [ 0.01450779]\n",
      " [ 0.00500712]]\n",
      "the iterations of  8800 the loss is 0.589999464440607 theta= [[-0.59176424]\n",
      " [ 0.01455316]\n",
      " [ 0.00505766]]\n",
      "the iterations of  8900 the loss is 0.5895807520784113 theta= [[-0.59823522]\n",
      " [ 0.0145985 ]\n",
      " [ 0.00510816]]\n",
      "the iterations of  9000 the loss is 0.5891627593503096 theta= [[-0.60470063]\n",
      " [ 0.0146438 ]\n",
      " [ 0.00515862]]\n",
      "the iterations of  9100 the loss is 0.5887454849385593 theta= [[-0.61116048]\n",
      " [ 0.01468907]\n",
      " [ 0.00520903]]\n",
      "the iterations of  9200 the loss is 0.5883289275266987 theta= [[-0.61761478]\n",
      " [ 0.01473431]\n",
      " [ 0.00525941]]\n",
      "the iterations of  9300 the loss is 0.5879130857995556 theta= [[-0.62406353]\n",
      " [ 0.01477951]\n",
      " [ 0.00530974]]\n",
      "the iterations of  9400 the loss is 0.5874979584432546 theta= [[-0.63050675]\n",
      " [ 0.01482468]\n",
      " [ 0.00536003]]\n",
      "the iterations of  9500 the loss is 0.5870835441452242 theta= [[-0.63694442]\n",
      " [ 0.01486982]\n",
      " [ 0.00541028]]\n",
      "the iterations of  9600 the loss is 0.586669841594205 theta= [[-0.64337657]\n",
      " [ 0.01491492]\n",
      " [ 0.00546049]]\n",
      "the iterations of  9700 the loss is 0.5862568494802554 theta= [[-0.64980319]\n",
      " [ 0.01495999]\n",
      " [ 0.00551066]]\n",
      "the iterations of  9800 the loss is 0.5858445664947609 theta= [[-0.65622429]\n",
      " [ 0.01500502]\n",
      " [ 0.00556079]]\n",
      "the iterations of  9900 the loss is 0.5854329913304399 theta= [[-0.66263987]\n",
      " [ 0.01505002]\n",
      " [ 0.00561087]]\n",
      "the iterations of  10000 the loss is 0.5850221226813507 theta= [[-0.66904995]\n",
      " [ 0.01509499]\n",
      " [ 0.00566092]]\n",
      "the iterations of  10100 the loss is 0.5846119592428997 theta= [[-0.67545452]\n",
      " [ 0.01513992]\n",
      " [ 0.00571092]]\n",
      "the iterations of  10200 the loss is 0.5842024997118473 theta= [[-0.6818536 ]\n",
      " [ 0.01518482]\n",
      " [ 0.00576088]]\n",
      "the iterations of  10300 the loss is 0.583793742786316 theta= [[-0.68824718]\n",
      " [ 0.01522969]\n",
      " [ 0.0058108 ]]\n",
      "the iterations of  10400 the loss is 0.5833856871657959 theta= [[-0.69463527]\n",
      " [ 0.01527453]\n",
      " [ 0.00586068]]\n",
      "the iterations of  10500 the loss is 0.5829783315511522 theta= [[-0.70101789]\n",
      " [ 0.01531933]\n",
      " [ 0.00591052]]\n",
      "the iterations of  10600 the loss is 0.5825716746446327 theta= [[-0.70739502]\n",
      " [ 0.01536409]\n",
      " [ 0.00596032]]\n",
      "the iterations of  10700 the loss is 0.582165715149873 theta= [[-0.71376669]\n",
      " [ 0.01540883]\n",
      " [ 0.00601008]]\n",
      "the iterations of  10800 the loss is 0.5817604517719047 theta= [[-0.72013289]\n",
      " [ 0.01545353]\n",
      " [ 0.0060598 ]]\n",
      "the iterations of  10900 the loss is 0.5813558832171613 theta= [[-0.72649363]\n",
      " [ 0.01549819]\n",
      " [ 0.00610948]]\n",
      "the iterations of  11000 the loss is 0.5809520081934835 theta= [[-0.73284892]\n",
      " [ 0.01554283]\n",
      " [ 0.00615911]]\n",
      "the iterations of  11100 the loss is 0.580548825410128 theta= [[-0.73919875]\n",
      " [ 0.01558743]\n",
      " [ 0.00620871]]\n",
      "the iterations of  11200 the loss is 0.5801463335777729 theta= [[-0.74554315]\n",
      " [ 0.015632  ]\n",
      " [ 0.00625826]]\n",
      "the iterations of  11300 the loss is 0.5797445314085236 theta= [[-0.75188211]\n",
      " [ 0.01567653]\n",
      " [ 0.00630777]]\n",
      "the iterations of  11400 the loss is 0.5793434176159191 theta= [[-0.75821563]\n",
      " [ 0.01572103]\n",
      " [ 0.00635725]]\n",
      "the iterations of  11500 the loss is 0.5789429909149395 theta= [[-0.76454373]\n",
      " [ 0.0157655 ]\n",
      " [ 0.00640668]]\n",
      "the iterations of  11600 the loss is 0.5785432500220107 theta= [[-0.7708664 ]\n",
      " [ 0.01580994]\n",
      " [ 0.00645607]]\n",
      "the iterations of  11700 the loss is 0.5781441936550112 theta= [[-0.77718366]\n",
      " [ 0.01585434]\n",
      " [ 0.00650542]]\n",
      "the iterations of  11800 the loss is 0.5777458205332785 theta= [[-0.78349551]\n",
      " [ 0.01589871]\n",
      " [ 0.00655473]]\n",
      "the iterations of  11900 the loss is 0.5773481293776144 theta= [[-0.78980195]\n",
      " [ 0.01594304]\n",
      " [ 0.006604  ]]\n",
      "the iterations of  12000 the loss is 0.5769511189102915 theta= [[-0.796103  ]\n",
      " [ 0.01598735]\n",
      " [ 0.00665323]]\n",
      "the iterations of  12100 the loss is 0.5765547878550586 theta= [[-0.80239865]\n",
      " [ 0.01603162]\n",
      " [ 0.00670242]]\n",
      "the iterations of  12200 the loss is 0.5761591349371475 theta= [[-0.80868891]\n",
      " [ 0.01607585]\n",
      " [ 0.00675157]]\n",
      "the iterations of  12300 the loss is 0.5757641588832779 theta= [[-0.81497379]\n",
      " [ 0.01612006]\n",
      " [ 0.00680068]]\n",
      "the iterations of  12400 the loss is 0.575369858421663 theta= [[-0.82125329]\n",
      " [ 0.01616423]\n",
      " [ 0.00684975]]\n",
      "the iterations of  12500 the loss is 0.5749762322820159 theta= [[-0.82752742]\n",
      " [ 0.01620837]\n",
      " [ 0.00689878]]\n",
      "the iterations of  12600 the loss is 0.5745832791955553 theta= [[-0.83379619]\n",
      " [ 0.01625248]\n",
      " [ 0.00694777]]\n",
      "the iterations of  12700 the loss is 0.5741909978950102 theta= [[-0.84005959]\n",
      " [ 0.01629655]\n",
      " [ 0.00699672]]\n",
      "the iterations of  12800 the loss is 0.5737993871146259 theta= [[-0.84631764]\n",
      " [ 0.01634059]\n",
      " [ 0.00704562]]\n",
      "the iterations of  12900 the loss is 0.5734084455901698 theta= [[-0.85257033]\n",
      " [ 0.0163846 ]\n",
      " [ 0.00709449]]\n",
      "the iterations of  13000 the loss is 0.5730181720589363 theta= [[-0.85881769]\n",
      " [ 0.01642857]\n",
      " [ 0.00714332]]\n",
      "the iterations of  13100 the loss is 0.5726285652597525 theta= [[-0.8650597 ]\n",
      " [ 0.01647252]\n",
      " [ 0.00719211]]\n",
      "the iterations of  13200 the loss is 0.5722396239329831 theta= [[-0.87129638]\n",
      " [ 0.01651643]\n",
      " [ 0.00724085]]\n",
      "the iterations of  13300 the loss is 0.5718513468205362 theta= [[-0.87752774]\n",
      " [ 0.0165603 ]\n",
      " [ 0.00728956]]\n",
      "the iterations of  13400 the loss is 0.5714637326658676 theta= [[-0.88375377]\n",
      " [ 0.01660415]\n",
      " [ 0.00733823]]\n",
      "the iterations of  13500 the loss is 0.5710767802139874 theta= [[-0.88997448]\n",
      " [ 0.01664796]\n",
      " [ 0.00738686]]\n",
      "the iterations of  13600 the loss is 0.5706904882114633 theta= [[-0.89618989]\n",
      " [ 0.01669174]\n",
      " [ 0.00743545]]\n",
      "the iterations of  13700 the loss is 0.5703048554064271 theta= [[-0.90239999]\n",
      " [ 0.01673549]\n",
      " [ 0.00748399]]\n",
      "the iterations of  13800 the loss is 0.5699198805485787 theta= [[-0.90860478]\n",
      " [ 0.0167792 ]\n",
      " [ 0.0075325 ]]\n",
      "the iterations of  13900 the loss is 0.569535562389192 theta= [[-0.91480429]\n",
      " [ 0.01682289]\n",
      " [ 0.00758097]]\n",
      "the iterations of  14000 the loss is 0.5691518996811187 theta= [[-0.9209985 ]\n",
      " [ 0.01686654]\n",
      " [ 0.0076294 ]]\n",
      "the iterations of  14100 the loss is 0.5687688911787937 theta= [[-0.92718743]\n",
      " [ 0.01691016]\n",
      " [ 0.00767779]]\n",
      "the iterations of  14200 the loss is 0.5683865356382397 theta= [[-0.93337109]\n",
      " [ 0.01695374]\n",
      " [ 0.00772614]]\n",
      "the iterations of  14300 the loss is 0.5680048318170726 theta= [[-0.93954947]\n",
      " [ 0.01699729]\n",
      " [ 0.00777444]]\n",
      "the iterations of  14400 the loss is 0.5676237784745047 theta= [[-0.94572258]\n",
      " [ 0.01704082]\n",
      " [ 0.00782271]]\n",
      "the iterations of  14500 the loss is 0.5672433743713509 theta= [[-0.95189043]\n",
      " [ 0.0170843 ]\n",
      " [ 0.00787094]]\n",
      "the iterations of  14600 the loss is 0.5668636182700318 theta= [[-0.95805303]\n",
      " [ 0.01712776]\n",
      " [ 0.00791914]]\n",
      "the iterations of  14700 the loss is 0.5664845089345796 theta= [[-0.96421038]\n",
      " [ 0.01717119]\n",
      " [ 0.00796729]]\n",
      "the iterations of  14800 the loss is 0.5661060451306412 theta= [[-0.97036248]\n",
      " [ 0.01721458]\n",
      " [ 0.0080154 ]]\n",
      "the iterations of  14900 the loss is 0.5657282256254844 theta= [[-0.97650935]\n",
      " [ 0.01725794]\n",
      " [ 0.00806347]]\n",
      "the iterations of  15000 the loss is 0.5653510491879998 theta= [[-0.98265098]\n",
      " [ 0.01730127]\n",
      " [ 0.0081115 ]]\n",
      "the iterations of  15100 the loss is 0.5649745145887076 theta= [[-0.98878738]\n",
      " [ 0.01734456]\n",
      " [ 0.0081595 ]]\n",
      "the iterations of  15200 the loss is 0.5645986205997597 theta= [[-0.99491856]\n",
      " [ 0.01738783]\n",
      " [ 0.00820745]]\n",
      "the iterations of  15300 the loss is 0.5642233659949456 theta= [[-1.00104452]\n",
      " [ 0.01743106]\n",
      " [ 0.00825536]]\n",
      "the iterations of  15400 the loss is 0.5638487495496957 theta= [[-1.00716527]\n",
      " [ 0.01747426]\n",
      " [ 0.00830324]]\n",
      "the iterations of  15500 the loss is 0.5634747700410851 theta= [[-1.01328082]\n",
      " [ 0.01751743]\n",
      " [ 0.00835108]]\n",
      "the iterations of  15600 the loss is 0.5631014262478391 theta= [[-1.01939117]\n",
      " [ 0.01756057]\n",
      " [ 0.00839887]]\n",
      "the iterations of  15700 the loss is 0.562728716950335 theta= [[-1.02549632]\n",
      " [ 0.01760367]\n",
      " [ 0.00844663]]\n",
      "the iterations of  15800 the loss is 0.5623566409306081 theta= [[-1.03159628]\n",
      " [ 0.01764674]\n",
      " [ 0.00849435]]\n",
      "the iterations of  15900 the loss is 0.5619851969723547 theta= [[-1.03769106]\n",
      " [ 0.01768979]\n",
      " [ 0.00854203]]\n",
      "the iterations of  16000 the loss is 0.5616143838609358 theta= [[-1.04378066]\n",
      " [ 0.0177328 ]\n",
      " [ 0.00858967]]\n",
      "the iterations of  16100 the loss is 0.5612442003833809 theta= [[-1.04986509]\n",
      " [ 0.01777577]\n",
      " [ 0.00863727]]\n",
      "the iterations of  16200 the loss is 0.5608746453283922 theta= [[-1.05594435]\n",
      " [ 0.01781872]\n",
      " [ 0.00868483]]\n",
      "the iterations of  16300 the loss is 0.5605057174863485 theta= [[-1.06201845]\n",
      " [ 0.01786163]\n",
      " [ 0.00873236]]\n",
      "the iterations of  16400 the loss is 0.5601374156493081 theta= [[-1.0680874 ]\n",
      " [ 0.01790452]\n",
      " [ 0.00877984]]\n",
      "the iterations of  16500 the loss is 0.5597697386110125 theta= [[-1.07415119]\n",
      " [ 0.01794737]\n",
      " [ 0.00882729]]\n",
      "the iterations of  16600 the loss is 0.5594026851668908 theta= [[-1.08020984]\n",
      " [ 0.01799019]\n",
      " [ 0.00887469]]\n",
      "the iterations of  16700 the loss is 0.5590362541140624 theta= [[-1.08626336]\n",
      " [ 0.01803297]\n",
      " [ 0.00892206]]\n",
      "the iterations of  16800 the loss is 0.5586704442513404 theta= [[-1.09231173]\n",
      " [ 0.01807573]\n",
      " [ 0.00896939]]\n",
      "the iterations of  16900 the loss is 0.558305254379236 theta= [[-1.09835499]\n",
      " [ 0.01811845]\n",
      " [ 0.00901668]]\n",
      "the iterations of  17000 the loss is 0.5579406832999609 theta= [[-1.10439311]\n",
      " [ 0.01816115]\n",
      " [ 0.00906393]]\n",
      "the iterations of  17100 the loss is 0.5575767298174312 theta= [[-1.11042613]\n",
      " [ 0.01820381]\n",
      " [ 0.00911114]]\n",
      "the iterations of  17200 the loss is 0.5572133927372699 theta= [[-1.11645403]\n",
      " [ 0.01824644]\n",
      " [ 0.00915832]]\n",
      "the iterations of  17300 the loss is 0.5568506708668114 theta= [[-1.12247682]\n",
      " [ 0.01828904]\n",
      " [ 0.00920545]]\n",
      "the iterations of  17400 the loss is 0.5564885630151033 theta= [[-1.12849452]\n",
      " [ 0.01833161]\n",
      " [ 0.00925255]]\n",
      "the iterations of  17500 the loss is 0.5561270679929108 theta= [[-1.13450712]\n",
      " [ 0.01837414]\n",
      " [ 0.00929961]]\n",
      "the iterations of  17600 the loss is 0.5557661846127193 theta= [[-1.14051463]\n",
      " [ 0.01841665]\n",
      " [ 0.00934663]]\n",
      "the iterations of  17700 the loss is 0.5554059116887368 theta= [[-1.14651706]\n",
      " [ 0.01845912]\n",
      " [ 0.00939361]]\n",
      "the iterations of  17800 the loss is 0.5550462480368981 theta= [[-1.15251441]\n",
      " [ 0.01850156]\n",
      " [ 0.00944055]]\n",
      "the iterations of  17900 the loss is 0.5546871924748671 theta= [[-1.15850669]\n",
      " [ 0.01854397]\n",
      " [ 0.00948746]]\n",
      "the iterations of  18000 the loss is 0.5543287438220393 theta= [[-1.1644939 ]\n",
      " [ 0.01858635]\n",
      " [ 0.00953432]]\n",
      "the iterations of  18100 the loss is 0.5539709008995459 theta= [[-1.17047605]\n",
      " [ 0.0186287 ]\n",
      " [ 0.00958115]]\n",
      "the iterations of  18200 the loss is 0.5536136625302553 theta= [[-1.17645315]\n",
      " [ 0.01867102]\n",
      " [ 0.00962794]]\n",
      "the iterations of  18300 the loss is 0.5532570275387771 theta= [[-1.18242519]\n",
      " [ 0.01871331]\n",
      " [ 0.00967469]]\n",
      "the iterations of  18400 the loss is 0.552900994751464 theta= [[-1.1883922 ]\n",
      " [ 0.01875556]\n",
      " [ 0.0097214 ]]\n",
      "the iterations of  18500 the loss is 0.5525455629964146 theta= [[-1.19435416]\n",
      " [ 0.01879779]\n",
      " [ 0.00976808]]\n",
      "the iterations of  18600 the loss is 0.5521907311034762 theta= [[-1.20031109]\n",
      " [ 0.01883998]\n",
      " [ 0.00981471]]\n",
      "the iterations of  18700 the loss is 0.5518364979042478 theta= [[-1.20626299]\n",
      " [ 0.01888214]\n",
      " [ 0.00986131]]\n",
      "the iterations of  18800 the loss is 0.5514828622320824 theta= [[-1.21220987]\n",
      " [ 0.01892428]\n",
      " [ 0.00990787]]\n",
      "the iterations of  18900 the loss is 0.5511298229220888 theta= [[-1.21815173]\n",
      " [ 0.01896638]\n",
      " [ 0.0099544 ]]\n",
      "the iterations of  19000 the loss is 0.5507773788111353 theta= [[-1.22408858]\n",
      " [ 0.01900845]\n",
      " [ 0.01000088]]\n",
      "the iterations of  19100 the loss is 0.5504255287378512 theta= [[-1.23002043]\n",
      " [ 0.01905049]\n",
      " [ 0.01004733]]\n",
      "the iterations of  19200 the loss is 0.5500742715426302 theta= [[-1.23594728]\n",
      " [ 0.01909249]\n",
      " [ 0.01009373]]\n",
      "the iterations of  19300 the loss is 0.5497236060676315 theta= [[-1.24186913]\n",
      " [ 0.01913447]\n",
      " [ 0.0101401 ]]\n",
      "the iterations of  19400 the loss is 0.5493735311567831 theta= [[-1.24778599]\n",
      " [ 0.01917642]\n",
      " [ 0.01018644]]\n",
      "the iterations of  19500 the loss is 0.549024045655784 theta= [[-1.25369787]\n",
      " [ 0.01921833]\n",
      " [ 0.01023273]]\n",
      "the iterations of  19600 the loss is 0.5486751484121053 theta= [[-1.25960478]\n",
      " [ 0.01926022]\n",
      " [ 0.01027899]]\n",
      "the iterations of  19700 the loss is 0.5483268382749945 theta= [[-1.26550671]\n",
      " [ 0.01930207]\n",
      " [ 0.01032521]]\n",
      "the iterations of  19800 the loss is 0.5479791140954755 theta= [[-1.27140367]\n",
      " [ 0.0193439 ]\n",
      " [ 0.01037139]]\n",
      "the iterations of  19900 the loss is 0.547631974726352 theta= [[-1.27729568]\n",
      " [ 0.01938569]\n",
      " [ 0.01041753]]\n",
      "the iterations of  20000 the loss is 0.5472854190222097 theta= [[-1.28318272]\n",
      " [ 0.01942745]\n",
      " [ 0.01046364]]\n",
      "the iterations of  20100 the loss is 0.5469394458394172 theta= [[-1.28906482]\n",
      " [ 0.01946918]\n",
      " [ 0.01050971]]\n",
      "the iterations of  20200 the loss is 0.5465940540361287 theta= [[-1.29494198]\n",
      " [ 0.01951089]\n",
      " [ 0.01055574]]\n",
      "the iterations of  20300 the loss is 0.5462492424722863 theta= [[-1.30081419]\n",
      " [ 0.01955256]\n",
      " [ 0.01060173]]\n",
      "the iterations of  20400 the loss is 0.5459050100096219 theta= [[-1.30668147]\n",
      " [ 0.0195942 ]\n",
      " [ 0.01064768]]\n",
      "the iterations of  20500 the loss is 0.5455613555116576 theta= [[-1.31254382]\n",
      " [ 0.01963581]\n",
      " [ 0.0106936 ]]\n",
      "the iterations of  20600 the loss is 0.5452182778437098 theta= [[-1.31840126]\n",
      " [ 0.01967739]\n",
      " [ 0.01073948]]\n",
      "the iterations of  20700 the loss is 0.5448757758728892 theta= [[-1.32425377]\n",
      " [ 0.01971894]\n",
      " [ 0.01078533]]\n",
      "the iterations of  20800 the loss is 0.5445338484681034 theta= [[-1.33010137]\n",
      " [ 0.01976046]\n",
      " [ 0.01083113]]\n",
      "the iterations of  20900 the loss is 0.5441924945000586 theta= [[-1.33594407]\n",
      " [ 0.01980194]\n",
      " [ 0.0108769 ]]\n",
      "the iterations of  21000 the loss is 0.5438517128412609 theta= [[-1.34178187]\n",
      " [ 0.0198434 ]\n",
      " [ 0.01092263]]\n",
      "the iterations of  21100 the loss is 0.5435115023660186 theta= [[-1.34761477]\n",
      " [ 0.01988483]\n",
      " [ 0.01096833]]\n",
      "the iterations of  21200 the loss is 0.543171861950443 theta= [[-1.35344279]\n",
      " [ 0.01992623]\n",
      " [ 0.01101398]]\n",
      "the iterations of  21300 the loss is 0.5428327904724501 theta= [[-1.35926592]\n",
      " [ 0.0199676 ]\n",
      " [ 0.0110596 ]]\n",
      "the iterations of  21400 the loss is 0.5424942868117635 theta= [[-1.36508417]\n",
      " [ 0.02000893]\n",
      " [ 0.01110518]]\n",
      "the iterations of  21500 the loss is 0.5421563498499136 theta= [[-1.37089755]\n",
      " [ 0.02005024]\n",
      " [ 0.01115073]]\n",
      "the iterations of  21600 the loss is 0.5418189784702407 theta= [[-1.37670607]\n",
      " [ 0.02009152]\n",
      " [ 0.01119624]]\n",
      "the iterations of  21700 the loss is 0.5414821715578967 theta= [[-1.38250972]\n",
      " [ 0.02013276]\n",
      " [ 0.01124171]]\n",
      "the iterations of  21800 the loss is 0.5411459279998444 theta= [[-1.38830852]\n",
      " [ 0.02017398]\n",
      " [ 0.01128714]]\n",
      "the iterations of  21900 the loss is 0.5408102466848611 theta= [[-1.39410246]\n",
      " [ 0.02021517]\n",
      " [ 0.01133254]]\n",
      "the iterations of  22000 the loss is 0.5404751265035389 theta= [[-1.39989157]\n",
      " [ 0.02025633]\n",
      " [ 0.0113779 ]]\n",
      "the iterations of  22100 the loss is 0.5401405663482862 theta= [[-1.40567583]\n",
      " [ 0.02029745]\n",
      " [ 0.01142322]]\n",
      "the iterations of  22200 the loss is 0.5398065651133286 theta= [[-1.41145526]\n",
      " [ 0.02033855]\n",
      " [ 0.01146851]]\n",
      "the iterations of  22300 the loss is 0.5394731216947107 theta= [[-1.41722986]\n",
      " [ 0.02037962]\n",
      " [ 0.01151376]]\n",
      "the iterations of  22400 the loss is 0.5391402349902963 theta= [[-1.42299963]\n",
      " [ 0.02042065]\n",
      " [ 0.01155897]]\n",
      "the iterations of  22500 the loss is 0.538807903899771 theta= [[-1.42876459]\n",
      " [ 0.02046166]\n",
      " [ 0.01160415]]\n",
      "the iterations of  22600 the loss is 0.538476127324642 theta= [[-1.43452474]\n",
      " [ 0.02050264]\n",
      " [ 0.01164929]]\n",
      "the iterations of  22700 the loss is 0.5381449041682396 theta= [[-1.44028009]\n",
      " [ 0.02054359]\n",
      " [ 0.01169439]]\n",
      "the iterations of  22800 the loss is 0.5378142333357184 theta= [[-1.44603063]\n",
      " [ 0.0205845 ]\n",
      " [ 0.01173946]]\n",
      "the iterations of  22900 the loss is 0.5374841137340585 theta= [[-1.45177637]\n",
      " [ 0.02062539]\n",
      " [ 0.01178448]]\n",
      "the iterations of  23000 the loss is 0.5371545442720653 theta= [[-1.45751733]\n",
      " [ 0.02066625]\n",
      " [ 0.01182948]]\n",
      "the iterations of  23100 the loss is 0.5368255238603722 theta= [[-1.4632535 ]\n",
      " [ 0.02070708]\n",
      " [ 0.01187443]]\n",
      "the iterations of  23200 the loss is 0.5364970514114398 theta= [[-1.4689849 ]\n",
      " [ 0.02074788]\n",
      " [ 0.01191935]]\n",
      "the iterations of  23300 the loss is 0.5361691258395583 theta= [[-1.47471152]\n",
      " [ 0.02078865]\n",
      " [ 0.01196424]]\n",
      "the iterations of  23400 the loss is 0.5358417460608469 theta= [[-1.48043337]\n",
      " [ 0.02082939]\n",
      " [ 0.01200908]]\n",
      "the iterations of  23500 the loss is 0.5355149109932554 theta= [[-1.48615046]\n",
      " [ 0.0208701 ]\n",
      " [ 0.01205389]]\n",
      "the iterations of  23600 the loss is 0.5351886195565652 theta= [[-1.4918628 ]\n",
      " [ 0.02091078]\n",
      " [ 0.01209867]]\n",
      "the iterations of  23700 the loss is 0.534862870672389 theta= [[-1.49757038]\n",
      " [ 0.02095143]\n",
      " [ 0.0121434 ]]\n",
      "the iterations of  23800 the loss is 0.5345376632641731 theta= [[-1.50327322]\n",
      " [ 0.02099205]\n",
      " [ 0.01218811]]\n",
      "the iterations of  23900 the loss is 0.5342129962571959 theta= [[-1.50897131]\n",
      " [ 0.02103265]\n",
      " [ 0.01223277]]\n",
      "the iterations of  24000 the loss is 0.533888868578571 theta= [[-1.51466468]\n",
      " [ 0.02107321]\n",
      " [ 0.0122774 ]]\n",
      "the iterations of  24100 the loss is 0.5335652791572454 theta= [[-1.52035331]\n",
      " [ 0.02111374]\n",
      " [ 0.01232199]]\n",
      "the iterations of  24200 the loss is 0.5332422269240026 theta= [[-1.52603722]\n",
      " [ 0.02115425]\n",
      " [ 0.01236655]]\n",
      "the iterations of  24300 the loss is 0.5329197108114604 theta= [[-1.53171641]\n",
      " [ 0.02119472]\n",
      " [ 0.01241107]]\n",
      "the iterations of  24400 the loss is 0.5325977297540735 theta= [[-1.53739088]\n",
      " [ 0.02123517]\n",
      " [ 0.01245555]]\n",
      "the iterations of  24500 the loss is 0.5322762826881332 theta= [[-1.54306065]\n",
      " [ 0.02127559]\n",
      " [ 0.0125    ]]\n",
      "the iterations of  24600 the loss is 0.531955368551768 theta= [[-1.54872572]\n",
      " [ 0.02131597]\n",
      " [ 0.01254442]]\n",
      "the iterations of  24700 the loss is 0.5316349862849435 theta= [[-1.55438609]\n",
      " [ 0.02135633]\n",
      " [ 0.01258879]]\n",
      "the iterations of  24800 the loss is 0.5313151348294639 theta= [[-1.56004177]\n",
      " [ 0.02139666]\n",
      " [ 0.01263313]]\n",
      "the iterations of  24900 the loss is 0.5309958131289709 theta= [[-1.56569277]\n",
      " [ 0.02143696]\n",
      " [ 0.01267744]]\n",
      "the iterations of  25000 the loss is 0.5306770201289457 theta= [[-1.57133908]\n",
      " [ 0.02147723]\n",
      " [ 0.01272171]]\n",
      "the iterations of  25100 the loss is 0.5303587547767075 theta= [[-1.57698072]\n",
      " [ 0.02151747]\n",
      " [ 0.01276594]]\n",
      "the iterations of  25200 the loss is 0.5300410160214151 theta= [[-1.58261769]\n",
      " [ 0.02155768]\n",
      " [ 0.01281014]]\n",
      "the iterations of  25300 the loss is 0.5297238028140666 theta= [[-1.58825   ]\n",
      " [ 0.02159787]\n",
      " [ 0.0128543 ]]\n",
      "the iterations of  25400 the loss is 0.5294071141075 theta= [[-1.59387764]\n",
      " [ 0.02163802]\n",
      " [ 0.01289842]]\n",
      "the iterations of  25500 the loss is 0.5290909488563927 theta= [[-1.59950064]\n",
      " [ 0.02167815]\n",
      " [ 0.01294251]]\n",
      "the iterations of  25600 the loss is 0.5287753060172623 theta= [[-1.60511899]\n",
      " [ 0.02171825]\n",
      " [ 0.01298657]]\n",
      "the iterations of  25700 the loss is 0.5284601845484661 theta= [[-1.61073269]\n",
      " [ 0.02175831]\n",
      " [ 0.01303059]]\n",
      "the iterations of  25800 the loss is 0.5281455834102023 theta= [[-1.61634176]\n",
      " [ 0.02179835]\n",
      " [ 0.01307457]]\n",
      "the iterations of  25900 the loss is 0.5278315015645085 theta= [[-1.6219462 ]\n",
      " [ 0.02183836]\n",
      " [ 0.01311852]]\n",
      "the iterations of  26000 the loss is 0.5275179379752631 theta= [[-1.62754601]\n",
      " [ 0.02187834]\n",
      " [ 0.01316243]]\n",
      "the iterations of  26100 the loss is 0.5272048916081844 theta= [[-1.6331412 ]\n",
      " [ 0.02191829]\n",
      " [ 0.01320631]]\n",
      "the iterations of  26200 the loss is 0.5268923614308314 theta= [[-1.63873177]\n",
      " [ 0.02195822]\n",
      " [ 0.01325015]]\n",
      "the iterations of  26300 the loss is 0.5265803464126029 theta= [[-1.64431774]\n",
      " [ 0.02199811]\n",
      " [ 0.01329396]]\n",
      "the iterations of  26400 the loss is 0.5262688455247378 theta= [[-1.6498991 ]\n",
      " [ 0.02203798]\n",
      " [ 0.01333773]]\n",
      "the iterations of  26500 the loss is 0.5259578577403156 theta= [[-1.65547587]\n",
      " [ 0.02207782]\n",
      " [ 0.01338146]]\n",
      "the iterations of  26600 the loss is 0.525647382034255 theta= [[-1.66104804]\n",
      " [ 0.02211763]\n",
      " [ 0.01342516]]\n",
      "the iterations of  26700 the loss is 0.525337417383315 theta= [[-1.66661562]\n",
      " [ 0.02215741]\n",
      " [ 0.01346883]]\n",
      "the iterations of  26800 the loss is 0.5250279627660938 theta= [[-1.67217862]\n",
      " [ 0.02219716]\n",
      " [ 0.01351246]]\n",
      "the iterations of  26900 the loss is 0.5247190171630293 theta= [[-1.67773704]\n",
      " [ 0.02223688]\n",
      " [ 0.01355605]]\n",
      "the iterations of  27000 the loss is 0.524410579556398 theta= [[-1.68329089]\n",
      " [ 0.02227658]\n",
      " [ 0.01359961]]\n",
      "the iterations of  27100 the loss is 0.5241026489303161 theta= [[-1.68884017]\n",
      " [ 0.02231624]\n",
      " [ 0.01364314]]\n",
      "the iterations of  27200 the loss is 0.5237952242707375 theta= [[-1.69438489]\n",
      " [ 0.02235588]\n",
      " [ 0.01368663]]\n",
      "the iterations of  27300 the loss is 0.5234883045654551 theta= [[-1.69992506]\n",
      " [ 0.02239549]\n",
      " [ 0.01373008]]\n",
      "the iterations of  27400 the loss is 0.5231818888040994 theta= [[-1.70546068]\n",
      " [ 0.02243507]\n",
      " [ 0.0137735 ]]\n",
      "the iterations of  27500 the loss is 0.5228759759781385 theta= [[-1.71099175]\n",
      " [ 0.02247462]\n",
      " [ 0.01381689]]\n",
      "the iterations of  27600 the loss is 0.5225705650808777 theta= [[-1.71651828]\n",
      " [ 0.02251414]\n",
      " [ 0.01386024]]\n",
      "the iterations of  27700 the loss is 0.5222656551074593 theta= [[-1.72204028]\n",
      " [ 0.02255364]\n",
      " [ 0.01390355]]\n",
      "the iterations of  27800 the loss is 0.5219612450548614 theta= [[-1.72755775]\n",
      " [ 0.02259311]\n",
      " [ 0.01394683]]\n",
      "the iterations of  27900 the loss is 0.5216573339218986 theta= [[-1.7330707 ]\n",
      " [ 0.02263255]\n",
      " [ 0.01399008]]\n",
      "the iterations of  28000 the loss is 0.5213539207092207 theta= [[-1.73857912]\n",
      " [ 0.02267196]\n",
      " [ 0.01403329]]\n",
      "the iterations of  28100 the loss is 0.5210510044193122 theta= [[-1.74408304]\n",
      " [ 0.02271134]\n",
      " [ 0.01407646]]\n",
      "the iterations of  28200 the loss is 0.5207485840564927 theta= [[-1.74958245]\n",
      " [ 0.02275069]\n",
      " [ 0.0141196 ]]\n",
      "the iterations of  28300 the loss is 0.5204466586269144 theta= [[-1.75507735]\n",
      " [ 0.02279002]\n",
      " [ 0.01416271]]\n",
      "the iterations of  28400 the loss is 0.5201452271385636 theta= [[-1.76056776]\n",
      " [ 0.02282932]\n",
      " [ 0.01420578]]\n",
      "the iterations of  28500 the loss is 0.5198442886012593 theta= [[-1.76605368]\n",
      " [ 0.02286859]\n",
      " [ 0.01424882]]\n",
      "the iterations of  28600 the loss is 0.5195438420266519 theta= [[-1.77153511]\n",
      " [ 0.02290783]\n",
      " [ 0.01429182]]\n",
      "the iterations of  28700 the loss is 0.5192438864282236 theta= [[-1.77701206]\n",
      " [ 0.02294704]\n",
      " [ 0.01433479]]\n",
      "the iterations of  28800 the loss is 0.5189444208212871 theta= [[-1.78248454]\n",
      " [ 0.02298623]\n",
      " [ 0.01437773]]\n",
      "the iterations of  28900 the loss is 0.5186454442229852 theta= [[-1.78795254]\n",
      " [ 0.02302539]\n",
      " [ 0.01442063]]\n",
      "the iterations of  29000 the loss is 0.51834695565229 theta= [[-1.79341608]\n",
      " [ 0.02306452]\n",
      " [ 0.01446349]]\n",
      "the iterations of  29100 the loss is 0.5180489541300018 theta= [[-1.79887516]\n",
      " [ 0.02310362]\n",
      " [ 0.01450632]]\n",
      "the iterations of  29200 the loss is 0.5177514386787491 theta= [[-1.80432979]\n",
      " [ 0.02314269]\n",
      " [ 0.01454912]]\n",
      "the iterations of  29300 the loss is 0.5174544083229872 theta= [[-1.80977997]\n",
      " [ 0.02318174]\n",
      " [ 0.01459188]]\n",
      "the iterations of  29400 the loss is 0.5171578620889972 theta= [[-1.8152257 ]\n",
      " [ 0.02322076]\n",
      " [ 0.01463461]]\n",
      "the iterations of  29500 the loss is 0.5168617990048864 theta= [[-1.820667  ]\n",
      " [ 0.02325975]\n",
      " [ 0.0146773 ]]\n",
      "the iterations of  29600 the loss is 0.5165662181005859 theta= [[-1.82610386]\n",
      " [ 0.02329871]\n",
      " [ 0.01471996]]\n",
      "the iterations of  29700 the loss is 0.5162711184078504 theta= [[-1.8315363 ]\n",
      " [ 0.02333765]\n",
      " [ 0.01476259]]\n",
      "the iterations of  29800 the loss is 0.5159764989602579 theta= [[-1.83696431]\n",
      " [ 0.02337656]\n",
      " [ 0.01480518]]\n",
      "the iterations of  29900 the loss is 0.5156823587932075 theta= [[-1.84238791]\n",
      " [ 0.02341544]\n",
      " [ 0.01484774]]\n",
      "the iterations of  30000 the loss is 0.5153886969439199 theta= [[-1.8478071 ]\n",
      " [ 0.02345429]\n",
      " [ 0.01489026]]\n",
      "the iterations of  30100 the loss is 0.5150955124514354 theta= [[-1.85322188]\n",
      " [ 0.02349311]\n",
      " [ 0.01493275]]\n",
      "the iterations of  30200 the loss is 0.5148028043566131 theta= [[-1.85863225]\n",
      " [ 0.02353191]\n",
      " [ 0.01497521]]\n",
      "the iterations of  30300 the loss is 0.5145105717021299 theta= [[-1.86403824]\n",
      " [ 0.02357068]\n",
      " [ 0.01501763]]\n",
      "the iterations of  30400 the loss is 0.5142188135324804 theta= [[-1.86943983]\n",
      " [ 0.02360942]\n",
      " [ 0.01506002]]\n",
      "the iterations of  30500 the loss is 0.5139275288939747 theta= [[-1.87483703]\n",
      " [ 0.02364813]\n",
      " [ 0.01510237]]\n",
      "the iterations of  30600 the loss is 0.5136367168347372 theta= [[-1.88022986]\n",
      " [ 0.02368682]\n",
      " [ 0.01514469]]\n",
      "the iterations of  30700 the loss is 0.5133463764047073 theta= [[-1.88561831]\n",
      " [ 0.02372548]\n",
      " [ 0.01518698]]\n",
      "the iterations of  30800 the loss is 0.5130565066556354 theta= [[-1.89100239]\n",
      " [ 0.02376411]\n",
      " [ 0.01522923]]\n",
      "the iterations of  30900 the loss is 0.5127671066410855 theta= [[-1.89638211]\n",
      " [ 0.02380272]\n",
      " [ 0.01527145]]\n",
      "the iterations of  31000 the loss is 0.51247817541643 theta= [[-1.90175747]\n",
      " [ 0.02384129]\n",
      " [ 0.01531364]]\n",
      "the iterations of  31100 the loss is 0.5121897120388524 theta= [[-1.90712847]\n",
      " [ 0.02387984]\n",
      " [ 0.01535579]]\n",
      "the iterations of  31200 the loss is 0.5119017155673428 theta= [[-1.91249513]\n",
      " [ 0.02391837]\n",
      " [ 0.01539791]]\n",
      "the iterations of  31300 the loss is 0.5116141850626993 theta= [[-1.91785744]\n",
      " [ 0.02395686]\n",
      " [ 0.01544   ]]\n",
      "the iterations of  31400 the loss is 0.5113271195875251 theta= [[-1.92321541]\n",
      " [ 0.02399533]\n",
      " [ 0.01548205]]\n",
      "the iterations of  31500 the loss is 0.511040518206228 theta= [[-1.92856905]\n",
      " [ 0.02403377]\n",
      " [ 0.01552407]]\n",
      "the iterations of  31600 the loss is 0.510754379985019 theta= [[-1.93391837]\n",
      " [ 0.02407218]\n",
      " [ 0.01556605]]\n",
      "the iterations of  31700 the loss is 0.5104687039919117 theta= [[-1.93926336]\n",
      " [ 0.02411057]\n",
      " [ 0.015608  ]]\n",
      "the iterations of  31800 the loss is 0.5101834892967192 theta= [[-1.94460403]\n",
      " [ 0.02414893]\n",
      " [ 0.01564992]]\n",
      "the iterations of  31900 the loss is 0.5098987349710553 theta= [[-1.94994039]\n",
      " [ 0.02418726]\n",
      " [ 0.01569181]]\n",
      "the iterations of  32000 the loss is 0.5096144400883308 theta= [[-1.95527245]\n",
      " [ 0.02422556]\n",
      " [ 0.01573366]]\n",
      "the iterations of  32100 the loss is 0.5093306037237535 theta= [[-1.9606002 ]\n",
      " [ 0.02426384]\n",
      " [ 0.01577548]]\n",
      "the iterations of  32200 the loss is 0.5090472249543274 theta= [[-1.96592366]\n",
      " [ 0.02430209]\n",
      " [ 0.01581726]]\n",
      "the iterations of  32300 the loss is 0.5087643028588493 theta= [[-1.97124282]\n",
      " [ 0.02434032]\n",
      " [ 0.01585901]]\n",
      "the iterations of  32400 the loss is 0.5084818365179095 theta= [[-1.9765577 ]\n",
      " [ 0.02437851]\n",
      " [ 0.01590073]]\n",
      "the iterations of  32500 the loss is 0.508199825013889 theta= [[-1.9818683 ]\n",
      " [ 0.02441668]\n",
      " [ 0.01594242]]\n",
      "the iterations of  32600 the loss is 0.5079182674309592 theta= [[-1.98717462]\n",
      " [ 0.02445483]\n",
      " [ 0.01598407]]\n",
      "the iterations of  32700 the loss is 0.5076371628550793 theta= [[-1.99247668]\n",
      " [ 0.02449294]\n",
      " [ 0.01602569]]\n",
      "the iterations of  32800 the loss is 0.5073565103739957 theta= [[-1.99777446]\n",
      " [ 0.02453103]\n",
      " [ 0.01606728]]\n",
      "the iterations of  32900 the loss is 0.5070763090772402 theta= [[-2.00306799]\n",
      " [ 0.02456909]\n",
      " [ 0.01610883]]\n",
      "the iterations of  33000 the loss is 0.5067965580561283 theta= [[-2.00835726]\n",
      " [ 0.02460713]\n",
      " [ 0.01615035]]\n",
      "the iterations of  33100 the loss is 0.5065172564037586 theta= [[-2.01364228]\n",
      " [ 0.02464514]\n",
      " [ 0.01619184]]\n",
      "the iterations of  33200 the loss is 0.5062384032150097 theta= [[-2.01892306]\n",
      " [ 0.02468312]\n",
      " [ 0.0162333 ]]\n",
      "the iterations of  33300 the loss is 0.505959997586541 theta= [[-2.02419959]\n",
      " [ 0.02472108]\n",
      " [ 0.01627472]]\n",
      "the iterations of  33400 the loss is 0.5056820386167883 theta= [[-2.0294719 ]\n",
      " [ 0.024759  ]\n",
      " [ 0.01631611]]\n",
      "the iterations of  33500 the loss is 0.5054045254059648 theta= [[-2.03473997]\n",
      " [ 0.02479691]\n",
      " [ 0.01635747]]\n",
      "the iterations of  33600 the loss is 0.5051274570560575 theta= [[-2.04000382]\n",
      " [ 0.02483478]\n",
      " [ 0.01639879]]\n",
      "the iterations of  33700 the loss is 0.5048508326708273 theta= [[-2.04526345]\n",
      " [ 0.02487263]\n",
      " [ 0.01644008]]\n",
      "the iterations of  33800 the loss is 0.5045746513558064 theta= [[-2.05051886]\n",
      " [ 0.02491045]\n",
      " [ 0.01648134]]\n",
      "the iterations of  33900 the loss is 0.5042989122182966 theta= [[-2.05577007]\n",
      " [ 0.02494825]\n",
      " [ 0.01652257]]\n",
      "the iterations of  34000 the loss is 0.5040236143673682 theta= [[-2.06101707]\n",
      " [ 0.02498602]\n",
      " [ 0.01656376]]\n",
      "the iterations of  34100 the loss is 0.503748756913858 theta= [[-2.06625988]\n",
      " [ 0.02502376]\n",
      " [ 0.01660492]]\n",
      "the iterations of  34200 the loss is 0.5034743389703682 theta= [[-2.07149849]\n",
      " [ 0.02506147]\n",
      " [ 0.01664605]]\n",
      "the iterations of  34300 the loss is 0.5032003596512632 theta= [[-2.07673291]\n",
      " [ 0.02509916]\n",
      " [ 0.01668715]]\n",
      "the iterations of  34400 the loss is 0.50292681807267 theta= [[-2.08196315]\n",
      " [ 0.02513683]\n",
      " [ 0.01672821]]\n",
      "the iterations of  34500 the loss is 0.5026537133524751 theta= [[-2.08718921]\n",
      " [ 0.02517446]\n",
      " [ 0.01676925]]\n",
      "the iterations of  34600 the loss is 0.502381044610323 theta= [[-2.0924111 ]\n",
      " [ 0.02521207]\n",
      " [ 0.01681025]]\n",
      "the iterations of  34700 the loss is 0.5021088109676145 theta= [[-2.09762882]\n",
      " [ 0.02524966]\n",
      " [ 0.01685121]]\n",
      "the iterations of  34800 the loss is 0.5018370115475054 theta= [[-2.10284238]\n",
      " [ 0.02528722]\n",
      " [ 0.01689215]]\n",
      "the iterations of  34900 the loss is 0.5015656454749042 theta= [[-2.10805178]\n",
      " [ 0.02532475]\n",
      " [ 0.01693305]]\n",
      "the iterations of  35000 the loss is 0.5012947118764702 theta= [[-2.11325702]\n",
      " [ 0.02536225]\n",
      " [ 0.01697392]]\n",
      "the iterations of  35100 the loss is 0.5010242098806124 theta= [[-2.11845812]\n",
      " [ 0.02539973]\n",
      " [ 0.01701476]]\n",
      "the iterations of  35200 the loss is 0.500754138617487 theta= [[-2.12365508]\n",
      " [ 0.02543719]\n",
      " [ 0.01705557]]\n",
      "the iterations of  35300 the loss is 0.500484497218996 theta= [[-2.12884789]\n",
      " [ 0.02547461]\n",
      " [ 0.01709634]]\n",
      "the iterations of  35400 the loss is 0.5002152848187854 theta= [[-2.13403658]\n",
      " [ 0.02551201]\n",
      " [ 0.01713709]]\n",
      "the iterations of  35500 the loss is 0.4999465005522428 theta= [[-2.13922114]\n",
      " [ 0.02554939]\n",
      " [ 0.0171778 ]]\n",
      "the iterations of  35600 the loss is 0.4996781435564964 theta= [[-2.14440157]\n",
      " [ 0.02558674]\n",
      " [ 0.01721848]]\n",
      "the iterations of  35700 the loss is 0.49941021297041227 theta= [[-2.14957789]\n",
      " [ 0.02562406]\n",
      " [ 0.01725912]]\n",
      "the iterations of  35800 the loss is 0.49914270793459314 theta= [[-2.15475009]\n",
      " [ 0.02566135]\n",
      " [ 0.01729974]]\n",
      "the iterations of  35900 the loss is 0.4988756275913763 theta= [[-2.15991819]\n",
      " [ 0.02569862]\n",
      " [ 0.01734032]]\n",
      "the iterations of  36000 the loss is 0.4986089710848313 theta= [[-2.16508219]\n",
      " [ 0.02573587]\n",
      " [ 0.01738087]]\n",
      "the iterations of  36100 the loss is 0.49834273756075886 theta= [[-2.17024208]\n",
      " [ 0.02577309]\n",
      " [ 0.01742139]]\n",
      "the iterations of  36200 the loss is 0.49807692616668786 theta= [[-2.17539789]\n",
      " [ 0.02581028]\n",
      " [ 0.01746188]]\n",
      "the iterations of  36300 the loss is 0.49781153605187434 theta= [[-2.18054961]\n",
      " [ 0.02584745]\n",
      " [ 0.01750234]]\n",
      "the iterations of  36400 the loss is 0.4975465663672991 theta= [[-2.18569724]\n",
      " [ 0.02588459]\n",
      " [ 0.01754276]]\n",
      "the iterations of  36500 the loss is 0.49728201626566637 theta= [[-2.1908408 ]\n",
      " [ 0.0259217 ]\n",
      " [ 0.01758315]]\n",
      "the iterations of  36600 the loss is 0.49701788490140025 theta= [[-2.19598029]\n",
      " [ 0.02595879]\n",
      " [ 0.01762352]]\n",
      "the iterations of  36700 the loss is 0.4967541714306445 theta= [[-2.20111571]\n",
      " [ 0.02599585]\n",
      " [ 0.01766385]]\n",
      "the iterations of  36800 the loss is 0.49649087501125977 theta= [[-2.20624706]\n",
      " [ 0.02603289]\n",
      " [ 0.01770414]]\n",
      "the iterations of  36900 the loss is 0.4962279948028215 theta= [[-2.21137436]\n",
      " [ 0.0260699 ]\n",
      " [ 0.01774441]]\n",
      "the iterations of  37000 the loss is 0.4959655299666181 theta= [[-2.21649761]\n",
      " [ 0.02610689]\n",
      " [ 0.01778465]]\n",
      "the iterations of  37100 the loss is 0.4957034796656486 theta= [[-2.22161681]\n",
      " [ 0.02614385]\n",
      " [ 0.01782485]]\n",
      "the iterations of  37200 the loss is 0.4954418430646217 theta= [[-2.22673196]\n",
      " [ 0.02618078]\n",
      " [ 0.01786502]]\n",
      "the iterations of  37300 the loss is 0.49518061932995183 theta= [[-2.23184308]\n",
      " [ 0.02621769]\n",
      " [ 0.01790516]]\n",
      "the iterations of  37400 the loss is 0.49491980762975885 theta= [[-2.23695017]\n",
      " [ 0.02625458]\n",
      " [ 0.01794527]]\n",
      "the iterations of  37500 the loss is 0.4946594071338651 theta= [[-2.24205322]\n",
      " [ 0.02629144]\n",
      " [ 0.01798535]]\n",
      "the iterations of  37600 the loss is 0.4943994170137932 theta= [[-2.24715226]\n",
      " [ 0.02632827]\n",
      " [ 0.0180254 ]]\n",
      "the iterations of  37700 the loss is 0.49413983644276505 theta= [[-2.25224728]\n",
      " [ 0.02636507]\n",
      " [ 0.01806542]]\n",
      "the iterations of  37800 the loss is 0.493880664595698 theta= [[-2.25733828]\n",
      " [ 0.02640186]\n",
      " [ 0.0181054 ]]\n",
      "the iterations of  37900 the loss is 0.49362190064920447 theta= [[-2.26242528]\n",
      " [ 0.02643861]\n",
      " [ 0.01814536]]\n",
      "the iterations of  38000 the loss is 0.4933635437815884 theta= [[-2.26750827]\n",
      " [ 0.02647534]\n",
      " [ 0.01818528]]\n",
      "the iterations of  38100 the loss is 0.4931055931728443 theta= [[-2.27258727]\n",
      " [ 0.02651205]\n",
      " [ 0.01822517]]\n",
      "the iterations of  38200 the loss is 0.49284804800465404 theta= [[-2.27766227]\n",
      " [ 0.02654873]\n",
      " [ 0.01826503]]\n",
      "the iterations of  38300 the loss is 0.49259090746038553 theta= [[-2.28273328]\n",
      " [ 0.02658538]\n",
      " [ 0.01830486]]\n",
      "the iterations of  38400 the loss is 0.4923341707250904 theta= [[-2.28780031]\n",
      " [ 0.02662201]\n",
      " [ 0.01834466]]\n",
      "the iterations of  38500 the loss is 0.49207783698550167 theta= [[-2.29286337]\n",
      " [ 0.02665862]\n",
      " [ 0.01838443]]\n",
      "the iterations of  38600 the loss is 0.49182190543003107 theta= [[-2.29792245]\n",
      " [ 0.0266952 ]\n",
      " [ 0.01842417]]\n",
      "the iterations of  38700 the loss is 0.4915663752487683 theta= [[-2.30297756]\n",
      " [ 0.02673175]\n",
      " [ 0.01846387]]\n",
      "the iterations of  38800 the loss is 0.4913112456334771 theta= [[-2.3080287 ]\n",
      " [ 0.02676828]\n",
      " [ 0.01850355]]\n",
      "the iterations of  38900 the loss is 0.4910565157775942 theta= [[-2.31307589]\n",
      " [ 0.02680478]\n",
      " [ 0.01854319]]\n",
      "the iterations of  39000 the loss is 0.4908021848762269 theta= [[-2.31811912]\n",
      " [ 0.02684126]\n",
      " [ 0.01858281]]\n",
      "the iterations of  39100 the loss is 0.4905482521261503 theta= [[-2.32315841]\n",
      " [ 0.02687771]\n",
      " [ 0.01862239]]\n",
      "the iterations of  39200 the loss is 0.49029471672580627 theta= [[-2.32819375]\n",
      " [ 0.02691414]\n",
      " [ 0.01866194]]\n",
      "the iterations of  39300 the loss is 0.49004157787529967 theta= [[-2.33322515]\n",
      " [ 0.02695054]\n",
      " [ 0.01870147]]\n",
      "the iterations of  39400 the loss is 0.4897888347763977 theta= [[-2.33825261]\n",
      " [ 0.02698692]\n",
      " [ 0.01874096]]\n",
      "the iterations of  39500 the loss is 0.4895364866325255 theta= [[-2.34327615]\n",
      " [ 0.02702327]\n",
      " [ 0.01878042]]\n",
      "the iterations of  39600 the loss is 0.48928453264876665 theta= [[-2.34829576]\n",
      " [ 0.0270596 ]\n",
      " [ 0.01881985]]\n",
      "the iterations of  39700 the loss is 0.489032972031859 theta= [[-2.35331145]\n",
      " [ 0.0270959 ]\n",
      " [ 0.01885925]]\n",
      "the iterations of  39800 the loss is 0.48878180399019244 theta= [[-2.35832322]\n",
      " [ 0.02713218]\n",
      " [ 0.01889862]]\n",
      "the iterations of  39900 the loss is 0.4885310277338074 theta= [[-2.36333109]\n",
      " [ 0.02716843]\n",
      " [ 0.01893796]]\n",
      "the iterations of  40000 the loss is 0.4882806424743922 theta= [[-2.36833505]\n",
      " [ 0.02720466]\n",
      " [ 0.01897727]]\n",
      "the iterations of  40100 the loss is 0.48803064742528074 theta= [[-2.37333511]\n",
      " [ 0.02724086]\n",
      " [ 0.01901655]]\n",
      "the iterations of  40200 the loss is 0.48778104180145027 theta= [[-2.37833127]\n",
      " [ 0.02727704]\n",
      " [ 0.0190558 ]]\n",
      "the iterations of  40300 the loss is 0.48753182481951896 theta= [[-2.38332354]\n",
      " [ 0.02731319]\n",
      " [ 0.01909501]]\n",
      "the iterations of  40400 the loss is 0.4872829956977437 theta= [[-2.38831193]\n",
      " [ 0.02734932]\n",
      " [ 0.0191342 ]]\n",
      "the iterations of  40500 the loss is 0.48703455365601755 theta= [[-2.39329643]\n",
      " [ 0.02738543]\n",
      " [ 0.01917336]]\n",
      "the iterations of  40600 the loss is 0.4867864979158678 theta= [[-2.39827706]\n",
      " [ 0.02742151]\n",
      " [ 0.01921249]]\n",
      "the iterations of  40700 the loss is 0.4865388277004531 theta= [[-2.40325381]\n",
      " [ 0.02745756]\n",
      " [ 0.01925158]]\n",
      "the iterations of  40800 the loss is 0.48629154223456184 theta= [[-2.4082267 ]\n",
      " [ 0.02749359]\n",
      " [ 0.01929065]]\n",
      "the iterations of  40900 the loss is 0.4860446407446089 theta= [[-2.41319572]\n",
      " [ 0.0275296 ]\n",
      " [ 0.01932969]]\n",
      "the iterations of  41000 the loss is 0.48579812245863435 theta= [[-2.41816089]\n",
      " [ 0.02756558]\n",
      " [ 0.0193687 ]]\n",
      "the iterations of  41100 the loss is 0.4855519866062999 theta= [[-2.4231222 ]\n",
      " [ 0.02760153]\n",
      " [ 0.01940767]]\n",
      "the iterations of  41200 the loss is 0.4853062324188872 theta= [[-2.42807967]\n",
      " [ 0.02763746]\n",
      " [ 0.01944662]]\n",
      "the iterations of  41300 the loss is 0.48506085912929575 theta= [[-2.43303329]\n",
      " [ 0.02767337]\n",
      " [ 0.01948554]]\n",
      "the iterations of  41400 the loss is 0.4848158659720399 theta= [[-2.43798308]\n",
      " [ 0.02770925]\n",
      " [ 0.01952443]]\n",
      "the iterations of  41500 the loss is 0.48457125218324676 theta= [[-2.44292903]\n",
      " [ 0.02774511]\n",
      " [ 0.01956328]]\n",
      "the iterations of  41600 the loss is 0.4843270170006535 theta= [[-2.44787115]\n",
      " [ 0.02778094]\n",
      " [ 0.01960211]]\n",
      "the iterations of  41700 the loss is 0.48408315966360604 theta= [[-2.45280944]\n",
      " [ 0.02781675]\n",
      " [ 0.01964091]]\n",
      "the iterations of  41800 the loss is 0.4838396794130545 theta= [[-2.45774392]\n",
      " [ 0.02785254]\n",
      " [ 0.01967968]]\n",
      "the iterations of  41900 the loss is 0.48359657549155316 theta= [[-2.46267458]\n",
      " [ 0.0278883 ]\n",
      " [ 0.01971842]]\n",
      "the iterations of  42000 the loss is 0.4833538471432562 theta= [[-2.46760143]\n",
      " [ 0.02792403]\n",
      " [ 0.01975713]]\n",
      "the iterations of  42100 the loss is 0.48311149361391664 theta= [[-2.47252448]\n",
      " [ 0.02795975]\n",
      " [ 0.01979581]]\n",
      "the iterations of  42200 the loss is 0.48286951415088303 theta= [[-2.47744373]\n",
      " [ 0.02799543]\n",
      " [ 0.01983446]]\n",
      "the iterations of  42300 the loss is 0.48262790800309724 theta= [[-2.48235917]\n",
      " [ 0.0280311 ]\n",
      " [ 0.01987308]]\n",
      "the iterations of  42400 the loss is 0.48238667442109207 theta= [[-2.48727083]\n",
      " [ 0.02806673]\n",
      " [ 0.01991167]]\n",
      "the iterations of  42500 the loss is 0.48214581265698897 theta= [[-2.4921787 ]\n",
      " [ 0.02810235]\n",
      " [ 0.01995023]]\n",
      "the iterations of  42600 the loss is 0.4819053219644951 theta= [[-2.49708279]\n",
      " [ 0.02813794]\n",
      " [ 0.01998876]]\n",
      "the iterations of  42700 the loss is 0.48166520159890136 theta= [[-2.5019831 ]\n",
      " [ 0.0281735 ]\n",
      " [ 0.02002726]]\n",
      "the iterations of  42800 the loss is 0.48142545081708016 theta= [[-2.50687964]\n",
      " [ 0.02820905]\n",
      " [ 0.02006574]]\n",
      "the iterations of  42900 the loss is 0.4811860688774815 theta= [[-2.51177241]\n",
      " [ 0.02824456]\n",
      " [ 0.02010418]]\n",
      "the iterations of  43000 the loss is 0.4809470550401327 theta= [[-2.51666142]\n",
      " [ 0.02828006]\n",
      " [ 0.02014259]]\n",
      "the iterations of  43100 the loss is 0.4807084085666339 theta= [[-2.52154667]\n",
      " [ 0.02831553]\n",
      " [ 0.02018098]]\n",
      "the iterations of  43200 the loss is 0.4804701287201569 theta= [[-2.52642816]\n",
      " [ 0.02835097]\n",
      " [ 0.02021934]]\n",
      "the iterations of  43300 the loss is 0.4802322147654421 theta= [[-2.53130591]\n",
      " [ 0.0283864 ]\n",
      " [ 0.02025766]]\n",
      "the iterations of  43400 the loss is 0.47999466596879614 theta= [[-2.53617991]\n",
      " [ 0.02842179]\n",
      " [ 0.02029596]]\n",
      "the iterations of  43500 the loss is 0.4797574815980895 theta= [[-2.54105016]\n",
      " [ 0.02845717]\n",
      " [ 0.02033423]]\n",
      "the iterations of  43600 the loss is 0.47952066092275336 theta= [[-2.54591669]\n",
      " [ 0.02849252]\n",
      " [ 0.02037247]]\n",
      "the iterations of  43700 the loss is 0.4792842032137784 theta= [[-2.55077948]\n",
      " [ 0.02852784]\n",
      " [ 0.02041068]]\n",
      "the iterations of  43800 the loss is 0.47904810774371115 theta= [[-2.55563855]\n",
      " [ 0.02856314]\n",
      " [ 0.02044886]]\n",
      "the iterations of  43900 the loss is 0.4788123737866517 theta= [[-2.56049389]\n",
      " [ 0.02859842]\n",
      " [ 0.02048701]]\n",
      "the iterations of  44000 the loss is 0.47857700061825137 theta= [[-2.56534552]\n",
      " [ 0.02863368]\n",
      " [ 0.02052513]]\n",
      "the iterations of  44100 the loss is 0.4783419875157105 theta= [[-2.57019344]\n",
      " [ 0.02866891]\n",
      " [ 0.02056323]]\n",
      "the iterations of  44200 the loss is 0.47810733375777525 theta= [[-2.57503765]\n",
      " [ 0.02870411]\n",
      " [ 0.02060129]]\n",
      "the iterations of  44300 the loss is 0.4778730386247354 theta= [[-2.57987815]\n",
      " [ 0.0287393 ]\n",
      " [ 0.02063933]]\n",
      "the iterations of  44400 the loss is 0.477639101398422 theta= [[-2.58471496]\n",
      " [ 0.02877446]\n",
      " [ 0.02067734]]\n",
      "the iterations of  44500 the loss is 0.4774055213622047 theta= [[-2.58954807]\n",
      " [ 0.02880959]\n",
      " [ 0.02071532]]\n",
      "the iterations of  44600 the loss is 0.4771722978009887 theta= [[-2.59437749]\n",
      " [ 0.0288447 ]\n",
      " [ 0.02075327]]\n",
      "the iterations of  44700 the loss is 0.4769394300012134 theta= [[-2.59920323]\n",
      " [ 0.02887979]\n",
      " [ 0.02079119]]\n",
      "the iterations of  44800 the loss is 0.47670691725084857 theta= [[-2.60402529]\n",
      " [ 0.02891486]\n",
      " [ 0.02082908]]\n",
      "the iterations of  44900 the loss is 0.4764747588393925 theta= [[-2.60884367]\n",
      " [ 0.0289499 ]\n",
      " [ 0.02086695]]\n",
      "the iterations of  45000 the loss is 0.47624295405786954 theta= [[-2.61365838]\n",
      " [ 0.02898491]\n",
      " [ 0.02090478]]\n",
      "the iterations of  45100 the loss is 0.47601150219882676 theta= [[-2.61846942]\n",
      " [ 0.02901991]\n",
      " [ 0.02094259]]\n",
      "the iterations of  45200 the loss is 0.4757804025563328 theta= [[-2.6232768 ]\n",
      " [ 0.02905488]\n",
      " [ 0.02098037]]\n",
      "the iterations of  45300 the loss is 0.4755496544259738 theta= [[-2.62808053]\n",
      " [ 0.02908982]\n",
      " [ 0.02101812]]\n",
      "the iterations of  45400 the loss is 0.4753192571048514 theta= [[-2.6328806 ]\n",
      " [ 0.02912475]\n",
      " [ 0.02105584]]\n",
      "the iterations of  45500 the loss is 0.47508920989158104 theta= [[-2.63767702]\n",
      " [ 0.02915965]\n",
      " [ 0.02109353]]\n",
      "the iterations of  45600 the loss is 0.4748595120862879 theta= [[-2.6424698 ]\n",
      " [ 0.02919452]\n",
      " [ 0.02113119]]\n",
      "the iterations of  45700 the loss is 0.47463016299060534 theta= [[-2.64725894]\n",
      " [ 0.02922938]\n",
      " [ 0.02116883]]\n",
      "the iterations of  45800 the loss is 0.474401161907672 theta= [[-2.65204444]\n",
      " [ 0.02926421]\n",
      " [ 0.02120644]]\n",
      "the iterations of  45900 the loss is 0.4741725081421294 theta= [[-2.65682631]\n",
      " [ 0.02929901]\n",
      " [ 0.02124402]]\n",
      "the iterations of  46000 the loss is 0.47394420100011897 theta= [[-2.66160456]\n",
      " [ 0.0293338 ]\n",
      " [ 0.02128157]]\n",
      "the iterations of  46100 the loss is 0.4737162397892796 theta= [[-2.66637919]\n",
      " [ 0.02936856]\n",
      " [ 0.02131909]]\n",
      "the iterations of  46200 the loss is 0.47348862381874574 theta= [[-2.6711502 ]\n",
      " [ 0.02940329]\n",
      " [ 0.02135658]]\n",
      "the iterations of  46300 the loss is 0.47326135239914374 theta= [[-2.67591759]\n",
      " [ 0.029438  ]\n",
      " [ 0.02139405]]\n",
      "the iterations of  46400 the loss is 0.47303442484259 theta= [[-2.68068138]\n",
      " [ 0.02947269]\n",
      " [ 0.02143149]]\n",
      "the iterations of  46500 the loss is 0.47280784046268814 theta= [[-2.68544157]\n",
      " [ 0.02950736]\n",
      " [ 0.0214689 ]]\n",
      "the iterations of  46600 the loss is 0.47258159857452603 theta= [[-2.69019815]\n",
      " [ 0.029542  ]\n",
      " [ 0.02150628]]\n",
      "the iterations of  46700 the loss is 0.4723556984946741 theta= [[-2.69495114]\n",
      " [ 0.02957663]\n",
      " [ 0.02154364]]\n",
      "the iterations of  46800 the loss is 0.47213013954118194 theta= [[-2.69970054]\n",
      " [ 0.02961122]\n",
      " [ 0.02158096]]\n",
      "the iterations of  46900 the loss is 0.4719049210335763 theta= [[-2.70444636]\n",
      " [ 0.0296458 ]\n",
      " [ 0.02161826]]\n",
      "the iterations of  47000 the loss is 0.47168004229285754 theta= [[-2.70918859]\n",
      " [ 0.02968035]\n",
      " [ 0.02165553]]\n",
      "the iterations of  47100 the loss is 0.4714555026414983 theta= [[-2.71392725]\n",
      " [ 0.02971488]\n",
      " [ 0.02169277]]\n",
      "the iterations of  47200 the loss is 0.47123130140344005 theta= [[-2.71866233]\n",
      " [ 0.02974938]\n",
      " [ 0.02172999]]\n",
      "the iterations of  47300 the loss is 0.4710074379040904 theta= [[-2.72339385]\n",
      " [ 0.02978386]\n",
      " [ 0.02176717]]\n",
      "the iterations of  47400 the loss is 0.47078391147032095 theta= [[-2.72812181]\n",
      " [ 0.02981832]\n",
      " [ 0.02180433]]\n",
      "the iterations of  47500 the loss is 0.4705607214304648 theta= [[-2.7328462 ]\n",
      " [ 0.02985276]\n",
      " [ 0.02184146]]\n",
      "the iterations of  47600 the loss is 0.47033786711431314 theta= [[-2.73756704]\n",
      " [ 0.02988717]\n",
      " [ 0.02187856]]\n",
      "the iterations of  47700 the loss is 0.4701153478531135 theta= [[-2.74228433]\n",
      " [ 0.02992156]\n",
      " [ 0.02191564]]\n",
      "the iterations of  47800 the loss is 0.4698931629795667 theta= [[-2.74699807]\n",
      " [ 0.02995593]\n",
      " [ 0.02195269]]\n",
      "the iterations of  47900 the loss is 0.4696713118278243 theta= [[-2.75170827]\n",
      " [ 0.02999027]\n",
      " [ 0.02198971]]\n",
      "the iterations of  48000 the loss is 0.46944979373348567 theta= [[-2.75641494]\n",
      " [ 0.0300246 ]\n",
      " [ 0.0220267 ]]\n",
      "the iterations of  48100 the loss is 0.46922860803359645 theta= [[-2.76111807]\n",
      " [ 0.03005889]\n",
      " [ 0.02206366]]\n",
      "the iterations of  48200 the loss is 0.4690077540666445 theta= [[-2.76581768]\n",
      " [ 0.03009317]\n",
      " [ 0.0221006 ]]\n",
      "the iterations of  48300 the loss is 0.4687872311725581 theta= [[-2.77051376]\n",
      " [ 0.03012742]\n",
      " [ 0.02213751]]\n",
      "the iterations of  48400 the loss is 0.4685670386927033 theta= [[-2.77520632]\n",
      " [ 0.03016165]\n",
      " [ 0.02217439]]\n",
      "the iterations of  48500 the loss is 0.468347175969881 theta= [[-2.77989536]\n",
      " [ 0.03019586]\n",
      " [ 0.02221125]]\n",
      "the iterations of  48600 the loss is 0.46812764234832477 theta= [[-2.7845809 ]\n",
      " [ 0.03023005]\n",
      " [ 0.02224807]]\n",
      "the iterations of  48700 the loss is 0.467908437173698 theta= [[-2.78926293]\n",
      " [ 0.03026421]\n",
      " [ 0.02228487]]\n",
      "the iterations of  48800 the loss is 0.46768955979309085 theta= [[-2.79394145]\n",
      " [ 0.03029835]\n",
      " [ 0.02232165]]\n",
      "the iterations of  48900 the loss is 0.46747100955501814 theta= [[-2.79861648]\n",
      " [ 0.03033247]\n",
      " [ 0.02235839]]\n",
      "the iterations of  49000 the loss is 0.4672527858094169 theta= [[-2.80328802]\n",
      " [ 0.03036656]\n",
      " [ 0.02239511]]\n",
      "the iterations of  49100 the loss is 0.46703488790764314 theta= [[-2.80795606]\n",
      " [ 0.03040064]\n",
      " [ 0.0224318 ]]\n",
      "the iterations of  49200 the loss is 0.4668173152024695 theta= [[-2.81262062]\n",
      " [ 0.03043469]\n",
      " [ 0.02246846]]\n",
      "the iterations of  49300 the loss is 0.4666000670480828 theta= [[-2.8172817 ]\n",
      " [ 0.03046871]\n",
      " [ 0.0225051 ]]\n",
      "the iterations of  49400 the loss is 0.4663831428000815 theta= [[-2.82193931]\n",
      " [ 0.03050272]\n",
      " [ 0.02254171]]\n",
      "the iterations of  49500 the loss is 0.46616654181547196 theta= [[-2.82659344]\n",
      " [ 0.0305367 ]\n",
      " [ 0.02257829]]\n",
      "the iterations of  49600 the loss is 0.46595026345266777 theta= [[-2.83124411]\n",
      " [ 0.03057066]\n",
      " [ 0.02261484]]\n",
      "the iterations of  49700 the loss is 0.46573430707148483 theta= [[-2.83589131]\n",
      " [ 0.0306046 ]\n",
      " [ 0.02265137]]\n",
      "the iterations of  49800 the loss is 0.46551867203314096 theta= [[-2.84053505]\n",
      " [ 0.03063851]\n",
      " [ 0.02268787]]\n",
      "the iterations of  49900 the loss is 0.4653033577002518 theta= [[-2.84517534]\n",
      " [ 0.03067241]\n",
      " [ 0.02272435]]\n",
      "the iterations of  50000 the loss is 0.46508836343682847 theta= [[-2.84981218]\n",
      " [ 0.03070628]\n",
      " [ 0.02276079]]\n",
      "the iterations of  50100 the loss is 0.46487368860827516 theta= [[-2.85444557]\n",
      " [ 0.03074012]\n",
      " [ 0.02279721]]\n",
      "the iterations of  50200 the loss is 0.46465933258138664 theta= [[-2.85907552]\n",
      " [ 0.03077395]\n",
      " [ 0.02283361]]\n",
      "the iterations of  50300 the loss is 0.4644452947243451 theta= [[-2.86370203]\n",
      " [ 0.03080775]\n",
      " [ 0.02286997]]\n",
      "the iterations of  50400 the loss is 0.46423157440671803 theta= [[-2.86832511]\n",
      " [ 0.03084153]\n",
      " [ 0.02290631]]\n",
      "the iterations of  50500 the loss is 0.46401817099945464 theta= [[-2.87294477]\n",
      " [ 0.03087529]\n",
      " [ 0.02294262]]\n",
      "the iterations of  50600 the loss is 0.46380508387488534 theta= [[-2.87756099]\n",
      " [ 0.03090903]\n",
      " [ 0.02297891]]\n",
      "the iterations of  50700 the loss is 0.46359231240671633 theta= [[-2.8821738 ]\n",
      " [ 0.03094275]\n",
      " [ 0.02301517]]\n",
      "the iterations of  50800 the loss is 0.4633798559700293 theta= [[-2.88678319]\n",
      " [ 0.03097644]\n",
      " [ 0.0230514 ]]\n",
      "the iterations of  50900 the loss is 0.4631677139412774 theta= [[-2.89138916]\n",
      " [ 0.03101011]\n",
      " [ 0.0230876 ]]\n",
      "the iterations of  51000 the loss is 0.46295588569828333 theta= [[-2.89599173]\n",
      " [ 0.03104376]\n",
      " [ 0.02312378]]\n",
      "the iterations of  51100 the loss is 0.46274437062023616 theta= [[-2.9005909 ]\n",
      " [ 0.03107738]\n",
      " [ 0.02315994]]\n",
      "the iterations of  51200 the loss is 0.4625331680876893 theta= [[-2.90518667]\n",
      " [ 0.03111099]\n",
      " [ 0.02319606]]\n",
      "the iterations of  51300 the loss is 0.46232227748255744 theta= [[-2.90977904]\n",
      " [ 0.03114457]\n",
      " [ 0.02323216]]\n",
      "the iterations of  51400 the loss is 0.4621116981881137 theta= [[-2.91436802]\n",
      " [ 0.03117813]\n",
      " [ 0.02326823]]\n",
      "the iterations of  51500 the loss is 0.46190142958898783 theta= [[-2.91895361]\n",
      " [ 0.03121167]\n",
      " [ 0.02330428]]\n",
      "the iterations of  51600 the loss is 0.46169147107116293 theta= [[-2.92353582]\n",
      " [ 0.03124519]\n",
      " [ 0.0233403 ]]\n",
      "the iterations of  51700 the loss is 0.46148182202197296 theta= [[-2.92811466]\n",
      " [ 0.03127868]\n",
      " [ 0.02337629]]\n",
      "the iterations of  51800 the loss is 0.46127248183010033 theta= [[-2.93269012]\n",
      " [ 0.03131215]\n",
      " [ 0.02341226]]\n",
      "the iterations of  51900 the loss is 0.4610634498855724 theta= [[-2.9372622]\n",
      " [ 0.0313456]\n",
      " [ 0.0234482]]\n",
      "the iterations of  52000 the loss is 0.4608547255797606 theta= [[-2.94183093]\n",
      " [ 0.03137903]\n",
      " [ 0.02348411]]\n",
      "the iterations of  52100 the loss is 0.4606463083053757 theta= [[-2.94639629]\n",
      " [ 0.03141244]\n",
      " [ 0.02352   ]]\n",
      "the iterations of  52200 the loss is 0.46043819745646664 theta= [[-2.95095829]\n",
      " [ 0.03144582]\n",
      " [ 0.02355586]]\n",
      "the iterations of  52300 the loss is 0.46023039242841796 theta= [[-2.95551695]\n",
      " [ 0.03147919]\n",
      " [ 0.0235917 ]]\n",
      "the iterations of  52400 the loss is 0.46002289261794616 theta= [[-2.96007225]\n",
      " [ 0.03151253]\n",
      " [ 0.02362751]]\n",
      "the iterations of  52500 the loss is 0.4598156974230975 theta= [[-2.9646242 ]\n",
      " [ 0.03154585]\n",
      " [ 0.02366329]]\n",
      "the iterations of  52600 the loss is 0.4596088062432458 theta= [[-2.96917282]\n",
      " [ 0.03157915]\n",
      " [ 0.02369905]]\n",
      "the iterations of  52700 the loss is 0.4594022184790896 theta= [[-2.9737181 ]\n",
      " [ 0.03161242]\n",
      " [ 0.02373478]]\n",
      "the iterations of  52800 the loss is 0.45919593353264937 theta= [[-2.97826005]\n",
      " [ 0.03164568]\n",
      " [ 0.02377048]]\n",
      "the iterations of  52900 the loss is 0.4589899508072646 theta= [[-2.98279866]\n",
      " [ 0.03167891]\n",
      " [ 0.02380616]]\n",
      "the iterations of  53000 the loss is 0.45878426970759234 theta= [[-2.98733396]\n",
      " [ 0.03171212]\n",
      " [ 0.02384181]]\n",
      "the iterations of  53100 the loss is 0.45857888963960325 theta= [[-2.99186593]\n",
      " [ 0.03174531]\n",
      " [ 0.02387744]]\n",
      "the iterations of  53200 the loss is 0.4583738100105794 theta= [[-2.99639459]\n",
      " [ 0.03177848]\n",
      " [ 0.02391304]]\n",
      "the iterations of  53300 the loss is 0.45816903022911254 theta= [[-3.00091994]\n",
      " [ 0.03181163]\n",
      " [ 0.02394862]]\n",
      "the iterations of  53400 the loss is 0.45796454970509987 theta= [[-3.00544197]\n",
      " [ 0.03184475]\n",
      " [ 0.02398416]]\n",
      "the iterations of  53500 the loss is 0.4577603678497431 theta= [[-3.00996071]\n",
      " [ 0.03187786]\n",
      " [ 0.02401969]]\n",
      "the iterations of  53600 the loss is 0.45755648407554445 theta= [[-3.01447614]\n",
      " [ 0.03191094]\n",
      " [ 0.02405518]]\n",
      "the iterations of  53700 the loss is 0.4573528977963047 theta= [[-3.01898828]\n",
      " [ 0.031944  ]\n",
      " [ 0.02409066]]\n",
      "the iterations of  53800 the loss is 0.4571496084271211 theta= [[-3.02349713]\n",
      " [ 0.03197704]\n",
      " [ 0.0241261 ]]\n",
      "the iterations of  53900 the loss is 0.45694661538438325 theta= [[-3.02800269]\n",
      " [ 0.03201006]\n",
      " [ 0.02416152]]\n",
      "the iterations of  54000 the loss is 0.45674391808577247 theta= [[-3.03250496]\n",
      " [ 0.03204306]\n",
      " [ 0.02419692]]\n",
      "the iterations of  54100 the loss is 0.4565415159502575 theta= [[-3.03700396]\n",
      " [ 0.03207603]\n",
      " [ 0.02423228]]\n",
      "the iterations of  54200 the loss is 0.4563394083980927 theta= [[-3.04149968]\n",
      " [ 0.03210899]\n",
      " [ 0.02426763]]\n",
      "the iterations of  54300 the loss is 0.4561375948508154 theta= [[-3.04599213]\n",
      " [ 0.03214192]\n",
      " [ 0.02430294]]\n",
      "the iterations of  54400 the loss is 0.45593607473124265 theta= [[-3.05048132]\n",
      " [ 0.03217483]\n",
      " [ 0.02433824]]\n",
      "the iterations of  54500 the loss is 0.45573484746347015 theta= [[-3.05496724]\n",
      " [ 0.03220772]\n",
      " [ 0.0243735 ]]\n",
      "the iterations of  54600 the loss is 0.4555339124728679 theta= [[-3.0594499 ]\n",
      " [ 0.03224059]\n",
      " [ 0.02440874]]\n",
      "the iterations of  54700 the loss is 0.4553332691860789 theta= [[-3.0639293 ]\n",
      " [ 0.03227344]\n",
      " [ 0.02444396]]\n",
      "the iterations of  54800 the loss is 0.4551329170310156 theta= [[-3.06840546]\n",
      " [ 0.03230626]\n",
      " [ 0.02447915]]\n",
      "the iterations of  54900 the loss is 0.4549328554368584 theta= [[-3.07287837]\n",
      " [ 0.03233907]\n",
      " [ 0.02451431]]\n",
      "the iterations of  55000 the loss is 0.45473308383405153 theta= [[-3.07734803]\n",
      " [ 0.03237185]\n",
      " [ 0.02454945]]\n",
      "the iterations of  55100 the loss is 0.4545336016543019 theta= [[-3.08181446]\n",
      " [ 0.03240462]\n",
      " [ 0.02458457]]\n",
      "the iterations of  55200 the loss is 0.4543344083305765 theta= [[-3.08627765]\n",
      " [ 0.03243736]\n",
      " [ 0.02461965]]\n",
      "the iterations of  55300 the loss is 0.4541355032970978 theta= [[-3.09073761]\n",
      " [ 0.03247008]\n",
      " [ 0.02465472]]\n",
      "the iterations of  55400 the loss is 0.45393688598934434 theta= [[-3.09519434]\n",
      " [ 0.03250278]\n",
      " [ 0.02468975]]\n",
      "the iterations of  55500 the loss is 0.4537385558440453 theta= [[-3.09964785]\n",
      " [ 0.03253546]\n",
      " [ 0.02472477]]\n",
      "the iterations of  55600 the loss is 0.4535405122991798 theta= [[-3.10409814]\n",
      " [ 0.03256811]\n",
      " [ 0.02475975]]\n",
      "the iterations of  55700 the loss is 0.45334275479397285 theta= [[-3.10854522]\n",
      " [ 0.03260075]\n",
      " [ 0.02479472]]\n",
      "the iterations of  55800 the loss is 0.4531452827688943 theta= [[-3.11298908]\n",
      " [ 0.03263337]\n",
      " [ 0.02482965]]\n",
      "the iterations of  55900 the loss is 0.452948095665655 theta= [[-3.11742974]\n",
      " [ 0.03266596]\n",
      " [ 0.02486456]]\n",
      "the iterations of  56000 the loss is 0.4527511929272052 theta= [[-3.12186719]\n",
      " [ 0.03269854]\n",
      " [ 0.02489945]]\n",
      "the iterations of  56100 the loss is 0.45255457399773114 theta= [[-3.12630145]\n",
      " [ 0.03273109]\n",
      " [ 0.02493431]]\n",
      "the iterations of  56200 the loss is 0.45235823832265304 theta= [[-3.13073251]\n",
      " [ 0.03276362]\n",
      " [ 0.02496915]]\n",
      "the iterations of  56300 the loss is 0.45216218534862235 theta= [[-3.13516038]\n",
      " [ 0.03279613]\n",
      " [ 0.02500396]]\n",
      "the iterations of  56400 the loss is 0.45196641452351893 theta= [[-3.13958506]\n",
      " [ 0.03282862]\n",
      " [ 0.02503875]]\n",
      "the iterations of  56500 the loss is 0.45177092529644924 theta= [[-3.14400655]\n",
      " [ 0.03286109]\n",
      " [ 0.02507351]]\n",
      "the iterations of  56600 the loss is 0.4515757171177433 theta= [[-3.14842487]\n",
      " [ 0.03289354]\n",
      " [ 0.02510825]]\n",
      "the iterations of  56700 the loss is 0.45138078943895166 theta= [[-3.15284001]\n",
      " [ 0.03292597]\n",
      " [ 0.02514296]]\n",
      "the iterations of  56800 the loss is 0.45118614171284377 theta= [[-3.15725198]\n",
      " [ 0.03295838]\n",
      " [ 0.02517765]]\n",
      "the iterations of  56900 the loss is 0.45099177339340474 theta= [[-3.16166079]\n",
      " [ 0.03299076]\n",
      " [ 0.02521231]]\n",
      "the iterations of  57000 the loss is 0.45079768393583314 theta= [[-3.16606643]\n",
      " [ 0.03302313]\n",
      " [ 0.02524695]]\n",
      "the iterations of  57100 the loss is 0.45060387279653874 theta= [[-3.1704689 ]\n",
      " [ 0.03305547]\n",
      " [ 0.02528156]]\n",
      "the iterations of  57200 the loss is 0.45041033943313896 theta= [[-3.17486823]\n",
      " [ 0.0330878 ]\n",
      " [ 0.02531615]]\n",
      "the iterations of  57300 the loss is 0.45021708330445726 theta= [[-3.1792644 ]\n",
      " [ 0.0331201 ]\n",
      " [ 0.02535071]]\n",
      "the iterations of  57400 the loss is 0.45002410387052033 theta= [[-3.18365742]\n",
      " [ 0.03315238]\n",
      " [ 0.02538525]]\n",
      "the iterations of  57500 the loss is 0.44983140059255555 theta= [[-3.1880473 ]\n",
      " [ 0.03318465]\n",
      " [ 0.02541976]]\n",
      "the iterations of  57600 the loss is 0.4496389729329885 theta= [[-3.19243403]\n",
      " [ 0.03321689]\n",
      " [ 0.02545425]]\n",
      "the iterations of  57700 the loss is 0.4494468203554402 theta= [[-3.19681763]\n",
      " [ 0.03324911]\n",
      " [ 0.02548872]]\n",
      "the iterations of  57800 the loss is 0.4492549423247245 theta= [[-3.2011981 ]\n",
      " [ 0.03328131]\n",
      " [ 0.02552316]]\n",
      "the iterations of  57900 the loss is 0.4490633383068468 theta= [[-3.20557544]\n",
      " [ 0.03331349]\n",
      " [ 0.02555757]]\n",
      "the iterations of  58000 the loss is 0.44887200776899916 theta= [[-3.20994965]\n",
      " [ 0.03334565]\n",
      " [ 0.02559196]]\n",
      "the iterations of  58100 the loss is 0.4486809501795603 theta= [[-3.21432075]\n",
      " [ 0.03337779]\n",
      " [ 0.02562633]]\n",
      "the iterations of  58200 the loss is 0.44849016500809136 theta= [[-3.21868872]\n",
      " [ 0.03340991]\n",
      " [ 0.02566067]]\n",
      "the iterations of  58300 the loss is 0.44829965172533426 theta= [[-3.22305358]\n",
      " [ 0.03344201]\n",
      " [ 0.02569499]]\n",
      "the iterations of  58400 the loss is 0.44810940980320824 theta= [[-3.22741533]\n",
      " [ 0.03347409]\n",
      " [ 0.02572929]]\n",
      "the iterations of  58500 the loss is 0.4479194387148089 theta= [[-3.23177398]\n",
      " [ 0.03350615]\n",
      " [ 0.02576356]]\n",
      "the iterations of  58600 the loss is 0.4477297379344045 theta= [[-3.23612952]\n",
      " [ 0.03353818]\n",
      " [ 0.0257978 ]]\n",
      "the iterations of  58700 the loss is 0.44754030693743324 theta= [[-3.24048197]\n",
      " [ 0.0335702 ]\n",
      " [ 0.02583202]]\n",
      "the iterations of  58800 the loss is 0.44735114520050195 theta= [[-3.24483132]\n",
      " [ 0.0336022 ]\n",
      " [ 0.02586622]]\n",
      "the iterations of  58900 the loss is 0.4471622522013826 theta= [[-3.24917758]\n",
      " [ 0.03363418]\n",
      " [ 0.02590039]]\n",
      "the iterations of  59000 the loss is 0.44697362741901014 theta= [[-3.25352075]\n",
      " [ 0.03366613]\n",
      " [ 0.02593454]]\n",
      "the iterations of  59100 the loss is 0.44678527033348003 theta= [[-3.25786084]\n",
      " [ 0.03369807]\n",
      " [ 0.02596866]]\n",
      "the iterations of  59200 the loss is 0.4465971804260457 theta= [[-3.26219785]\n",
      " [ 0.03372998]\n",
      " [ 0.02600276]]\n",
      "the iterations of  59300 the loss is 0.44640935717911573 theta= [[-3.26653178]\n",
      " [ 0.03376188]\n",
      " [ 0.02603684]]\n",
      "the iterations of  59400 the loss is 0.44622180007625256 theta= [[-3.27086265]\n",
      " [ 0.03379376]\n",
      " [ 0.02607089]]\n",
      "the iterations of  59500 the loss is 0.44603450860216826 theta= [[-3.27519044]\n",
      " [ 0.03382561]\n",
      " [ 0.02610492]]\n",
      "the iterations of  59600 the loss is 0.4458474822427233 theta= [[-3.27951517]\n",
      " [ 0.03385745]\n",
      " [ 0.02613892]]\n",
      "the iterations of  59700 the loss is 0.44566072048492394 theta= [[-3.28383684]\n",
      " [ 0.03388926]\n",
      " [ 0.0261729 ]]\n",
      "the iterations of  59800 the loss is 0.44547422281691906 theta= [[-3.28815546]\n",
      " [ 0.03392106]\n",
      " [ 0.02620686]]\n",
      "the iterations of  59900 the loss is 0.44528798872799863 theta= [[-3.29247102]\n",
      " [ 0.03395283]\n",
      " [ 0.02624079]]\n",
      "the iterations of  60000 the loss is 0.4451020177085903 theta= [[-3.29678353]\n",
      " [ 0.03398459]\n",
      " [ 0.0262747 ]]\n",
      "the iterations of  60100 the loss is 0.44491630925025816 theta= [[-3.301093  ]\n",
      " [ 0.03401632]\n",
      " [ 0.02630858]]\n",
      "the iterations of  60200 the loss is 0.44473086284569857 theta= [[-3.30539942]\n",
      " [ 0.03404804]\n",
      " [ 0.02634244]]\n",
      "the iterations of  60300 the loss is 0.4445456779887395 theta= [[-3.30970281]\n",
      " [ 0.03407973]\n",
      " [ 0.02637628]]\n",
      "the iterations of  60400 the loss is 0.44436075417433685 theta= [[-3.31400316]\n",
      " [ 0.03411141]\n",
      " [ 0.02641009]]\n",
      "the iterations of  60500 the loss is 0.4441760908985725 theta= [[-3.31830048]\n",
      " [ 0.03414306]\n",
      " [ 0.02644388]]\n",
      "the iterations of  60600 the loss is 0.44399168765865155 theta= [[-3.32259478]\n",
      " [ 0.0341747 ]\n",
      " [ 0.02647764]]\n",
      "the iterations of  60700 the loss is 0.4438075439529004 theta= [[-3.32688605]\n",
      " [ 0.03420631]\n",
      " [ 0.02651138]]\n",
      "the iterations of  60800 the loss is 0.4436236592807638 theta= [[-3.3311743 ]\n",
      " [ 0.03423791]\n",
      " [ 0.0265451 ]]\n",
      "the iterations of  60900 the loss is 0.4434400331428025 theta= [[-3.33545954]\n",
      " [ 0.03426949]\n",
      " [ 0.0265788 ]]\n",
      "the iterations of  61000 the loss is 0.44325666504069133 theta= [[-3.33974176]\n",
      " [ 0.03430104]\n",
      " [ 0.02661247]]\n",
      "the iterations of  61100 the loss is 0.4430735544772159 theta= [[-3.34402098]\n",
      " [ 0.03433258]\n",
      " [ 0.02664611]]\n",
      "the iterations of  61200 the loss is 0.442890700956271 theta= [[-3.34829719]\n",
      " [ 0.03436409]\n",
      " [ 0.02667974]]\n",
      "the iterations of  61300 the loss is 0.4427081039828572 theta= [[-3.3525704 ]\n",
      " [ 0.03439559]\n",
      " [ 0.02671334]]\n",
      "the iterations of  61400 the loss is 0.4425257630630797 theta= [[-3.35684061]\n",
      " [ 0.03442707]\n",
      " [ 0.02674691]]\n",
      "the iterations of  61500 the loss is 0.44234367770414484 theta= [[-3.36110783]\n",
      " [ 0.03445852]\n",
      " [ 0.02678047]]\n",
      "the iterations of  61600 the loss is 0.4421618474143585 theta= [[-3.36537206]\n",
      " [ 0.03448996]\n",
      " [ 0.026814  ]]\n",
      "the iterations of  61700 the loss is 0.4419802717031225 theta= [[-3.36963331]\n",
      " [ 0.03452138]\n",
      " [ 0.0268475 ]]\n",
      "the iterations of  61800 the loss is 0.44179895008093373 theta= [[-3.37389157]\n",
      " [ 0.03455278]\n",
      " [ 0.02688098]]\n",
      "the iterations of  61900 the loss is 0.4416178820593805 theta= [[-3.37814685]\n",
      " [ 0.03458416]\n",
      " [ 0.02691444]]\n",
      "the iterations of  62000 the loss is 0.44143706715114095 theta= [[-3.38239916]\n",
      " [ 0.03461551]\n",
      " [ 0.02694788]]\n",
      "the iterations of  62100 the loss is 0.44125650486997986 theta= [[-3.38664849]\n",
      " [ 0.03464685]\n",
      " [ 0.02698129]]\n",
      "the iterations of  62200 the loss is 0.4410761947307471 theta= [[-3.39089486]\n",
      " [ 0.03467817]\n",
      " [ 0.02701468]]\n",
      "the iterations of  62300 the loss is 0.4408961362493747 theta= [[-3.39513826]\n",
      " [ 0.03470947]\n",
      " [ 0.02704805]]\n",
      "the iterations of  62400 the loss is 0.4407163289428744 theta= [[-3.39937871]\n",
      " [ 0.03474075]\n",
      " [ 0.02708139]]\n",
      "the iterations of  62500 the loss is 0.44053677232933597 theta= [[-3.40361619]\n",
      " [ 0.03477202]\n",
      " [ 0.02711471]]\n",
      "the iterations of  62600 the loss is 0.4403574659279238 theta= [[-3.40785072]\n",
      " [ 0.03480326]\n",
      " [ 0.02714801]]\n",
      "the iterations of  62700 the loss is 0.44017840925887514 theta= [[-3.4120823 ]\n",
      " [ 0.03483448]\n",
      " [ 0.02718128]]\n",
      "the iterations of  62800 the loss is 0.4399996018434976 theta= [[-3.41631094]\n",
      " [ 0.03486568]\n",
      " [ 0.02721453]]\n",
      "the iterations of  62900 the loss is 0.43982104320416715 theta= [[-3.42053663]\n",
      " [ 0.03489687]\n",
      " [ 0.02724776]]\n",
      "the iterations of  63000 the loss is 0.43964273286432504 theta= [[-3.42475938]\n",
      " [ 0.03492803]\n",
      " [ 0.02728096]]\n",
      "the iterations of  63100 the loss is 0.439464670348476 theta= [[-3.4289792 ]\n",
      " [ 0.03495917]\n",
      " [ 0.02731414]]\n",
      "the iterations of  63200 the loss is 0.4392868551821855 theta= [[-3.43319608]\n",
      " [ 0.0349903 ]\n",
      " [ 0.0273473 ]]\n",
      "the iterations of  63300 the loss is 0.43910928689207746 theta= [[-3.43741004]\n",
      " [ 0.0350214 ]\n",
      " [ 0.02738044]]\n",
      "the iterations of  63400 the loss is 0.4389319650058323 theta= [[-3.44162107]\n",
      " [ 0.03505249]\n",
      " [ 0.02741355]]\n",
      "the iterations of  63500 the loss is 0.4387548890521841 theta= [[-3.44582918]\n",
      " [ 0.03508356]\n",
      " [ 0.02744664]]\n",
      "the iterations of  63600 the loss is 0.4385780585609187 theta= [[-3.45003438]\n",
      " [ 0.03511461]\n",
      " [ 0.0274797 ]]\n",
      "the iterations of  63700 the loss is 0.43840147306287036 theta= [[-3.45423665]\n",
      " [ 0.03514563]\n",
      " [ 0.02751275]]\n",
      "the iterations of  63800 the loss is 0.438225132089921 theta= [[-3.45843602]\n",
      " [ 0.03517664]\n",
      " [ 0.02754577]]\n",
      "the iterations of  63900 the loss is 0.4380490351749966 theta= [[-3.46263248]\n",
      " [ 0.03520763]\n",
      " [ 0.02757877]]\n",
      "the iterations of  64000 the loss is 0.4378731818520653 theta= [[-3.46682604]\n",
      " [ 0.03523861]\n",
      " [ 0.02761174]]\n",
      "the iterations of  64100 the loss is 0.437697571656135 theta= [[-3.4710167 ]\n",
      " [ 0.03526956]\n",
      " [ 0.02764469]]\n",
      "the iterations of  64200 the loss is 0.4375222041232512 theta= [[-3.47520446]\n",
      " [ 0.03530049]\n",
      " [ 0.02767762]]\n",
      "the iterations of  64300 the loss is 0.4373470787904945 theta= [[-3.47938933]\n",
      " [ 0.0353314 ]\n",
      " [ 0.02771053]]\n",
      "the iterations of  64400 the loss is 0.4371721951959783 theta= [[-3.48357131]\n",
      " [ 0.0353623 ]\n",
      " [ 0.02774341]]\n",
      "the iterations of  64500 the loss is 0.4369975528788464 theta= [[-3.4877504 ]\n",
      " [ 0.03539317]\n",
      " [ 0.02777628]]\n",
      "the iterations of  64600 the loss is 0.43682315137927136 theta= [[-3.49192661]\n",
      " [ 0.03542403]\n",
      " [ 0.02780912]]\n",
      "the iterations of  64700 the loss is 0.4366489902384507 theta= [[-3.49609994]\n",
      " [ 0.03545487]\n",
      " [ 0.02784193]]\n",
      "the iterations of  64800 the loss is 0.43647506899860644 theta= [[-3.5002704 ]\n",
      " [ 0.03548568]\n",
      " [ 0.02787473]]\n",
      "the iterations of  64900 the loss is 0.43630138720298134 theta= [[-3.50443798]\n",
      " [ 0.03551648]\n",
      " [ 0.0279075 ]]\n",
      "the iterations of  65000 the loss is 0.4361279443958371 theta= [[-3.5086027 ]\n",
      " [ 0.03554726]\n",
      " [ 0.02794025]]\n",
      "the iterations of  65100 the loss is 0.4359547401224525 theta= [[-3.51276455]\n",
      " [ 0.03557803]\n",
      " [ 0.02797297]]\n",
      "the iterations of  65200 the loss is 0.4357817739291203 theta= [[-3.51692354]\n",
      " [ 0.03560877]\n",
      " [ 0.02800568]]\n",
      "the iterations of  65300 the loss is 0.43560904536314604 theta= [[-3.52107967]\n",
      " [ 0.03563949]\n",
      " [ 0.02803836]]\n",
      "the iterations of  65400 the loss is 0.4354365539728442 theta= [[-3.52523294]\n",
      " [ 0.03567019]\n",
      " [ 0.02807102]]\n",
      "the iterations of  65500 the loss is 0.4352642993075373 theta= [[-3.52938337]\n",
      " [ 0.03570088]\n",
      " [ 0.02810365]]\n",
      "the iterations of  65600 the loss is 0.43509228091755325 theta= [[-3.53353095]\n",
      " [ 0.03573155]\n",
      " [ 0.02813627]]\n",
      "the iterations of  65700 the loss is 0.4349204983542228 theta= [[-3.53767568]\n",
      " [ 0.03576219]\n",
      " [ 0.02816886]]\n",
      "the iterations of  65800 the loss is 0.43474895116987716 theta= [[-3.54181758]\n",
      " [ 0.03579282]\n",
      " [ 0.02820143]]\n",
      "the iterations of  65900 the loss is 0.4345776389178467 theta= [[-3.54595663]\n",
      " [ 0.03582343]\n",
      " [ 0.02823398]]\n",
      "the iterations of  66000 the loss is 0.43440656115245724 theta= [[-3.55009286]\n",
      " [ 0.03585403]\n",
      " [ 0.0282665 ]]\n",
      "the iterations of  66100 the loss is 0.4342357174290289 theta= [[-3.55422625]\n",
      " [ 0.0358846 ]\n",
      " [ 0.02829901]]\n",
      "the iterations of  66200 the loss is 0.4340651073038735 theta= [[-3.55835681]\n",
      " [ 0.03591515]\n",
      " [ 0.02833149]]\n",
      "the iterations of  66300 the loss is 0.43389473033429204 theta= [[-3.56248456]\n",
      " [ 0.03594569]\n",
      " [ 0.02836395]]\n",
      "the iterations of  66400 the loss is 0.4337245860785732 theta= [[-3.56660948]\n",
      " [ 0.0359762 ]\n",
      " [ 0.02839638]]\n",
      "the iterations of  66500 the loss is 0.4335546740959903 theta= [[-3.57073158]\n",
      " [ 0.0360067 ]\n",
      " [ 0.0284288 ]]\n",
      "the iterations of  66600 the loss is 0.4333849939467994 theta= [[-3.57485088]\n",
      " [ 0.03603718]\n",
      " [ 0.02846119]]\n",
      "the iterations of  66700 the loss is 0.433215545192237 theta= [[-3.57896736]\n",
      " [ 0.03606764]\n",
      " [ 0.02849356]]\n",
      "the iterations of  66800 the loss is 0.43304632739451804 theta= [[-3.58308103]\n",
      " [ 0.03609808]\n",
      " [ 0.02852591]]\n",
      "the iterations of  66900 the loss is 0.43287734011683326 theta= [[-3.58719191]\n",
      " [ 0.03612851]\n",
      " [ 0.02855824]]\n",
      "the iterations of  67000 the loss is 0.43270858292334746 theta= [[-3.59129998]\n",
      " [ 0.03615891]\n",
      " [ 0.02859054]]\n",
      "the iterations of  67100 the loss is 0.43254005537919665 theta= [[-3.59540526]\n",
      " [ 0.0361893 ]\n",
      " [ 0.02862283]]\n",
      "the iterations of  67200 the loss is 0.4323717570504862 theta= [[-3.59950774]\n",
      " [ 0.03621966]\n",
      " [ 0.02865509]]\n",
      "the iterations of  67300 the loss is 0.4322036875042891 theta= [[-3.60360744]\n",
      " [ 0.03625001]\n",
      " [ 0.02868733]]\n",
      "the iterations of  67400 the loss is 0.432035846308643 theta= [[-3.60770435]\n",
      " [ 0.03628034]\n",
      " [ 0.02871954]]\n",
      "the iterations of  67500 the loss is 0.4318682330325481 theta= [[-3.61179847]\n",
      " [ 0.03631066]\n",
      " [ 0.02875174]]\n",
      "the iterations of  67600 the loss is 0.4317008472459654 theta= [[-3.61588982]\n",
      " [ 0.03634095]\n",
      " [ 0.02878391]]\n",
      "the iterations of  67700 the loss is 0.4315336885198144 theta= [[-3.61997839]\n",
      " [ 0.03637122]\n",
      " [ 0.02881606]]\n",
      "the iterations of  67800 the loss is 0.43136675642597 theta= [[-3.62406419]\n",
      " [ 0.03640148]\n",
      " [ 0.02884819]]\n",
      "the iterations of  67900 the loss is 0.4312000505372617 theta= [[-3.62814722]\n",
      " [ 0.03643172]\n",
      " [ 0.0288803 ]]\n",
      "the iterations of  68000 the loss is 0.4310335704274707 theta= [[-3.63222748]\n",
      " [ 0.03646194]\n",
      " [ 0.02891239]]\n",
      "the iterations of  68100 the loss is 0.4308673156713276 theta= [[-3.63630498]\n",
      " [ 0.03649214]\n",
      " [ 0.02894445]]\n",
      "the iterations of  68200 the loss is 0.4307012858445108 theta= [[-3.64037972]\n",
      " [ 0.03652232]\n",
      " [ 0.0289765 ]]\n",
      "the iterations of  68300 the loss is 0.4305354805236432 theta= [[-3.64445171]\n",
      " [ 0.03655249]\n",
      " [ 0.02900852]]\n",
      "the iterations of  68400 the loss is 0.4303698992862916 theta= [[-3.64852094]\n",
      " [ 0.03658263]\n",
      " [ 0.02904052]]\n",
      "the iterations of  68500 the loss is 0.4302045417109629 theta= [[-3.65258742]\n",
      " [ 0.03661276]\n",
      " [ 0.02907249]]\n",
      "the iterations of  68600 the loss is 0.4300394073771035 theta= [[-3.65665116]\n",
      " [ 0.03664287]\n",
      " [ 0.02910445]]\n",
      "the iterations of  68700 the loss is 0.4298744958650959 theta= [[-3.66071215]\n",
      " [ 0.03667296]\n",
      " [ 0.02913639]]\n",
      "the iterations of  68800 the loss is 0.42970980675625703 theta= [[-3.66477041]\n",
      " [ 0.03670304]\n",
      " [ 0.0291683 ]]\n",
      "the iterations of  68900 the loss is 0.4295453396328361 theta= [[-3.66882593]\n",
      " [ 0.03673309]\n",
      " [ 0.02920019]]\n",
      "the iterations of  69000 the loss is 0.42938109407801284 theta= [[-3.67287871]\n",
      " [ 0.03676313]\n",
      " [ 0.02923206]]\n",
      "the iterations of  69100 the loss is 0.42921706967589457 theta= [[-3.67692877]\n",
      " [ 0.03679315]\n",
      " [ 0.02926391]]\n",
      "the iterations of  69200 the loss is 0.42905326601151444 theta= [[-3.6809761 ]\n",
      " [ 0.03682315]\n",
      " [ 0.02929574]]\n",
      "the iterations of  69300 the loss is 0.4288896826708295 theta= [[-3.68502071]\n",
      " [ 0.03685313]\n",
      " [ 0.02932755]]\n",
      "the iterations of  69400 the loss is 0.42872631924071825 theta= [[-3.6890626 ]\n",
      " [ 0.0368831 ]\n",
      " [ 0.02935933]]\n",
      "the iterations of  69500 the loss is 0.4285631753089789 theta= [[-3.69310177]\n",
      " [ 0.03691304]\n",
      " [ 0.0293911 ]]\n",
      "the iterations of  69600 the loss is 0.42840025046432634 theta= [[-3.69713823]\n",
      " [ 0.03694297]\n",
      " [ 0.02942284]]\n",
      "the iterations of  69700 the loss is 0.4282375442963915 theta= [[-3.70117197]\n",
      " [ 0.03697288]\n",
      " [ 0.02945456]]\n",
      "the iterations of  69800 the loss is 0.42807505639571786 theta= [[-3.70520302]\n",
      " [ 0.03700277]\n",
      " [ 0.02948626]]\n",
      "the iterations of  69900 the loss is 0.42791278635376 theta= [[-3.70923136]\n",
      " [ 0.03703265]\n",
      " [ 0.02951794]]\n",
      "the iterations of  70000 the loss is 0.42775073376288125 theta= [[-3.71325699]\n",
      " [ 0.0370625 ]\n",
      " [ 0.0295496 ]]\n",
      "the iterations of  70100 the loss is 0.42758889821635215 theta= [[-3.71727994]\n",
      " [ 0.03709234]\n",
      " [ 0.02958124]]\n",
      "the iterations of  70200 the loss is 0.42742727930834745 theta= [[-3.72130018]\n",
      " [ 0.03712216]\n",
      " [ 0.02961285]]\n",
      "the iterations of  70300 the loss is 0.42726587663394466 theta= [[-3.72531774]\n",
      " [ 0.03715196]\n",
      " [ 0.02964445]]\n",
      "the iterations of  70400 the loss is 0.42710468978912175 theta= [[-3.72933261]\n",
      " [ 0.03718175]\n",
      " [ 0.02967602]]\n",
      "the iterations of  70500 the loss is 0.42694371837075507 theta= [[-3.7333448 ]\n",
      " [ 0.03721151]\n",
      " [ 0.02970757]]\n",
      "the iterations of  70600 the loss is 0.42678296197661736 theta= [[-3.73735431]\n",
      " [ 0.03724126]\n",
      " [ 0.0297391 ]]\n",
      "the iterations of  70700 the loss is 0.4266224202053755 theta= [[-3.74136113]\n",
      " [ 0.03727099]\n",
      " [ 0.02977061]]\n",
      "the iterations of  70800 the loss is 0.42646209265658863 theta= [[-3.74536529]\n",
      " [ 0.03730071]\n",
      " [ 0.0298021 ]]\n",
      "the iterations of  70900 the loss is 0.42630197893070565 theta= [[-3.74936677]\n",
      " [ 0.0373304 ]\n",
      " [ 0.02983357]]\n",
      "the iterations of  71000 the loss is 0.4261420786290638 theta= [[-3.75336559]\n",
      " [ 0.03736008]\n",
      " [ 0.02986502]]\n",
      "the iterations of  71100 the loss is 0.42598239135388616 theta= [[-3.75736174]\n",
      " [ 0.03738974]\n",
      " [ 0.02989644]]\n",
      "the iterations of  71200 the loss is 0.42582291670828004 theta= [[-3.76135523]\n",
      " [ 0.03741938]\n",
      " [ 0.02992785]]\n",
      "the iterations of  71300 the loss is 0.4256636542962339 theta= [[-3.76534606]\n",
      " [ 0.037449  ]\n",
      " [ 0.02995923]]\n",
      "the iterations of  71400 the loss is 0.42550460372261656 theta= [[-3.76933423]\n",
      " [ 0.03747861]\n",
      " [ 0.0299906 ]]\n",
      "the iterations of  71500 the loss is 0.42534576459317436 theta= [[-3.77331976]\n",
      " [ 0.0375082 ]\n",
      " [ 0.03002194]]\n",
      "the iterations of  71600 the loss is 0.4251871365145291 theta= [[-3.77730263]\n",
      " [ 0.03753777]\n",
      " [ 0.03005326]]\n",
      "the iterations of  71700 the loss is 0.4250287190941766 theta= [[-3.78128286]\n",
      " [ 0.03756732]\n",
      " [ 0.03008457]]\n",
      "the iterations of  71800 the loss is 0.4248705119404842 theta= [[-3.78526045]\n",
      " [ 0.03759686]\n",
      " [ 0.03011585]]\n",
      "the iterations of  71900 the loss is 0.42471251466268883 theta= [[-3.7892354 ]\n",
      " [ 0.03762637]\n",
      " [ 0.03014711]]\n",
      "the iterations of  72000 the loss is 0.4245547268708947 theta= [[-3.79320771]\n",
      " [ 0.03765587]\n",
      " [ 0.03017835]]\n",
      "the iterations of  72100 the loss is 0.4243971481760718 theta= [[-3.79717739]\n",
      " [ 0.03768536]\n",
      " [ 0.03020956]]\n",
      "the iterations of  72200 the loss is 0.42423977819005354 theta= [[-3.80114444]\n",
      " [ 0.03771482]\n",
      " [ 0.03024076]]\n",
      "the iterations of  72300 the loss is 0.42408261652553486 theta= [[-3.80510886]\n",
      " [ 0.03774427]\n",
      " [ 0.03027194]]\n",
      "the iterations of  72400 the loss is 0.42392566279607036 theta= [[-3.80907066]\n",
      " [ 0.0377737 ]\n",
      " [ 0.0303031 ]]\n",
      "the iterations of  72500 the loss is 0.42376891661607163 theta= [[-3.81302984]\n",
      " [ 0.03780311]\n",
      " [ 0.03033423]]\n",
      "the iterations of  72600 the loss is 0.423612377600806 theta= [[-3.8169864 ]\n",
      " [ 0.0378325 ]\n",
      " [ 0.03036535]]\n",
      "the iterations of  72700 the loss is 0.4234560453663946 theta= [[-3.82094034]\n",
      " [ 0.03786188]\n",
      " [ 0.03039644]]\n",
      "the iterations of  72800 the loss is 0.42329991952980917 theta= [[-3.82489168]\n",
      " [ 0.03789124]\n",
      " [ 0.03042752]]\n",
      "the iterations of  72900 the loss is 0.42314399970887195 theta= [[-3.82884041]\n",
      " [ 0.03792058]\n",
      " [ 0.03045857]]\n",
      "the iterations of  73000 the loss is 0.42298828552225176 theta= [[-3.83278653]\n",
      " [ 0.03794991]\n",
      " [ 0.03048961]]\n",
      "the iterations of  73100 the loss is 0.4228327765894636 theta= [[-3.83673005]\n",
      " [ 0.03797922]\n",
      " [ 0.03052062]]\n",
      "the iterations of  73200 the loss is 0.42267747253086535 theta= [[-3.84067098]\n",
      " [ 0.03800851]\n",
      " [ 0.03055161]]\n",
      "the iterations of  73300 the loss is 0.42252237296765693 theta= [[-3.8446093 ]\n",
      " [ 0.03803778]\n",
      " [ 0.03058259]]\n",
      "the iterations of  73400 the loss is 0.4223674775218778 theta= [[-3.84854504]\n",
      " [ 0.03806703]\n",
      " [ 0.03061354]]\n",
      "the iterations of  73500 the loss is 0.42221278581640476 theta= [[-3.85247818]\n",
      " [ 0.03809627]\n",
      " [ 0.03064447]]\n",
      "the iterations of  73600 the loss is 0.4220582974749501 theta= [[-3.85640874]\n",
      " [ 0.03812549]\n",
      " [ 0.03067538]]\n",
      "the iterations of  73700 the loss is 0.4219040121220604 theta= [[-3.86033672]\n",
      " [ 0.03815469]\n",
      " [ 0.03070627]]\n",
      "the iterations of  73800 the loss is 0.4217499293831133 theta= [[-3.86426212]\n",
      " [ 0.03818388]\n",
      " [ 0.03073714]]\n",
      "the iterations of  73900 the loss is 0.4215960488843167 theta= [[-3.86818493]\n",
      " [ 0.03821305]\n",
      " [ 0.030768  ]]\n",
      "the iterations of  74000 the loss is 0.42144237025270587 theta= [[-3.87210518]\n",
      " [ 0.0382422 ]\n",
      " [ 0.03079883]]\n",
      "the iterations of  74100 the loss is 0.4212888931161424 theta= [[-3.87602285]\n",
      " [ 0.03827133]\n",
      " [ 0.03082964]]\n",
      "the iterations of  74200 the loss is 0.4211356171033115 theta= [[-3.87993796]\n",
      " [ 0.03830045]\n",
      " [ 0.03086043]]\n",
      "the iterations of  74300 the loss is 0.42098254184372036 theta= [[-3.8838505 ]\n",
      " [ 0.03832955]\n",
      " [ 0.0308912 ]]\n",
      "the iterations of  74400 the loss is 0.4208296669676965 theta= [[-3.88776048]\n",
      " [ 0.03835863]\n",
      " [ 0.03092195]]\n",
      "the iterations of  74500 the loss is 0.42067699210638493 theta= [[-3.8916679 ]\n",
      " [ 0.0383877 ]\n",
      " [ 0.03095268]]\n",
      "the iterations of  74600 the loss is 0.42052451689174747 theta= [[-3.89557277]\n",
      " [ 0.03841675]\n",
      " [ 0.03098339]]\n",
      "the iterations of  74700 the loss is 0.4203722409565602 theta= [[-3.89947508]\n",
      " [ 0.03844578]\n",
      " [ 0.03101408]]\n",
      "the iterations of  74800 the loss is 0.4202201639344111 theta= [[-3.90337484]\n",
      " [ 0.03847479]\n",
      " [ 0.03104475]]\n",
      "the iterations of  74900 the loss is 0.42006828545969904 theta= [[-3.90727206]\n",
      " [ 0.03850379]\n",
      " [ 0.0310754 ]]\n",
      "the iterations of  75000 the loss is 0.41991660516763124 theta= [[-3.91116673]\n",
      " [ 0.03853277]\n",
      " [ 0.03110603]]\n",
      "the iterations of  75100 the loss is 0.41976512269422167 theta= [[-3.91505886]\n",
      " [ 0.03856173]\n",
      " [ 0.03113664]]\n",
      "the iterations of  75200 the loss is 0.4196138376762889 theta= [[-3.91894846]\n",
      " [ 0.03859068]\n",
      " [ 0.03116723]]\n",
      "the iterations of  75300 the loss is 0.4194627497514541 theta= [[-3.92283552]\n",
      " [ 0.03861961]\n",
      " [ 0.0311978 ]]\n",
      "the iterations of  75400 the loss is 0.4193118585581398 theta= [[-3.92672005]\n",
      " [ 0.03864852]\n",
      " [ 0.03122835]]\n",
      "the iterations of  75500 the loss is 0.4191611637355673 theta= [[-3.93060205]\n",
      " [ 0.03867741]\n",
      " [ 0.03125888]]\n",
      "the iterations of  75600 the loss is 0.4190106649237551 theta= [[-3.93448152]\n",
      " [ 0.03870629]\n",
      " [ 0.03128939]]\n",
      "the iterations of  75700 the loss is 0.4188603617635173 theta= [[-3.93835847]\n",
      " [ 0.03873515]\n",
      " [ 0.03131988]]\n",
      "the iterations of  75800 the loss is 0.418710253896461 theta= [[-3.9422329 ]\n",
      " [ 0.038764  ]\n",
      " [ 0.03135035]]\n",
      "the iterations of  75900 the loss is 0.4185603409649847 theta= [[-3.94610482]\n",
      " [ 0.03879282]\n",
      " [ 0.0313808 ]]\n",
      "the iterations of  76000 the loss is 0.4184106226122772 theta= [[-3.94997422]\n",
      " [ 0.03882163]\n",
      " [ 0.03141124]]\n",
      "the iterations of  76100 the loss is 0.41826109848231424 theta= [[-3.95384111]\n",
      " [ 0.03885043]\n",
      " [ 0.03144165]]\n",
      "the iterations of  76200 the loss is 0.41811176821985824 theta= [[-3.9577055 ]\n",
      " [ 0.0388792 ]\n",
      " [ 0.03147204]]\n",
      "the iterations of  76300 the loss is 0.4179626314704553 theta= [[-3.96156737]\n",
      " [ 0.03890796]\n",
      " [ 0.03150241]]\n",
      "the iterations of  76400 the loss is 0.4178136878804338 theta= [[-3.96542675]\n",
      " [ 0.0389367 ]\n",
      " [ 0.03153277]]\n",
      "the iterations of  76500 the loss is 0.4176649370969024 theta= [[-3.96928363]\n",
      " [ 0.03896543]\n",
      " [ 0.0315631 ]]\n",
      "the iterations of  76600 the loss is 0.41751637876774844 theta= [[-3.97313801]\n",
      " [ 0.03899414]\n",
      " [ 0.03159341]]\n",
      "the iterations of  76700 the loss is 0.4173680125416358 theta= [[-3.9769899 ]\n",
      " [ 0.03902283]\n",
      " [ 0.03162371]]\n",
      "the iterations of  76800 the loss is 0.4172198380680032 theta= [[-3.9808393 ]\n",
      " [ 0.03905151]\n",
      " [ 0.03165398]]\n",
      "the iterations of  76900 the loss is 0.4170718549970625 theta= [[-3.98468621]\n",
      " [ 0.03908017]\n",
      " [ 0.03168424]]\n",
      "the iterations of  77000 the loss is 0.4169240629797965 theta= [[-3.98853064]\n",
      " [ 0.03910881]\n",
      " [ 0.03171447]]\n",
      "the iterations of  77100 the loss is 0.4167764616679574 theta= [[-3.99237259]\n",
      " [ 0.03913743]\n",
      " [ 0.03174469]]\n",
      "the iterations of  77200 the loss is 0.41662905071406514 theta= [[-3.99621205]\n",
      " [ 0.03916604]\n",
      " [ 0.03177489]]\n",
      "the iterations of  77300 the loss is 0.41648182977140513 theta= [[-4.00004905]\n",
      " [ 0.03919463]\n",
      " [ 0.03180506]]\n",
      "the iterations of  77400 the loss is 0.41633479849402627 theta= [[-4.00388357]\n",
      " [ 0.03922321]\n",
      " [ 0.03183522]]\n",
      "the iterations of  77500 the loss is 0.41618795653674034 theta= [[-4.00771562]\n",
      " [ 0.03925177]\n",
      " [ 0.03186536]]\n",
      "the iterations of  77600 the loss is 0.4160413035551189 theta= [[-4.0115452 ]\n",
      " [ 0.03928031]\n",
      " [ 0.03189548]]\n",
      "the iterations of  77700 the loss is 0.4158948392054924 theta= [[-4.01537232]\n",
      " [ 0.03930884]\n",
      " [ 0.03192558]]\n",
      "the iterations of  77800 the loss is 0.4157485631449469 theta= [[-4.01919698]\n",
      " [ 0.03933734]\n",
      " [ 0.03195566]]\n",
      "the iterations of  77900 the loss is 0.41560247503132475 theta= [[-4.02301918]\n",
      " [ 0.03936584]\n",
      " [ 0.03198572]]\n",
      "the iterations of  78000 the loss is 0.41545657452322 theta= [[-4.02683893]\n",
      " [ 0.03939431]\n",
      " [ 0.03201576]]\n",
      "the iterations of  78100 the loss is 0.41531086127997907 theta= [[-4.03065622]\n",
      " [ 0.03942277]\n",
      " [ 0.03204579]]\n",
      "the iterations of  78200 the loss is 0.4151653349616972 theta= [[-4.03447107]\n",
      " [ 0.03945122]\n",
      " [ 0.03207579]]\n",
      "the iterations of  78300 the loss is 0.41501999522921756 theta= [[-4.03828347]\n",
      " [ 0.03947964]\n",
      " [ 0.03210577]]\n",
      "the iterations of  78400 the loss is 0.4148748417441292 theta= [[-4.04209342]\n",
      " [ 0.03950805]\n",
      " [ 0.03213574]]\n",
      "the iterations of  78500 the loss is 0.41472987416876544 theta= [[-4.04590094]\n",
      " [ 0.03953644]\n",
      " [ 0.03216569]]\n",
      "the iterations of  78600 the loss is 0.41458509216620215 theta= [[-4.04970601]\n",
      " [ 0.03956482]\n",
      " [ 0.03219561]]\n",
      "the iterations of  78700 the loss is 0.4144404954002552 theta= [[-4.05350865]\n",
      " [ 0.03959318]\n",
      " [ 0.03222552]]\n",
      "the iterations of  78800 the loss is 0.41429608353548 theta= [[-4.05730886]\n",
      " [ 0.03962153]\n",
      " [ 0.03225541]]\n",
      "the iterations of  78900 the loss is 0.4141518562371688 theta= [[-4.06110664]\n",
      " [ 0.03964985]\n",
      " [ 0.03228528]]\n",
      "the iterations of  79000 the loss is 0.41400781317134916 theta= [[-4.064902  ]\n",
      " [ 0.03967817]\n",
      " [ 0.03231513]]\n",
      "the iterations of  79100 the loss is 0.41386395400478226 theta= [[-4.06869493]\n",
      " [ 0.03970646]\n",
      " [ 0.03234496]]\n",
      "the iterations of  79200 the loss is 0.4137202784049612 theta= [[-4.07248544]\n",
      " [ 0.03973474]\n",
      " [ 0.03237477]]\n",
      "the iterations of  79300 the loss is 0.41357678604010917 theta= [[-4.07627353]\n",
      " [ 0.039763  ]\n",
      " [ 0.03240457]]\n",
      "the iterations of  79400 the loss is 0.41343347657917767 theta= [[-4.0800592 ]\n",
      " [ 0.03979125]\n",
      " [ 0.03243434]]\n",
      "the iterations of  79500 the loss is 0.4132903496918452 theta= [[-4.08384247]\n",
      " [ 0.03981948]\n",
      " [ 0.0324641 ]]\n",
      "the iterations of  79600 the loss is 0.41314740504851466 theta= [[-4.08762332]\n",
      " [ 0.03984769]\n",
      " [ 0.03249384]]\n",
      "the iterations of  79700 the loss is 0.4130046423203123 theta= [[-4.09140177]\n",
      " [ 0.03987589]\n",
      " [ 0.03252355]]\n",
      "the iterations of  79800 the loss is 0.4128620611790859 theta= [[-4.09517781]\n",
      " [ 0.03990407]\n",
      " [ 0.03255325]]\n",
      "the iterations of  79900 the loss is 0.4127196612974033 theta= [[-4.09895146]\n",
      " [ 0.03993223]\n",
      " [ 0.03258293]]\n",
      "the iterations of  80000 the loss is 0.41257744234854976 theta= [[-4.1027227 ]\n",
      " [ 0.03996038]\n",
      " [ 0.03261259]]\n",
      "the iterations of  80100 the loss is 0.4124354040065271 theta= [[-4.10649155]\n",
      " [ 0.03998851]\n",
      " [ 0.03264224]]\n",
      "the iterations of  80200 the loss is 0.412293545946052 theta= [[-4.11025801]\n",
      " [ 0.04001663]\n",
      " [ 0.03267186]]\n",
      "the iterations of  80300 the loss is 0.41215186784255387 theta= [[-4.11402207]\n",
      " [ 0.04004473]\n",
      " [ 0.03270147]]\n",
      "the iterations of  80400 the loss is 0.4120103693721734 theta= [[-4.11778375]\n",
      " [ 0.04007281]\n",
      " [ 0.03273105]]\n",
      "the iterations of  80500 the loss is 0.4118690502117603 theta= [[-4.12154305]\n",
      " [ 0.04010088]\n",
      " [ 0.03276062]]\n",
      "the iterations of  80600 the loss is 0.41172791003887277 theta= [[-4.12529996]\n",
      " [ 0.04012893]\n",
      " [ 0.03279017]]\n",
      "the iterations of  80700 the loss is 0.41158694853177447 theta= [[-4.1290545 ]\n",
      " [ 0.04015697]\n",
      " [ 0.0328197 ]]\n",
      "the iterations of  80800 the loss is 0.4114461653694344 theta= [[-4.13280666]\n",
      " [ 0.04018499]\n",
      " [ 0.03284921]]\n",
      "the iterations of  80900 the loss is 0.41130556023152354 theta= [[-4.13655644]\n",
      " [ 0.04021299]\n",
      " [ 0.0328787 ]]\n",
      "the iterations of  81000 the loss is 0.4111651327984142 theta= [[-4.14030386]\n",
      " [ 0.04024098]\n",
      " [ 0.03290818]]\n",
      "the iterations of  81100 the loss is 0.4110248827511782 theta= [[-4.14404891]\n",
      " [ 0.04026895]\n",
      " [ 0.03293763]]\n",
      "the iterations of  81200 the loss is 0.41088480977158504 theta= [[-4.14779159]\n",
      " [ 0.04029691]\n",
      " [ 0.03296707]]\n",
      "the iterations of  81300 the loss is 0.4107449135421002 theta= [[-4.15153191]\n",
      " [ 0.04032485]\n",
      " [ 0.03299649]]\n",
      "the iterations of  81400 the loss is 0.41060519374588383 theta= [[-4.15526987]\n",
      " [ 0.04035277]\n",
      " [ 0.03302589]]\n",
      "the iterations of  81500 the loss is 0.4104656500667881 theta= [[-4.15900548]\n",
      " [ 0.04038068]\n",
      " [ 0.03305527]]\n",
      "the iterations of  81600 the loss is 0.41032628218935746 theta= [[-4.16273873]\n",
      " [ 0.04040857]\n",
      " [ 0.03308463]]\n",
      "the iterations of  81700 the loss is 0.4101870897988249 theta= [[-4.16646963]\n",
      " [ 0.04043644]\n",
      " [ 0.03311398]]\n",
      "the iterations of  81800 the loss is 0.4100480725811114 theta= [[-4.17019818]\n",
      " [ 0.0404643 ]\n",
      " [ 0.03314331]]\n",
      "the iterations of  81900 the loss is 0.40990923022282416 theta= [[-4.17392439]\n",
      " [ 0.04049215]\n",
      " [ 0.03317261]]\n",
      "the iterations of  82000 the loss is 0.40977056241125503 theta= [[-4.17764825]\n",
      " [ 0.04051997]\n",
      " [ 0.0332019 ]]\n",
      "the iterations of  82100 the loss is 0.40963206883437847 theta= [[-4.18136977]\n",
      " [ 0.04054779]\n",
      " [ 0.03323117]]\n",
      "the iterations of  82200 the loss is 0.40949374918085046 theta= [[-4.18508896]\n",
      " [ 0.04057558]\n",
      " [ 0.03326043]]\n",
      "the iterations of  82300 the loss is 0.4093556031400062 theta= [[-4.18880581]\n",
      " [ 0.04060336]\n",
      " [ 0.03328966]]\n",
      "the iterations of  82400 the loss is 0.409217630401859 theta= [[-4.19252032]\n",
      " [ 0.04063113]\n",
      " [ 0.03331888]]\n",
      "the iterations of  82500 the loss is 0.40907983065709913 theta= [[-4.19623251]\n",
      " [ 0.04065888]\n",
      " [ 0.03334808]]\n",
      "the iterations of  82600 the loss is 0.40894220359709105 theta= [[-4.19994237]\n",
      " [ 0.04068661]\n",
      " [ 0.03337726]]\n",
      "the iterations of  82700 the loss is 0.40880474891387236 theta= [[-4.20364991]\n",
      " [ 0.04071433]\n",
      " [ 0.03340642]]\n",
      "the iterations of  82800 the loss is 0.4086674663001522 theta= [[-4.20735513]\n",
      " [ 0.04074203]\n",
      " [ 0.03343556]]\n",
      "the iterations of  82900 the loss is 0.40853035544931 theta= [[-4.21105802]\n",
      " [ 0.04076971]\n",
      " [ 0.03346469]]\n",
      "the iterations of  83000 the loss is 0.4083934160553926 theta= [[-4.21475861]\n",
      " [ 0.04079738]\n",
      " [ 0.03349379]]\n",
      "the iterations of  83100 the loss is 0.40825664781311444 theta= [[-4.21845687]\n",
      " [ 0.04082504]\n",
      " [ 0.03352288]]\n",
      "the iterations of  83200 the loss is 0.40812005041785526 theta= [[-4.22215283]\n",
      " [ 0.04085268]\n",
      " [ 0.03355195]]\n",
      "the iterations of  83300 the loss is 0.40798362356565726 theta= [[-4.22584648]\n",
      " [ 0.0408803 ]\n",
      " [ 0.033581  ]]\n",
      "the iterations of  83400 the loss is 0.4078473669532255 theta= [[-4.22953782]\n",
      " [ 0.04090791]\n",
      " [ 0.03361004]]\n",
      "the iterations of  83500 the loss is 0.407711280277925 theta= [[-4.23322686]\n",
      " [ 0.0409355 ]\n",
      " [ 0.03363906]]\n",
      "the iterations of  83600 the loss is 0.4075753632377795 theta= [[-4.23691361]\n",
      " [ 0.04096308]\n",
      " [ 0.03366805]]\n",
      "the iterations of  83700 the loss is 0.40743961553147046 theta= [[-4.24059805]\n",
      " [ 0.04099064]\n",
      " [ 0.03369703]]\n",
      "the iterations of  83800 the loss is 0.40730403685833383 theta= [[-4.2442802 ]\n",
      " [ 0.04101818]\n",
      " [ 0.033726  ]]\n",
      "the iterations of  83900 the loss is 0.40716862691836075 theta= [[-4.24796006]\n",
      " [ 0.04104571]\n",
      " [ 0.03375494]]\n",
      "the iterations of  84000 the loss is 0.40703338541219464 theta= [[-4.25163762]\n",
      " [ 0.04107322]\n",
      " [ 0.03378387]]\n",
      "the iterations of  84100 the loss is 0.4068983120411294 theta= [[-4.2553129 ]\n",
      " [ 0.04110072]\n",
      " [ 0.03381278]]\n",
      "the iterations of  84200 the loss is 0.4067634065071086 theta= [[-4.2589859 ]\n",
      " [ 0.04112821]\n",
      " [ 0.03384167]]\n",
      "the iterations of  84300 the loss is 0.40662866851272356 theta= [[-4.26265662]\n",
      " [ 0.04115567]\n",
      " [ 0.03387054]]\n",
      "the iterations of  84400 the loss is 0.406494097761212 theta= [[-4.26632505]\n",
      " [ 0.04118312]\n",
      " [ 0.03389939]]\n",
      "the iterations of  84500 the loss is 0.40635969395645566 theta= [[-4.26999121]\n",
      " [ 0.04121056]\n",
      " [ 0.03392823]]\n",
      "the iterations of  84600 the loss is 0.40622545680298017 theta= [[-4.2736551 ]\n",
      " [ 0.04123798]\n",
      " [ 0.03395705]]\n",
      "the iterations of  84700 the loss is 0.4060913860059528 theta= [[-4.27731671]\n",
      " [ 0.04126539]\n",
      " [ 0.03398585]]\n",
      "the iterations of  84800 the loss is 0.4059574812711804 theta= [[-4.28097606]\n",
      " [ 0.04129278]\n",
      " [ 0.03401463]]\n",
      "the iterations of  84900 the loss is 0.40582374230510865 theta= [[-4.28463314]\n",
      " [ 0.04132015]\n",
      " [ 0.0340434 ]]\n",
      "the iterations of  85000 the loss is 0.40569016881482023 theta= [[-4.28828796]\n",
      " [ 0.04134751]\n",
      " [ 0.03407215]]\n",
      "the iterations of  85100 the loss is 0.4055567605080333 theta= [[-4.29194052]\n",
      " [ 0.04137486]\n",
      " [ 0.03410088]]\n",
      "the iterations of  85200 the loss is 0.40542351709310004 theta= [[-4.29559081]\n",
      " [ 0.04140218]\n",
      " [ 0.03412959]]\n",
      "the iterations of  85300 the loss is 0.4052904382790048 theta= [[-4.29923886]\n",
      " [ 0.0414295 ]\n",
      " [ 0.03415828]]\n",
      "the iterations of  85400 the loss is 0.40515752377536296 theta= [[-4.30288465]\n",
      " [ 0.0414568 ]\n",
      " [ 0.03418696]]\n",
      "the iterations of  85500 the loss is 0.40502477329241976 theta= [[-4.30652819]\n",
      " [ 0.04148408]\n",
      " [ 0.03421562]]\n",
      "the iterations of  85600 the loss is 0.4048921865410478 theta= [[-4.31016948]\n",
      " [ 0.04151135]\n",
      " [ 0.03424426]]\n",
      "the iterations of  85700 the loss is 0.40475976323274643 theta= [[-4.31380853]\n",
      " [ 0.0415386 ]\n",
      " [ 0.03427289]]\n",
      "the iterations of  85800 the loss is 0.40462750307963946 theta= [[-4.31744534]\n",
      " [ 0.04156583]\n",
      " [ 0.03430149]]\n",
      "the iterations of  85900 the loss is 0.40449540579447424 theta= [[-4.3210799 ]\n",
      " [ 0.04159306]\n",
      " [ 0.03433008]]\n",
      "the iterations of  86000 the loss is 0.40436347109062043 theta= [[-4.32471223]\n",
      " [ 0.04162026]\n",
      " [ 0.03435865]]\n",
      "the iterations of  86100 the loss is 0.4042316986820675 theta= [[-4.32834232]\n",
      " [ 0.04164745]\n",
      " [ 0.03438721]]\n",
      "the iterations of  86200 the loss is 0.4041000882834244 theta= [[-4.33197018]\n",
      " [ 0.04167463]\n",
      " [ 0.03441574]]\n",
      "the iterations of  86300 the loss is 0.40396863960991736 theta= [[-4.33559581]\n",
      " [ 0.04170179]\n",
      " [ 0.03444426]]\n",
      "the iterations of  86400 the loss is 0.4038373523773881 theta= [[-4.33921922]\n",
      " [ 0.04172893]\n",
      " [ 0.03447276]]\n",
      "the iterations of  86500 the loss is 0.4037062263022934 theta= [[-4.3428404 ]\n",
      " [ 0.04175606]\n",
      " [ 0.03450125]]\n",
      "the iterations of  86600 the loss is 0.40357526110170255 theta= [[-4.34645935]\n",
      " [ 0.04178318]\n",
      " [ 0.03452971]]\n",
      "the iterations of  86700 the loss is 0.4034444564932971 theta= [[-4.35007609]\n",
      " [ 0.04181028]\n",
      " [ 0.03455816]]\n",
      "the iterations of  86800 the loss is 0.4033138121953675 theta= [[-4.35369061]\n",
      " [ 0.04183736]\n",
      " [ 0.03458659]]\n",
      "the iterations of  86900 the loss is 0.40318332792681383 theta= [[-4.35730292]\n",
      " [ 0.04186443]\n",
      " [ 0.03461501]]\n",
      "the iterations of  87000 the loss is 0.40305300340714306 theta= [[-4.36091301]\n",
      " [ 0.04189149]\n",
      " [ 0.03464341]]\n",
      "the iterations of  87100 the loss is 0.4029228383564677 theta= [[-4.36452089]\n",
      " [ 0.04191853]\n",
      " [ 0.03467179]]\n",
      "the iterations of  87200 the loss is 0.40279283249550424 theta= [[-4.36812657]\n",
      " [ 0.04194555]\n",
      " [ 0.03470015]]\n",
      "the iterations of  87300 the loss is 0.40266298554557245 theta= [[-4.37173004]\n",
      " [ 0.04197256]\n",
      " [ 0.03472849]]\n",
      "the iterations of  87400 the loss is 0.4025332972285931 theta= [[-4.37533132]\n",
      " [ 0.04199956]\n",
      " [ 0.03475682]]\n",
      "the iterations of  87500 the loss is 0.4024037672670871 theta= [[-4.37893039]\n",
      " [ 0.04202654]\n",
      " [ 0.03478513]]\n",
      "the iterations of  87600 the loss is 0.4022743953841735 theta= [[-4.38252726]\n",
      " [ 0.0420535 ]\n",
      " [ 0.03481342]]\n",
      "the iterations of  87700 the loss is 0.40214518130356824 theta= [[-4.38612194]\n",
      " [ 0.04208045]\n",
      " [ 0.0348417 ]]\n",
      "the iterations of  87800 the loss is 0.40201612474958337 theta= [[-4.38971443]\n",
      " [ 0.04210739]\n",
      " [ 0.03486996]]\n",
      "the iterations of  87900 the loss is 0.4018872254471249 theta= [[-4.39330473]\n",
      " [ 0.04213431]\n",
      " [ 0.0348982 ]]\n",
      "the iterations of  88000 the loss is 0.4017584831216914 theta= [[-4.39689284]\n",
      " [ 0.04216121]\n",
      " [ 0.03492643]]\n",
      "the iterations of  88100 the loss is 0.4016298974993724 theta= [[-4.40047877]\n",
      " [ 0.0421881 ]\n",
      " [ 0.03495463]]\n",
      "the iterations of  88200 the loss is 0.40150146830684813 theta= [[-4.40406252]\n",
      " [ 0.04221498]\n",
      " [ 0.03498282]]\n",
      "the iterations of  88300 the loss is 0.4013731952713868 theta= [[-4.40764408]\n",
      " [ 0.04224184]\n",
      " [ 0.035011  ]]\n",
      "the iterations of  88400 the loss is 0.4012450781208436 theta= [[-4.41122347]\n",
      " [ 0.04226868]\n",
      " [ 0.03503915]]\n",
      "the iterations of  88500 the loss is 0.4011171165836594 theta= [[-4.41480069]\n",
      " [ 0.04229551]\n",
      " [ 0.03506729]]\n",
      "the iterations of  88600 the loss is 0.4009893103888597 theta= [[-4.41837573]\n",
      " [ 0.04232233]\n",
      " [ 0.03509541]]\n",
      "the iterations of  88700 the loss is 0.40086165926605183 theta= [[-4.4219486 ]\n",
      " [ 0.04234913]\n",
      " [ 0.03512352]]\n",
      "the iterations of  88800 the loss is 0.4007341629454257 theta= [[-4.42551931]\n",
      " [ 0.04237591]\n",
      " [ 0.03515161]]\n",
      "the iterations of  88900 the loss is 0.4006068211577506 theta= [[-4.42908785]\n",
      " [ 0.04240269]\n",
      " [ 0.03517968]]\n",
      "the iterations of  89000 the loss is 0.4004796336343748 theta= [[-4.43265423]\n",
      " [ 0.04242944]\n",
      " [ 0.03520773]]\n",
      "the iterations of  89100 the loss is 0.4003526001072235 theta= [[-4.43621845]\n",
      " [ 0.04245618]\n",
      " [ 0.03523577]]\n",
      "the iterations of  89200 the loss is 0.4002257203087979 theta= [[-4.43978051]\n",
      " [ 0.04248291]\n",
      " [ 0.03526379]]\n",
      "the iterations of  89300 the loss is 0.40009899397217374 theta= [[-4.44334042]\n",
      " [ 0.04250962]\n",
      " [ 0.03529179]]\n",
      "the iterations of  89400 the loss is 0.3999724208309996 theta= [[-4.44689818]\n",
      " [ 0.04253632]\n",
      " [ 0.03531978]]\n",
      "the iterations of  89500 the loss is 0.39984600061949604 theta= [[-4.45045378]\n",
      " [ 0.042563  ]\n",
      " [ 0.03534775]]\n",
      "the iterations of  89600 the loss is 0.3997197330724538 theta= [[-4.45400724]\n",
      " [ 0.04258967]\n",
      " [ 0.0353757 ]]\n",
      "the iterations of  89700 the loss is 0.3995936179252329 theta= [[-4.45755855]\n",
      " [ 0.04261633]\n",
      " [ 0.03540364]]\n",
      "the iterations of  89800 the loss is 0.39946765491376096 theta= [[-4.46110772]\n",
      " [ 0.04264296]\n",
      " [ 0.03543156]]\n",
      "the iterations of  89900 the loss is 0.3993418437745313 theta= [[-4.46465475]\n",
      " [ 0.04266959]\n",
      " [ 0.03545946]]\n",
      "the iterations of  90000 the loss is 0.39921618424460276 theta= [[-4.46819964]\n",
      " [ 0.0426962 ]\n",
      " [ 0.03548734]]\n",
      "the iterations of  90100 the loss is 0.39909067606159776 theta= [[-4.47174239]\n",
      " [ 0.04272279]\n",
      " [ 0.03551521]]\n",
      "the iterations of  90200 the loss is 0.3989653189636998 theta= [[-4.47528302]\n",
      " [ 0.04274937]\n",
      " [ 0.03554306]]\n",
      "the iterations of  90300 the loss is 0.3988401126896543 theta= [[-4.47882151]\n",
      " [ 0.04277594]\n",
      " [ 0.0355709 ]]\n",
      "the iterations of  90400 the loss is 0.3987150569787663 theta= [[-4.48235787]\n",
      " [ 0.04280249]\n",
      " [ 0.03559872]]\n",
      "the iterations of  90500 the loss is 0.39859015157089805 theta= [[-4.48589211]\n",
      " [ 0.04282903]\n",
      " [ 0.03562652]]\n",
      "the iterations of  90600 the loss is 0.3984653962064697 theta= [[-4.48942423]\n",
      " [ 0.04285555]\n",
      " [ 0.0356543 ]]\n",
      "the iterations of  90700 the loss is 0.39834079062645555 theta= [[-4.49295422]\n",
      " [ 0.04288206]\n",
      " [ 0.03568207]]\n",
      "the iterations of  90800 the loss is 0.3982163345723853 theta= [[-4.49648209]\n",
      " [ 0.04290855]\n",
      " [ 0.03570983]]\n",
      "the iterations of  90900 the loss is 0.3980920277863412 theta= [[-4.50000785]\n",
      " [ 0.04293503]\n",
      " [ 0.03573756]]\n",
      "the iterations of  91000 the loss is 0.3979678700109558 theta= [[-4.5035315 ]\n",
      " [ 0.04296149]\n",
      " [ 0.03576528]]\n",
      "the iterations of  91100 the loss is 0.39784386098941327 theta= [[-4.50705303]\n",
      " [ 0.04298794]\n",
      " [ 0.03579298]]\n",
      "the iterations of  91200 the loss is 0.39772000046544603 theta= [[-4.51057245]\n",
      " [ 0.04301438]\n",
      " [ 0.03582067]]\n",
      "the iterations of  91300 the loss is 0.397596288183334 theta= [[-4.51408977]\n",
      " [ 0.0430408 ]\n",
      " [ 0.03584834]]\n",
      "the iterations of  91400 the loss is 0.39747272388790317 theta= [[-4.51760498]\n",
      " [ 0.0430672 ]\n",
      " [ 0.03587599]]\n",
      "the iterations of  91500 the loss is 0.3973493073245246 theta= [[-4.52111809]\n",
      " [ 0.0430936 ]\n",
      " [ 0.03590363]]\n",
      "the iterations of  91600 the loss is 0.3972260382391129 theta= [[-4.5246291 ]\n",
      " [ 0.04311997]\n",
      " [ 0.03593125]]\n",
      "the iterations of  91700 the loss is 0.3971029163781252 theta= [[-4.52813802]\n",
      " [ 0.04314634]\n",
      " [ 0.03595885]]\n",
      "the iterations of  91800 the loss is 0.39697994148855875 theta= [[-4.53164484]\n",
      " [ 0.04317269]\n",
      " [ 0.03598644]]\n",
      "the iterations of  91900 the loss is 0.3968571133179518 theta= [[-4.53514956]\n",
      " [ 0.04319902]\n",
      " [ 0.03601401]]\n",
      "the iterations of  92000 the loss is 0.3967344316143797 theta= [[-4.5386522 ]\n",
      " [ 0.04322534]\n",
      " [ 0.03604156]]\n",
      "the iterations of  92100 the loss is 0.39661189612645553 theta= [[-4.54215275]\n",
      " [ 0.04325165]\n",
      " [ 0.0360691 ]]\n",
      "the iterations of  92200 the loss is 0.39648950660332827 theta= [[-4.54565121]\n",
      " [ 0.04327794]\n",
      " [ 0.03609662]]\n",
      "the iterations of  92300 the loss is 0.3963672627946809 theta= [[-4.54914759]\n",
      " [ 0.04330421]\n",
      " [ 0.03612412]]\n",
      "the iterations of  92400 the loss is 0.39624516445073005 theta= [[-4.55264188]\n",
      " [ 0.04333048]\n",
      " [ 0.03615161]]\n",
      "the iterations of  92500 the loss is 0.3961232113222244 theta= [[-4.5561341 ]\n",
      " [ 0.04335673]\n",
      " [ 0.03617909]]\n",
      "the iterations of  92600 the loss is 0.3960014031604425 theta= [[-4.55962425]\n",
      " [ 0.04338296]\n",
      " [ 0.03620654]]\n",
      "the iterations of  92700 the loss is 0.3958797397171938 theta= [[-4.56311231]\n",
      " [ 0.04340918]\n",
      " [ 0.03623398]]\n",
      "the iterations of  92800 the loss is 0.3957582207448139 theta= [[-4.56659831]\n",
      " [ 0.04343539]\n",
      " [ 0.03626141]]\n",
      "the iterations of  92900 the loss is 0.3956368459961672 theta= [[-4.57008224]\n",
      " [ 0.04346158]\n",
      " [ 0.03628881]]\n",
      "the iterations of  93000 the loss is 0.3955156152246421 theta= [[-4.5735641 ]\n",
      " [ 0.04348776]\n",
      " [ 0.0363162 ]]\n",
      "the iterations of  93100 the loss is 0.39539452818415255 theta= [[-4.5770439 ]\n",
      " [ 0.04351392]\n",
      " [ 0.03634358]]\n",
      "the iterations of  93200 the loss is 0.39527358462913464 theta= [[-4.58052163]\n",
      " [ 0.04354007]\n",
      " [ 0.03637094]]\n",
      "the iterations of  93300 the loss is 0.395152784314547 theta= [[-4.5839973 ]\n",
      " [ 0.0435662 ]\n",
      " [ 0.03639828]]\n",
      "the iterations of  93400 the loss is 0.39503212699586826 theta= [[-4.58747092]\n",
      " [ 0.04359232]\n",
      " [ 0.03642561]]\n",
      "the iterations of  93500 the loss is 0.394911612429097 theta= [[-4.59094248]\n",
      " [ 0.04361843]\n",
      " [ 0.03645292]]\n",
      "the iterations of  93600 the loss is 0.39479124037074925 theta= [[-4.59441199]\n",
      " [ 0.04364452]\n",
      " [ 0.03648021]]\n",
      "the iterations of  93700 the loss is 0.39467101057785847 theta= [[-4.59787944]\n",
      " [ 0.0436706 ]\n",
      " [ 0.03650749]]\n",
      "the iterations of  93800 the loss is 0.39455092280797316 theta= [[-4.60134485]\n",
      " [ 0.04369667]\n",
      " [ 0.03653475]]\n",
      "the iterations of  93900 the loss is 0.3944309768191564 theta= [[-4.60480821]\n",
      " [ 0.04372272]\n",
      " [ 0.036562  ]]\n",
      "the iterations of  94000 the loss is 0.39431117236998436 theta= [[-4.60826953]\n",
      " [ 0.04374875]\n",
      " [ 0.03658923]]\n",
      "the iterations of  94100 the loss is 0.39419150921954516 theta= [[-4.6117288 ]\n",
      " [ 0.04377478]\n",
      " [ 0.03661644]]\n",
      "the iterations of  94200 the loss is 0.3940719871274376 theta= [[-4.61518604]\n",
      " [ 0.04380078]\n",
      " [ 0.03664364]]\n",
      "the iterations of  94300 the loss is 0.3939526058537698 theta= [[-4.61864124]\n",
      " [ 0.04382678]\n",
      " [ 0.03667082]]\n",
      "the iterations of  94400 the loss is 0.3938333651591583 theta= [[-4.6220944 ]\n",
      " [ 0.04385276]\n",
      " [ 0.03669799]]\n",
      "the iterations of  94500 the loss is 0.3937142648047265 theta= [[-4.62554553]\n",
      " [ 0.04387873]\n",
      " [ 0.03672514]]\n",
      "the iterations of  94600 the loss is 0.39359530455210345 theta= [[-4.62899463]\n",
      " [ 0.04390468]\n",
      " [ 0.03675227]]\n",
      "the iterations of  94700 the loss is 0.3934764841634235 theta= [[-4.6324417 ]\n",
      " [ 0.04393062]\n",
      " [ 0.03677939]]\n",
      "the iterations of  94800 the loss is 0.39335780340132304 theta= [[-4.63588675]\n",
      " [ 0.04395654]\n",
      " [ 0.03680649]]\n",
      "the iterations of  94900 the loss is 0.3932392620289417 theta= [[-4.63932977]\n",
      " [ 0.04398245]\n",
      " [ 0.03683358]]\n",
      "the iterations of  95000 the loss is 0.3931208598099197 theta= [[-4.64277077]\n",
      " [ 0.04400835]\n",
      " [ 0.03686065]]\n",
      "the iterations of  95100 the loss is 0.3930025965083969 theta= [[-4.64620975]\n",
      " [ 0.04403423]\n",
      " [ 0.03688771]]\n",
      "the iterations of  95200 the loss is 0.39288447188901215 theta= [[-4.64964672]\n",
      " [ 0.0440601 ]\n",
      " [ 0.03691475]]\n",
      "the iterations of  95300 the loss is 0.39276648571690115 theta= [[-4.65308167]\n",
      " [ 0.04408596]\n",
      " [ 0.03694177]]\n",
      "the iterations of  95400 the loss is 0.39264863775769604 theta= [[-4.6565146 ]\n",
      " [ 0.0441118 ]\n",
      " [ 0.03696878]]\n",
      "the iterations of  95500 the loss is 0.3925309277775239 theta= [[-4.65994553]\n",
      " [ 0.04413762]\n",
      " [ 0.03699577]]\n",
      "the iterations of  95600 the loss is 0.3924133555430054 theta= [[-4.66337445]\n",
      " [ 0.04416344]\n",
      " [ 0.03702275]]\n",
      "the iterations of  95700 the loss is 0.3922959208212543 theta= [[-4.66680136]\n",
      " [ 0.04418924]\n",
      " [ 0.03704971]]\n",
      "the iterations of  95800 the loss is 0.3921786233798754 theta= [[-4.67022627]\n",
      " [ 0.04421502]\n",
      " [ 0.03707665]]\n",
      "the iterations of  95900 the loss is 0.3920614629869635 theta= [[-4.67364918]\n",
      " [ 0.0442408 ]\n",
      " [ 0.03710358]]\n",
      "the iterations of  96000 the loss is 0.39194443941110313 theta= [[-4.67707009]\n",
      " [ 0.04426656]\n",
      " [ 0.03713049]]\n",
      "the iterations of  96100 the loss is 0.3918275524213664 theta= [[-4.680489  ]\n",
      " [ 0.0442923 ]\n",
      " [ 0.03715739]]\n",
      "the iterations of  96200 the loss is 0.3917108017873121 theta= [[-4.68390591]\n",
      " [ 0.04431803]\n",
      " [ 0.03718427]]\n",
      "the iterations of  96300 the loss is 0.39159418727898493 theta= [[-4.68732084]\n",
      " [ 0.04434375]\n",
      " [ 0.03721114]]\n",
      "the iterations of  96400 the loss is 0.39147770866691345 theta= [[-4.69073377]\n",
      " [ 0.04436945]\n",
      " [ 0.03723799]]\n",
      "the iterations of  96500 the loss is 0.3913613657221097 theta= [[-4.69414472]\n",
      " [ 0.04439514]\n",
      " [ 0.03726482]]\n",
      "the iterations of  96600 the loss is 0.39124515821606826 theta= [[-4.69755367]\n",
      " [ 0.04442082]\n",
      " [ 0.03729164]]\n",
      "the iterations of  96700 the loss is 0.3911290859207638 theta= [[-4.70096065]\n",
      " [ 0.04444648]\n",
      " [ 0.03731845]]\n",
      "the iterations of  96800 the loss is 0.3910131486086516 theta= [[-4.70436564]\n",
      " [ 0.04447213]\n",
      " [ 0.03734524]]\n",
      "the iterations of  96900 the loss is 0.39089734605266513 theta= [[-4.70776866]\n",
      " [ 0.04449776]\n",
      " [ 0.03737201]]\n",
      "the iterations of  97000 the loss is 0.3907816780262154 theta= [[-4.71116969]\n",
      " [ 0.04452339]\n",
      " [ 0.03739877]]\n",
      "the iterations of  97100 the loss is 0.3906661443031899 theta= [[-4.71456876]\n",
      " [ 0.04454899]\n",
      " [ 0.03742551]]\n",
      "the iterations of  97200 the loss is 0.39055074465795125 theta= [[-4.71796585]\n",
      " [ 0.04457459]\n",
      " [ 0.03745224]]\n",
      "the iterations of  97300 the loss is 0.39043547886533597 theta= [[-4.72136096]\n",
      " [ 0.04460017]\n",
      " [ 0.03747895]]\n",
      "the iterations of  97400 the loss is 0.39032034670065413 theta= [[-4.72475411]\n",
      " [ 0.04462574]\n",
      " [ 0.03750564]]\n",
      "the iterations of  97500 the loss is 0.3902053479396867 theta= [[-4.7281453 ]\n",
      " [ 0.04465129]\n",
      " [ 0.03753232]]\n",
      "the iterations of  97600 the loss is 0.3900904823586863 theta= [[-4.73153451]\n",
      " [ 0.04467683]\n",
      " [ 0.03755899]]\n",
      "the iterations of  97700 the loss is 0.3899757497343743 theta= [[-4.73492177]\n",
      " [ 0.04470235]\n",
      " [ 0.03758564]]\n",
      "the iterations of  97800 the loss is 0.38986114984394077 theta= [[-4.73830707]\n",
      " [ 0.04472787]\n",
      " [ 0.03761227]]\n",
      "the iterations of  97900 the loss is 0.38974668246504357 theta= [[-4.74169041]\n",
      " [ 0.04475337]\n",
      " [ 0.03763889]]\n",
      "the iterations of  98000 the loss is 0.3896323473758057 theta= [[-4.74507179]\n",
      " [ 0.04477885]\n",
      " [ 0.03766549]]\n",
      "the iterations of  98100 the loss is 0.3895181443548163 theta= [[-4.74845122]\n",
      " [ 0.04480433]\n",
      " [ 0.03769208]]\n",
      "the iterations of  98200 the loss is 0.3894040731811284 theta= [[-4.7518287 ]\n",
      " [ 0.04482978]\n",
      " [ 0.03771865]]\n",
      "the iterations of  98300 the loss is 0.3892901336342568 theta= [[-4.75520423]\n",
      " [ 0.04485523]\n",
      " [ 0.03774521]]\n",
      "the iterations of  98400 the loss is 0.3891763254941792 theta= [[-4.75857781]\n",
      " [ 0.04488066]\n",
      " [ 0.03777175]]\n",
      "the iterations of  98500 the loss is 0.3890626485413337 theta= [[-4.76194944]\n",
      " [ 0.04490608]\n",
      " [ 0.03779828]]\n",
      "the iterations of  98600 the loss is 0.3889491025566174 theta= [[-4.76531914]\n",
      " [ 0.04493149]\n",
      " [ 0.03782479]]\n",
      "the iterations of  98700 the loss is 0.3888356873213868 theta= [[-4.76868689]\n",
      " [ 0.04495688]\n",
      " [ 0.03785129]]\n",
      "the iterations of  98800 the loss is 0.38872240261745483 theta= [[-4.77205271]\n",
      " [ 0.04498226]\n",
      " [ 0.03787777]]\n",
      "the iterations of  98900 the loss is 0.38860924822709103 theta= [[-4.77541658]\n",
      " [ 0.04500762]\n",
      " [ 0.03790424]]\n",
      "the iterations of  99000 the loss is 0.3884962239330204 theta= [[-4.77877853]\n",
      " [ 0.04503297]\n",
      " [ 0.03793069]]\n",
      "the iterations of  99100 the loss is 0.3883833295184218 theta= [[-4.78213854]\n",
      " [ 0.04505831]\n",
      " [ 0.03795712]]\n",
      "the iterations of  99200 the loss is 0.38827056476692695 theta= [[-4.78549662]\n",
      " [ 0.04508364]\n",
      " [ 0.03798355]]\n",
      "the iterations of  99300 the loss is 0.3881579294626196 theta= [[-4.78885277]\n",
      " [ 0.04510895]\n",
      " [ 0.03800995]]\n",
      "the iterations of  99400 the loss is 0.38804542339003484 theta= [[-4.792207  ]\n",
      " [ 0.04513425]\n",
      " [ 0.03803634]]\n",
      "the iterations of  99500 the loss is 0.38793304633415643 theta= [[-4.7955593 ]\n",
      " [ 0.04515953]\n",
      " [ 0.03806272]]\n",
      "the iterations of  99600 the loss is 0.3878207980804177 theta= [[-4.79890969]\n",
      " [ 0.0451848 ]\n",
      " [ 0.03808908]]\n",
      "the iterations of  99700 the loss is 0.3877086784146997 theta= [[-4.80225815]\n",
      " [ 0.04521006]\n",
      " [ 0.03811542]]\n",
      "the iterations of  99800 the loss is 0.3875966871233292 theta= [[-4.80560469]\n",
      " [ 0.04523531]\n",
      " [ 0.03814175]]\n",
      "the iterations of  99900 the loss is 0.3874848239930791 theta= [[-4.80894932]\n",
      " [ 0.04526054]\n",
      " [ 0.03816807]]\n",
      "the iterations of  100000 the loss is 0.38737308881116683 theta= [[-4.81229204]\n",
      " [ 0.04528576]\n",
      " [ 0.03819437]]\n",
      "the iterations of  100100 the loss is 0.3872614813652524 theta= [[-4.81563284]\n",
      " [ 0.04531096]\n",
      " [ 0.03822066]]\n",
      "the iterations of  100200 the loss is 0.38715000144343903 theta= [[-4.81897174]\n",
      " [ 0.04533615]\n",
      " [ 0.03824693]]\n",
      "the iterations of  100300 the loss is 0.387038648834271 theta= [[-4.82230872]\n",
      " [ 0.04536133]\n",
      " [ 0.03827318]]\n",
      "the iterations of  100400 the loss is 0.38692742332673247 theta= [[-4.82564381]\n",
      " [ 0.0453865 ]\n",
      " [ 0.03829942]]\n",
      "the iterations of  100500 the loss is 0.3868163247102466 theta= [[-4.82897699]\n",
      " [ 0.04541165]\n",
      " [ 0.03832565]]\n",
      "the iterations of  100600 the loss is 0.38670535277467516 theta= [[-4.83230827]\n",
      " [ 0.04543679]\n",
      " [ 0.03835186]]\n",
      "the iterations of  100700 the loss is 0.38659450731031697 theta= [[-4.83563765]\n",
      " [ 0.04546192]\n",
      " [ 0.03837805]]\n",
      "the iterations of  100800 the loss is 0.3864837881079064 theta= [[-4.83896513]\n",
      " [ 0.04548703]\n",
      " [ 0.03840424]]\n",
      "the iterations of  100900 the loss is 0.3863731949586131 theta= [[-4.84229072]\n",
      " [ 0.04551213]\n",
      " [ 0.0384304 ]]\n",
      "the iterations of  101000 the loss is 0.3862627276540411 theta= [[-4.84561441]\n",
      " [ 0.04553722]\n",
      " [ 0.03845655]]\n",
      "the iterations of  101100 the loss is 0.3861523859862266 theta= [[-4.84893622]\n",
      " [ 0.04556229]\n",
      " [ 0.03848269]]\n",
      "the iterations of  101200 the loss is 0.38604216974763794 theta= [[-4.85225613]\n",
      " [ 0.04558735]\n",
      " [ 0.03850881]]\n",
      "the iterations of  101300 the loss is 0.3859320787311741 theta= [[-4.85557416]\n",
      " [ 0.0456124 ]\n",
      " [ 0.03853492]]\n",
      "the iterations of  101400 the loss is 0.3858221127301644 theta= [[-4.85889031]\n",
      " [ 0.04563744]\n",
      " [ 0.03856101]]\n",
      "the iterations of  101500 the loss is 0.38571227153836646 theta= [[-4.86220457]\n",
      " [ 0.04566246]\n",
      " [ 0.03858709]]\n",
      "the iterations of  101600 the loss is 0.38560255494996637 theta= [[-4.86551696]\n",
      " [ 0.04568747]\n",
      " [ 0.03861315]]\n",
      "the iterations of  101700 the loss is 0.38549296275957595 theta= [[-4.86882746]\n",
      " [ 0.04571246]\n",
      " [ 0.0386392 ]]\n",
      "the iterations of  101800 the loss is 0.38538349476223366 theta= [[-4.87213609]\n",
      " [ 0.04573744]\n",
      " [ 0.03866523]]\n",
      "the iterations of  101900 the loss is 0.3852741507534021 theta= [[-4.87544284]\n",
      " [ 0.04576241]\n",
      " [ 0.03869125]]\n",
      "the iterations of  102000 the loss is 0.3851649305289683 theta= [[-4.87874772]\n",
      " [ 0.04578737]\n",
      " [ 0.03871726]]\n",
      "the iterations of  102100 the loss is 0.3850558338852409 theta= [[-4.88205073]\n",
      " [ 0.04581231]\n",
      " [ 0.03874324]]\n",
      "the iterations of  102200 the loss is 0.38494686061895145 theta= [[-4.88535188]\n",
      " [ 0.04583724]\n",
      " [ 0.03876922]]\n",
      "the iterations of  102300 the loss is 0.38483801052725164 theta= [[-4.88865115]\n",
      " [ 0.04586216]\n",
      " [ 0.03879518]]\n",
      "the iterations of  102400 the loss is 0.38472928340771256 theta= [[-4.89194856]\n",
      " [ 0.04588707]\n",
      " [ 0.03882112]]\n",
      "the iterations of  102500 the loss is 0.3846206790583245 theta= [[-4.89524411]\n",
      " [ 0.04591196]\n",
      " [ 0.03884706]]\n",
      "the iterations of  102600 the loss is 0.384512197277495 theta= [[-4.8985378 ]\n",
      " [ 0.04593684]\n",
      " [ 0.03887297]]\n",
      "the iterations of  102700 the loss is 0.38440383786404864 theta= [[-4.90182963]\n",
      " [ 0.0459617 ]\n",
      " [ 0.03889887]]\n",
      "the iterations of  102800 the loss is 0.38429560061722623 theta= [[-4.90511961]\n",
      " [ 0.04598656]\n",
      " [ 0.03892476]]\n",
      "the iterations of  102900 the loss is 0.38418748533668234 theta= [[-4.90840773]\n",
      " [ 0.0460114 ]\n",
      " [ 0.03895063]]\n",
      "the iterations of  103000 the loss is 0.38407949182248563 theta= [[-4.911694  ]\n",
      " [ 0.04603623]\n",
      " [ 0.03897649]]\n",
      "the iterations of  103100 the loss is 0.38397161987511796 theta= [[-4.91497842]\n",
      " [ 0.04606104]\n",
      " [ 0.03900234]]\n",
      "the iterations of  103200 the loss is 0.3838638692954721 theta= [[-4.91826099]\n",
      " [ 0.04608584]\n",
      " [ 0.03902816]]\n",
      "the iterations of  103300 the loss is 0.3837562398848528 theta= [[-4.92154171]\n",
      " [ 0.04611063]\n",
      " [ 0.03905398]]\n",
      "the iterations of  103400 the loss is 0.3836487314449737 theta= [[-4.92482059]\n",
      " [ 0.04613541]\n",
      " [ 0.03907978]]\n",
      "the iterations of  103500 the loss is 0.3835413437779574 theta= [[-4.92809763]\n",
      " [ 0.04616017]\n",
      " [ 0.03910557]]\n",
      "the iterations of  103600 the loss is 0.3834340766863351 theta= [[-4.93137283]\n",
      " [ 0.04618492]\n",
      " [ 0.03913134]]\n",
      "the iterations of  103700 the loss is 0.38332692997304396 theta= [[-4.93464619]\n",
      " [ 0.04620966]\n",
      " [ 0.03915709]]\n",
      "the iterations of  103800 the loss is 0.38321990344142776 theta= [[-4.93791771]\n",
      " [ 0.04623438]\n",
      " [ 0.03918284]]\n",
      "the iterations of  103900 the loss is 0.3831129968952348 theta= [[-4.9411874 ]\n",
      " [ 0.0462591 ]\n",
      " [ 0.03920856]]\n",
      "the iterations of  104000 the loss is 0.38300621013861813 theta= [[-4.94445525]\n",
      " [ 0.0462838 ]\n",
      " [ 0.03923428]]\n",
      "the iterations of  104100 the loss is 0.38289954297613343 theta= [[-4.94772128]\n",
      " [ 0.04630848]\n",
      " [ 0.03925998]]\n",
      "the iterations of  104200 the loss is 0.3827929952127381 theta= [[-4.95098547]\n",
      " [ 0.04633316]\n",
      " [ 0.03928566]]\n",
      "the iterations of  104300 the loss is 0.3826865666537913 theta= [[-4.95424784]\n",
      " [ 0.04635782]\n",
      " [ 0.03931133]]\n",
      "the iterations of  104400 the loss is 0.38258025710505245 theta= [[-4.95750839]\n",
      " [ 0.04638247]\n",
      " [ 0.03933699]]\n",
      "the iterations of  104500 the loss is 0.38247406637267933 theta= [[-4.96076711]\n",
      " [ 0.0464071 ]\n",
      " [ 0.03936263]]\n",
      "the iterations of  104600 the loss is 0.38236799426322937 theta= [[-4.96402402]\n",
      " [ 0.04643173]\n",
      " [ 0.03938826]]\n",
      "the iterations of  104700 the loss is 0.3822620405836561 theta= [[-4.9672791 ]\n",
      " [ 0.04645634]\n",
      " [ 0.03941388]]\n",
      "the iterations of  104800 the loss is 0.3821562051413105 theta= [[-4.97053237]\n",
      " [ 0.04648094]\n",
      " [ 0.03943948]]\n",
      "the iterations of  104900 the loss is 0.3820504877439383 theta= [[-4.97378382]\n",
      " [ 0.04650552]\n",
      " [ 0.03946506]]\n",
      "the iterations of  105000 the loss is 0.3819448881996806 theta= [[-4.97703346]\n",
      " [ 0.0465301 ]\n",
      " [ 0.03949063]]\n",
      "the iterations of  105100 the loss is 0.38183940631707086 theta= [[-4.98028129]\n",
      " [ 0.04655466]\n",
      " [ 0.03951619]]\n",
      "the iterations of  105200 the loss is 0.38173404190503646 theta= [[-4.9835273 ]\n",
      " [ 0.0465792 ]\n",
      " [ 0.03954173]]\n",
      "the iterations of  105300 the loss is 0.38162879477289585 theta= [[-4.98677152]\n",
      " [ 0.04660374]\n",
      " [ 0.03956726]]\n",
      "the iterations of  105400 the loss is 0.3815236647303588 theta= [[-4.99001392]\n",
      " [ 0.04662826]\n",
      " [ 0.03959278]]\n",
      "the iterations of  105500 the loss is 0.3814186515875243 theta= [[-4.99325453]\n",
      " [ 0.04665277]\n",
      " [ 0.03961828]]\n",
      "the iterations of  105600 the loss is 0.38131375515488103 theta= [[-4.99649333]\n",
      " [ 0.04667727]\n",
      " [ 0.03964376]]\n",
      "the iterations of  105700 the loss is 0.3812089752433052 theta= [[-4.99973033]\n",
      " [ 0.04670176]\n",
      " [ 0.03966923]]\n",
      "the iterations of  105800 the loss is 0.3811043116640605 theta= [[-5.00296554]\n",
      " [ 0.04672623]\n",
      " [ 0.03969469]]\n",
      "the iterations of  105900 the loss is 0.3809997642287965 theta= [[-5.00619895]\n",
      " [ 0.04675069]\n",
      " [ 0.03972014]]\n",
      "the iterations of  106000 the loss is 0.3808953327495487 theta= [[-5.00943056]\n",
      " [ 0.04677514]\n",
      " [ 0.03974557]]\n",
      "the iterations of  106100 the loss is 0.3807910170387363 theta= [[-5.01266039]\n",
      " [ 0.04679957]\n",
      " [ 0.03977098]]\n",
      "the iterations of  106200 the loss is 0.38068681690916234 theta= [[-5.01588842]\n",
      " [ 0.04682399]\n",
      " [ 0.03979638]]\n",
      "the iterations of  106300 the loss is 0.38058273217401256 theta= [[-5.01911467]\n",
      " [ 0.0468484 ]\n",
      " [ 0.03982177]]\n",
      "the iterations of  106400 the loss is 0.3804787626468537 theta= [[-5.02233913]\n",
      " [ 0.0468728 ]\n",
      " [ 0.03984715]]\n",
      "the iterations of  106500 the loss is 0.3803749081416343 theta= [[-5.0255618 ]\n",
      " [ 0.04689719]\n",
      " [ 0.03987251]]\n",
      "the iterations of  106600 the loss is 0.3802711684726821 theta= [[-5.0287827 ]\n",
      " [ 0.04692156]\n",
      " [ 0.03989785]]\n",
      "the iterations of  106700 the loss is 0.38016754345470416 theta= [[-5.03200181]\n",
      " [ 0.04694592]\n",
      " [ 0.03992319]]\n",
      "the iterations of  106800 the loss is 0.38006403290278507 theta= [[-5.03521915]\n",
      " [ 0.04697027]\n",
      " [ 0.0399485 ]]\n",
      "the iterations of  106900 the loss is 0.37996063663238744 theta= [[-5.0384347 ]\n",
      " [ 0.04699461]\n",
      " [ 0.03997381]]\n",
      "the iterations of  107000 the loss is 0.37985735445934926 theta= [[-5.04164849]\n",
      " [ 0.04701893]\n",
      " [ 0.0399991 ]]\n",
      "the iterations of  107100 the loss is 0.3797541861998848 theta= [[-5.0448605 ]\n",
      " [ 0.04704324]\n",
      " [ 0.04002437]]\n",
      "the iterations of  107200 the loss is 0.3796511316705822 theta= [[-5.04807074]\n",
      " [ 0.04706754]\n",
      " [ 0.04004964]]\n",
      "the iterations of  107300 the loss is 0.3795481906884035 theta= [[-5.05127921]\n",
      " [ 0.04709183]\n",
      " [ 0.04007489]]\n",
      "the iterations of  107400 the loss is 0.37944536307068366 theta= [[-5.05448591]\n",
      " [ 0.0471161 ]\n",
      " [ 0.04010012]]\n",
      "the iterations of  107500 the loss is 0.37934264863512934 theta= [[-5.05769085]\n",
      " [ 0.04714036]\n",
      " [ 0.04012534]]\n",
      "the iterations of  107600 the loss is 0.37924004719981824 theta= [[-5.06089403]\n",
      " [ 0.04716461]\n",
      " [ 0.04015055]]\n",
      "the iterations of  107700 the loss is 0.3791375585831983 theta= [[-5.06409544]\n",
      " [ 0.04718885]\n",
      " [ 0.04017574]]\n",
      "the iterations of  107800 the loss is 0.3790351826040863 theta= [[-5.06729509]\n",
      " [ 0.04721307]\n",
      " [ 0.04020092]]\n",
      "the iterations of  107900 the loss is 0.37893291908166815 theta= [[-5.07049299]\n",
      " [ 0.04723729]\n",
      " [ 0.04022609]]\n",
      "the iterations of  108000 the loss is 0.3788307678354962 theta= [[-5.07368913]\n",
      " [ 0.04726149]\n",
      " [ 0.04025124]]\n",
      "the iterations of  108100 the loss is 0.3787287286854905 theta= [[-5.07688351]\n",
      " [ 0.04728567]\n",
      " [ 0.04027638]]\n",
      "the iterations of  108200 the loss is 0.3786268014519365 theta= [[-5.08007615]\n",
      " [ 0.04730985]\n",
      " [ 0.0403015 ]]\n",
      "the iterations of  108300 the loss is 0.3785249859554844 theta= [[-5.08326703]\n",
      " [ 0.04733401]\n",
      " [ 0.04032662]]\n",
      "the iterations of  108400 the loss is 0.37842328201714837 theta= [[-5.08645616]\n",
      " [ 0.04735817]\n",
      " [ 0.04035171]]\n",
      "the iterations of  108500 the loss is 0.37832168945830646 theta= [[-5.08964355]\n",
      " [ 0.0473823 ]\n",
      " [ 0.0403768 ]]\n",
      "the iterations of  108600 the loss is 0.37822020810069823 theta= [[-5.09282919]\n",
      " [ 0.04740643]\n",
      " [ 0.04040187]]\n",
      "the iterations of  108700 the loss is 0.3781188377664253 theta= [[-5.09601309]\n",
      " [ 0.04743055]\n",
      " [ 0.04042692]]\n",
      "the iterations of  108800 the loss is 0.37801757827794985 theta= [[-5.09919525]\n",
      " [ 0.04745465]\n",
      " [ 0.04045197]]\n",
      "the iterations of  108900 the loss is 0.37791642945809345 theta= [[-5.10237567]\n",
      " [ 0.04747874]\n",
      " [ 0.040477  ]]\n",
      "the iterations of  109000 the loss is 0.3778153911300376 theta= [[-5.10555435]\n",
      " [ 0.04750282]\n",
      " [ 0.04050201]]\n",
      "the iterations of  109100 the loss is 0.3777144631173207 theta= [[-5.10873129]\n",
      " [ 0.04752689]\n",
      " [ 0.04052701]]\n",
      "the iterations of  109200 the loss is 0.37761364524383917 theta= [[-5.1119065 ]\n",
      " [ 0.04755094]\n",
      " [ 0.040552  ]]\n",
      "the iterations of  109300 the loss is 0.37751293733384583 theta= [[-5.11507998]\n",
      " [ 0.04757498]\n",
      " [ 0.04057698]]\n",
      "the iterations of  109400 the loss is 0.3774123392119485 theta= [[-5.11825173]\n",
      " [ 0.04759901]\n",
      " [ 0.04060194]]\n",
      "the iterations of  109500 the loss is 0.3773118507031103 theta= [[-5.12142175]\n",
      " [ 0.04762303]\n",
      " [ 0.04062689]]\n",
      "the iterations of  109600 the loss is 0.3772114716326481 theta= [[-5.12459004]\n",
      " [ 0.04764704]\n",
      " [ 0.04065182]]\n",
      "the iterations of  109700 the loss is 0.3771112018262322 theta= [[-5.12775661]\n",
      " [ 0.04767103]\n",
      " [ 0.04067674]]\n",
      "the iterations of  109800 the loss is 0.37701104110988454 theta= [[-5.13092146]\n",
      " [ 0.04769501]\n",
      " [ 0.04070165]]\n",
      "the iterations of  109900 the loss is 0.37691098930997896 theta= [[-5.13408458]\n",
      " [ 0.04771898]\n",
      " [ 0.04072654]]\n",
      "the iterations of  110000 the loss is 0.3768110462532393 theta= [[-5.13724599]\n",
      " [ 0.04774294]\n",
      " [ 0.04075142]]\n",
      "the iterations of  110100 the loss is 0.37671121176674 theta= [[-5.14040567]\n",
      " [ 0.04776689]\n",
      " [ 0.04077629]]\n",
      "the iterations of  110200 the loss is 0.37661148567790415 theta= [[-5.14356364]\n",
      " [ 0.04779082]\n",
      " [ 0.04080114]]\n",
      "the iterations of  110300 the loss is 0.3765118678145026 theta= [[-5.1467199 ]\n",
      " [ 0.04781474]\n",
      " [ 0.04082598]]\n",
      "the iterations of  110400 the loss is 0.37641235800465384 theta= [[-5.14987445]\n",
      " [ 0.04783865]\n",
      " [ 0.04085081]]\n",
      "the iterations of  110500 the loss is 0.3763129560768229 theta= [[-5.15302728]\n",
      " [ 0.04786255]\n",
      " [ 0.04087562]]\n",
      "the iterations of  110600 the loss is 0.3762136618598205 theta= [[-5.15617841]\n",
      " [ 0.04788644]\n",
      " [ 0.04090042]]\n",
      "the iterations of  110700 the loss is 0.3761144751828024 theta= [[-5.15932782]\n",
      " [ 0.04791031]\n",
      " [ 0.04092521]]\n",
      "the iterations of  110800 the loss is 0.3760153958752675 theta= [[-5.16247554]\n",
      " [ 0.04793417]\n",
      " [ 0.04094998]]\n",
      "the iterations of  110900 the loss is 0.37591642376705925 theta= [[-5.16562155]\n",
      " [ 0.04795802]\n",
      " [ 0.04097474]]\n",
      "the iterations of  111000 the loss is 0.3758175586883628 theta= [[-5.16876586]\n",
      " [ 0.04798186]\n",
      " [ 0.04099949]]\n",
      "the iterations of  111100 the loss is 0.3757188004697052 theta= [[-5.17190846]\n",
      " [ 0.04800569]\n",
      " [ 0.04102422]]\n",
      "the iterations of  111200 the loss is 0.375620148941954 theta= [[-5.17504938]\n",
      " [ 0.0480295 ]\n",
      " [ 0.04104894]]\n",
      "the iterations of  111300 the loss is 0.3755216039363175 theta= [[-5.17818859]\n",
      " [ 0.04805331]\n",
      " [ 0.04107365]]\n",
      "the iterations of  111400 the loss is 0.37542316528434305 theta= [[-5.18132611]\n",
      " [ 0.0480771 ]\n",
      " [ 0.04109834]]\n",
      "the iterations of  111500 the loss is 0.37532483281791573 theta= [[-5.18446194]\n",
      " [ 0.04810088]\n",
      " [ 0.04112302]]\n",
      "the iterations of  111600 the loss is 0.3752266063692592 theta= [[-5.18759607]\n",
      " [ 0.04812464]\n",
      " [ 0.04114769]]\n",
      "the iterations of  111700 the loss is 0.3751284857709332 theta= [[-5.19072852]\n",
      " [ 0.0481484 ]\n",
      " [ 0.04117234]]\n",
      "the iterations of  111800 the loss is 0.3750304708558348 theta= [[-5.19385928]\n",
      " [ 0.04817214]\n",
      " [ 0.04119698]]\n",
      "the iterations of  111900 the loss is 0.37493256145719495 theta= [[-5.19698835]\n",
      " [ 0.04819587]\n",
      " [ 0.04122161]]\n",
      "the iterations of  112000 the loss is 0.3748347574085802 theta= [[-5.20011574]\n",
      " [ 0.04821959]\n",
      " [ 0.04124622]]\n",
      "the iterations of  112100 the loss is 0.37473705854389006 theta= [[-5.20324145]\n",
      " [ 0.0482433 ]\n",
      " [ 0.04127082]]\n",
      "the iterations of  112200 the loss is 0.37463946469735765 theta= [[-5.20636548]\n",
      " [ 0.048267  ]\n",
      " [ 0.04129541]]\n",
      "the iterations of  112300 the loss is 0.3745419757035479 theta= [[-5.20948783]\n",
      " [ 0.04829068]\n",
      " [ 0.04131999]]\n",
      "the iterations of  112400 the loss is 0.37444459139735714 theta= [[-5.2126085 ]\n",
      " [ 0.04831436]\n",
      " [ 0.04134455]]\n",
      "the iterations of  112500 the loss is 0.3743473116140126 theta= [[-5.2157275 ]\n",
      " [ 0.04833802]\n",
      " [ 0.0413691 ]]\n",
      "the iterations of  112600 the loss is 0.3742501361890709 theta= [[-5.21884482]\n",
      " [ 0.04836167]\n",
      " [ 0.04139363]]\n",
      "the iterations of  112700 the loss is 0.3741530649584183 theta= [[-5.22196047]\n",
      " [ 0.0483853 ]\n",
      " [ 0.04141815]]\n",
      "the iterations of  112800 the loss is 0.374056097758269 theta= [[-5.22507445]\n",
      " [ 0.04840893]\n",
      " [ 0.04144266]]\n",
      "the iterations of  112900 the loss is 0.37395923442516493 theta= [[-5.22818676]\n",
      " [ 0.04843254]\n",
      " [ 0.04146716]]\n",
      "the iterations of  113000 the loss is 0.3738624747959747 theta= [[-5.23129741]\n",
      " [ 0.04845615]\n",
      " [ 0.04149164]]\n",
      "the iterations of  113100 the loss is 0.3737658187078931 theta= [[-5.23440639]\n",
      " [ 0.04847974]\n",
      " [ 0.04151611]]\n",
      "the iterations of  113200 the loss is 0.37366926599843986 theta= [[-5.23751371]\n",
      " [ 0.04850332]\n",
      " [ 0.04154057]]\n",
      "the iterations of  113300 the loss is 0.37357281650545976 theta= [[-5.24061937]\n",
      " [ 0.04852688]\n",
      " [ 0.04156501]]\n",
      "the iterations of  113400 the loss is 0.3734764700671207 theta= [[-5.24372336]\n",
      " [ 0.04855044]\n",
      " [ 0.04158944]]\n",
      "the iterations of  113500 the loss is 0.373380226521914 theta= [[-5.2468257 ]\n",
      " [ 0.04857399]\n",
      " [ 0.04161386]]\n",
      "the iterations of  113600 the loss is 0.37328408570865307 theta= [[-5.24992639]\n",
      " [ 0.04859752]\n",
      " [ 0.04163827]]\n",
      "the iterations of  113700 the loss is 0.3731880474664729 theta= [[-5.25302542]\n",
      " [ 0.04862104]\n",
      " [ 0.04166266]]\n",
      "the iterations of  113800 the loss is 0.3730921116348293 theta= [[-5.25612279]\n",
      " [ 0.04864455]\n",
      " [ 0.04168704]]\n",
      "the iterations of  113900 the loss is 0.3729962780534974 theta= [[-5.25921852]\n",
      " [ 0.04866805]\n",
      " [ 0.04171141]]\n",
      "the iterations of  114000 the loss is 0.37290054656257254 theta= [[-5.26231259]\n",
      " [ 0.04869153]\n",
      " [ 0.04173576]]\n",
      "the iterations of  114100 the loss is 0.372804917002468 theta= [[-5.26540502]\n",
      " [ 0.04871501]\n",
      " [ 0.0417601 ]]\n",
      "the iterations of  114200 the loss is 0.37270938921391483 theta= [[-5.2684958 ]\n",
      " [ 0.04873847]\n",
      " [ 0.04178443]]\n",
      "the iterations of  114300 the loss is 0.37261396303796146 theta= [[-5.27158494]\n",
      " [ 0.04876192]\n",
      " [ 0.04180874]]\n",
      "the iterations of  114400 the loss is 0.37251863831597243 theta= [[-5.27467243]\n",
      " [ 0.04878536]\n",
      " [ 0.04183304]]\n",
      "the iterations of  114500 the loss is 0.37242341488962727 theta= [[-5.27775829]\n",
      " [ 0.04880879]\n",
      " [ 0.04185733]]\n",
      "the iterations of  114600 the loss is 0.3723282926009213 theta= [[-5.2808425 ]\n",
      " [ 0.04883221]\n",
      " [ 0.04188161]]\n",
      "the iterations of  114700 the loss is 0.372233271292163 theta= [[-5.28392508]\n",
      " [ 0.04885562]\n",
      " [ 0.04190587]]\n",
      "the iterations of  114800 the loss is 0.3721383508059749 theta= [[-5.28700602]\n",
      " [ 0.04887901]\n",
      " [ 0.04193012]]\n",
      "the iterations of  114900 the loss is 0.3720435309852915 theta= [[-5.29008533]\n",
      " [ 0.04890239]\n",
      " [ 0.04195436]]\n",
      "the iterations of  115000 the loss is 0.3719488116733602 theta= [[-5.293163  ]\n",
      " [ 0.04892576]\n",
      " [ 0.04197859]]\n",
      "the iterations of  115100 the loss is 0.37185419271373843 theta= [[-5.29623905]\n",
      " [ 0.04894912]\n",
      " [ 0.0420028 ]]\n",
      "the iterations of  115200 the loss is 0.37175967395029474 theta= [[-5.29931346]\n",
      " [ 0.04897247]\n",
      " [ 0.042027  ]]\n",
      "the iterations of  115300 the loss is 0.37166525522720706 theta= [[-5.30238625]\n",
      " [ 0.04899581]\n",
      " [ 0.04205118]]\n",
      "the iterations of  115400 the loss is 0.3715709363889625 theta= [[-5.30545741]\n",
      " [ 0.04901914]\n",
      " [ 0.04207536]]\n",
      "the iterations of  115500 the loss is 0.3714767172803567 theta= [[-5.30852695]\n",
      " [ 0.04904245]\n",
      " [ 0.04209952]]\n",
      "the iterations of  115600 the loss is 0.3713825977464923 theta= [[-5.31159486]\n",
      " [ 0.04906575]\n",
      " [ 0.04212367]]\n",
      "the iterations of  115700 the loss is 0.3712885776327792 theta= [[-5.31466115]\n",
      " [ 0.04908904]\n",
      " [ 0.0421478 ]]\n",
      "the iterations of  115800 the loss is 0.3711946567849338 theta= [[-5.31772583]\n",
      " [ 0.04911232]\n",
      " [ 0.04217193]]\n",
      "the iterations of  115900 the loss is 0.37110083504897673 theta= [[-5.32078889]\n",
      " [ 0.04913559]\n",
      " [ 0.04219604]]\n",
      "the iterations of  116000 the loss is 0.3710071122712348 theta= [[-5.32385033]\n",
      " [ 0.04915885]\n",
      " [ 0.04222014]]\n",
      "the iterations of  116100 the loss is 0.37091348829833726 theta= [[-5.32691015]\n",
      " [ 0.0491821 ]\n",
      " [ 0.04224422]]\n",
      "the iterations of  116200 the loss is 0.37081996297721836 theta= [[-5.32996837]\n",
      " [ 0.04920533]\n",
      " [ 0.0422683 ]]\n",
      "the iterations of  116300 the loss is 0.3707265361551139 theta= [[-5.33302497]\n",
      " [ 0.04922855]\n",
      " [ 0.04229236]]\n",
      "the iterations of  116400 the loss is 0.370633207679562 theta= [[-5.33607997]\n",
      " [ 0.04925177]\n",
      " [ 0.0423164 ]]\n",
      "the iterations of  116500 the loss is 0.37053997739840133 theta= [[-5.33913335]\n",
      " [ 0.04927497]\n",
      " [ 0.04234044]]\n",
      "the iterations of  116600 the loss is 0.3704468451597717 theta= [[-5.34218513]\n",
      " [ 0.04929816]\n",
      " [ 0.04236446]]\n",
      "the iterations of  116700 the loss is 0.37035381081211305 theta= [[-5.34523531]\n",
      " [ 0.04932133]\n",
      " [ 0.04238847]]\n",
      "the iterations of  116800 the loss is 0.37026087420416376 theta= [[-5.34828389]\n",
      " [ 0.0493445 ]\n",
      " [ 0.04241247]]\n",
      "the iterations of  116900 the loss is 0.3701680351849605 theta= [[-5.35133086]\n",
      " [ 0.04936766]\n",
      " [ 0.04243645]]\n",
      "the iterations of  117000 the loss is 0.3700752936038383 theta= [[-5.35437623]\n",
      " [ 0.0493908 ]\n",
      " [ 0.04246042]]\n",
      "the iterations of  117100 the loss is 0.3699826493104291 theta= [[-5.35742001]\n",
      " [ 0.04941393]\n",
      " [ 0.04248438]]\n",
      "the iterations of  117200 the loss is 0.3698901021546608 theta= [[-5.36046219]\n",
      " [ 0.04943705]\n",
      " [ 0.04250833]]\n",
      "the iterations of  117300 the loss is 0.36979765198675757 theta= [[-5.36350277]\n",
      " [ 0.04946017]\n",
      " [ 0.04253227]]\n",
      "the iterations of  117400 the loss is 0.36970529865723817 theta= [[-5.36654176]\n",
      " [ 0.04948326]\n",
      " [ 0.04255619]]\n",
      "the iterations of  117500 the loss is 0.3696130420169158 theta= [[-5.36957916]\n",
      " [ 0.04950635]\n",
      " [ 0.0425801 ]]\n",
      "the iterations of  117600 the loss is 0.36952088191689697 theta= [[-5.37261497]\n",
      " [ 0.04952943]\n",
      " [ 0.042604  ]]\n",
      "the iterations of  117700 the loss is 0.36942881820858164 theta= [[-5.3756492 ]\n",
      " [ 0.0495525 ]\n",
      " [ 0.04262788]]\n",
      "the iterations of  117800 the loss is 0.3693368507436615 theta= [[-5.37868183]\n",
      " [ 0.04957555]\n",
      " [ 0.04265175]]\n",
      "the iterations of  117900 the loss is 0.36924497937412065 theta= [[-5.38171289]\n",
      " [ 0.04959859]\n",
      " [ 0.04267562]]\n",
      "the iterations of  118000 the loss is 0.3691532039522331 theta= [[-5.38474235]\n",
      " [ 0.04962163]\n",
      " [ 0.04269946]]\n",
      "the iterations of  118100 the loss is 0.36906152433056405 theta= [[-5.38777024]\n",
      " [ 0.04964465]\n",
      " [ 0.0427233 ]]\n",
      "the iterations of  118200 the loss is 0.36896994036196745 theta= [[-5.39079655]\n",
      " [ 0.04966766]\n",
      " [ 0.04274712]]\n",
      "the iterations of  118300 the loss is 0.3688784518995867 theta= [[-5.39382128]\n",
      " [ 0.04969066]\n",
      " [ 0.04277093]]\n",
      "the iterations of  118400 the loss is 0.3687870587968534 theta= [[-5.39684443]\n",
      " [ 0.04971364]\n",
      " [ 0.04279473]]\n",
      "the iterations of  118500 the loss is 0.36869576090748674 theta= [[-5.399866  ]\n",
      " [ 0.04973662]\n",
      " [ 0.04281852]]\n",
      "the iterations of  118600 the loss is 0.36860455808549253 theta= [[-5.40288601]\n",
      " [ 0.04975959]\n",
      " [ 0.04284229]]\n",
      "the iterations of  118700 the loss is 0.3685134501851629 theta= [[-5.40590444]\n",
      " [ 0.04978254]\n",
      " [ 0.04286605]]\n",
      "the iterations of  118800 the loss is 0.3684224370610761 theta= [[-5.4089213 ]\n",
      " [ 0.04980549]\n",
      " [ 0.0428898 ]]\n",
      "the iterations of  118900 the loss is 0.3683315185680947 theta= [[-5.41193659]\n",
      " [ 0.04982842]\n",
      " [ 0.04291354]]\n",
      "the iterations of  119000 the loss is 0.3682406945613658 theta= [[-5.41495031]\n",
      " [ 0.04985134]\n",
      " [ 0.04293726]]\n",
      "the iterations of  119100 the loss is 0.36814996489632024 theta= [[-5.41796247]\n",
      " [ 0.04987425]\n",
      " [ 0.04296098]]\n",
      "the iterations of  119200 the loss is 0.3680593294286714 theta= [[-5.42097306]\n",
      " [ 0.04989715]\n",
      " [ 0.04298468]]\n",
      "the iterations of  119300 the loss is 0.36796878801441535 theta= [[-5.4239821 ]\n",
      " [ 0.04992004]\n",
      " [ 0.04300837]]\n",
      "the iterations of  119400 the loss is 0.3678783405098299 theta= [[-5.42698957]\n",
      " [ 0.04994291]\n",
      " [ 0.04303204]]\n",
      "the iterations of  119500 the loss is 0.3677879867714738 theta= [[-5.42999548]\n",
      " [ 0.04996578]\n",
      " [ 0.04305571]]\n",
      "the iterations of  119600 the loss is 0.36769772665618594 theta= [[-5.43299983]\n",
      " [ 0.04998864]\n",
      " [ 0.04307936]]\n",
      "the iterations of  119700 the loss is 0.36760756002108536 theta= [[-5.43600263]\n",
      " [ 0.05001148]\n",
      " [ 0.043103  ]]\n",
      "the iterations of  119800 the loss is 0.36751748672357 theta= [[-5.43900387]\n",
      " [ 0.05003431]\n",
      " [ 0.04312662]]\n",
      "the iterations of  119900 the loss is 0.3674275066213156 theta= [[-5.44200356]\n",
      " [ 0.05005714]\n",
      " [ 0.04315024]]\n",
      "the iterations of  120000 the loss is 0.367337619572277 theta= [[-5.4450017 ]\n",
      " [ 0.05007995]\n",
      " [ 0.04317384]]\n",
      "the iterations of  120100 the loss is 0.36724782543468476 theta= [[-5.44799829]\n",
      " [ 0.05010275]\n",
      " [ 0.04319743]]\n",
      "the iterations of  120200 the loss is 0.3671581240670476 theta= [[-5.45099333]\n",
      " [ 0.05012554]\n",
      " [ 0.04322101]]\n",
      "the iterations of  120300 the loss is 0.3670685153281487 theta= [[-5.45398682]\n",
      " [ 0.05014832]\n",
      " [ 0.04324458]]\n",
      "the iterations of  120400 the loss is 0.3669789990770474 theta= [[-5.45697877]\n",
      " [ 0.05017109]\n",
      " [ 0.04326813]]\n",
      "the iterations of  120500 the loss is 0.3668895751730769 theta= [[-5.45996917]\n",
      " [ 0.05019384]\n",
      " [ 0.04329167]]\n",
      "the iterations of  120600 the loss is 0.36680024347584533 theta= [[-5.46295804]\n",
      " [ 0.05021659]\n",
      " [ 0.0433152 ]]\n",
      "the iterations of  120700 the loss is 0.36671100384523336 theta= [[-5.46594536]\n",
      " [ 0.05023932]\n",
      " [ 0.04333872]]\n",
      "the iterations of  120800 the loss is 0.36662185614139503 theta= [[-5.46893114]\n",
      " [ 0.05026205]\n",
      " [ 0.04336223]]\n",
      "the iterations of  120900 the loss is 0.3665328002247557 theta= [[-5.47191538]\n",
      " [ 0.05028476]\n",
      " [ 0.04338572]]\n",
      "the iterations of  121000 the loss is 0.366443835956013 theta= [[-5.47489809]\n",
      " [ 0.05030746]\n",
      " [ 0.04340921]]\n",
      "the iterations of  121100 the loss is 0.3663549631961354 theta= [[-5.47787927]\n",
      " [ 0.05033016]\n",
      " [ 0.04343268]]\n",
      "the iterations of  121200 the loss is 0.36626618180636106 theta= [[-5.48085891]\n",
      " [ 0.05035284]\n",
      " [ 0.04345613]]\n",
      "the iterations of  121300 the loss is 0.3661774916481982 theta= [[-5.48383702]\n",
      " [ 0.05037551]\n",
      " [ 0.04347958]]\n",
      "the iterations of  121400 the loss is 0.36608889258342375 theta= [[-5.4868136 ]\n",
      " [ 0.05039817]\n",
      " [ 0.04350302]]\n",
      "the iterations of  121500 the loss is 0.36600038447408356 theta= [[-5.48978865]\n",
      " [ 0.05042082]\n",
      " [ 0.04352644]]\n",
      "the iterations of  121600 the loss is 0.36591196718249075 theta= [[-5.49276218]\n",
      " [ 0.05044345]\n",
      " [ 0.04354985]]\n",
      "the iterations of  121700 the loss is 0.36582364057122585 theta= [[-5.49573418]\n",
      " [ 0.05046608]\n",
      " [ 0.04357325]]\n",
      "the iterations of  121800 the loss is 0.3657354045031358 theta= [[-5.49870465]\n",
      " [ 0.0504887 ]\n",
      " [ 0.04359663]]\n",
      "the iterations of  121900 the loss is 0.36564725884133387 theta= [[-5.50167361]\n",
      " [ 0.0505113 ]\n",
      " [ 0.04362001]]\n",
      "the iterations of  122000 the loss is 0.3655592034491977 theta= [[-5.50464104]\n",
      " [ 0.0505339 ]\n",
      " [ 0.04364337]]\n",
      "the iterations of  122100 the loss is 0.36547123819037125 theta= [[-5.50760695]\n",
      " [ 0.05055648]\n",
      " [ 0.04366672]]\n",
      "the iterations of  122200 the loss is 0.36538336292876095 theta= [[-5.51057135]\n",
      " [ 0.05057905]\n",
      " [ 0.04369006]]\n",
      "the iterations of  122300 the loss is 0.3652955775285379 theta= [[-5.51353423]\n",
      " [ 0.05060162]\n",
      " [ 0.04371339]]\n",
      "the iterations of  122400 the loss is 0.36520788185413544 theta= [[-5.51649559]\n",
      " [ 0.05062417]\n",
      " [ 0.0437367 ]]\n",
      "the iterations of  122500 the loss is 0.3651202757702496 theta= [[-5.51945545]\n",
      " [ 0.05064671]\n",
      " [ 0.04376001]]\n",
      "the iterations of  122600 the loss is 0.3650327591418383 theta= [[-5.52241379]\n",
      " [ 0.05066924]\n",
      " [ 0.0437833 ]]\n",
      "the iterations of  122700 the loss is 0.3649453318341198 theta= [[-5.52537062]\n",
      " [ 0.05069176]\n",
      " [ 0.04380658]]\n",
      "the iterations of  122800 the loss is 0.36485799371257416 theta= [[-5.52832594]\n",
      " [ 0.05071427]\n",
      " [ 0.04382985]]\n",
      "the iterations of  122900 the loss is 0.36477074464294 theta= [[-5.53127976]\n",
      " [ 0.05073677]\n",
      " [ 0.0438531 ]]\n",
      "the iterations of  123000 the loss is 0.3646835844912161 theta= [[-5.53423207]\n",
      " [ 0.05075926]\n",
      " [ 0.04387635]]\n",
      "the iterations of  123100 the loss is 0.36459651312365987 theta= [[-5.53718287]\n",
      " [ 0.05078173]\n",
      " [ 0.04389958]]\n",
      "the iterations of  123200 the loss is 0.3645095304067867 theta= [[-5.54013217]\n",
      " [ 0.0508042 ]\n",
      " [ 0.0439228 ]]\n",
      "the iterations of  123300 the loss is 0.3644226362073693 theta= [[-5.54307998]\n",
      " [ 0.05082666]\n",
      " [ 0.04394601]]\n",
      "the iterations of  123400 the loss is 0.3643358303924382 theta= [[-5.54602628]\n",
      " [ 0.0508491 ]\n",
      " [ 0.04396921]]\n",
      "the iterations of  123500 the loss is 0.36424911282927974 theta= [[-5.54897108]\n",
      " [ 0.05087154]\n",
      " [ 0.04399239]]\n",
      "the iterations of  123600 the loss is 0.36416248338543594 theta= [[-5.55191439]\n",
      " [ 0.05089396]\n",
      " [ 0.04401557]]\n",
      "the iterations of  123700 the loss is 0.36407594192870457 theta= [[-5.5548562 ]\n",
      " [ 0.05091637]\n",
      " [ 0.04403873]]\n",
      "the iterations of  123800 the loss is 0.36398948832713734 theta= [[-5.55779652]\n",
      " [ 0.05093878]\n",
      " [ 0.04406188]]\n",
      "the iterations of  123900 the loss is 0.363903122449041 theta= [[-5.56073535]\n",
      " [ 0.05096117]\n",
      " [ 0.04408502]]\n",
      "the iterations of  124000 the loss is 0.36381684416297466 theta= [[-5.56367268]\n",
      " [ 0.05098355]\n",
      " [ 0.04410815]]\n",
      "the iterations of  124100 the loss is 0.36373065333775095 theta= [[-5.56660853]\n",
      " [ 0.05100592]\n",
      " [ 0.04413126]]\n",
      "the iterations of  124200 the loss is 0.36364454984243494 theta= [[-5.56954289]\n",
      " [ 0.05102828]\n",
      " [ 0.04415437]]\n",
      "the iterations of  124300 the loss is 0.36355853354634327 theta= [[-5.57247576]\n",
      " [ 0.05105063]\n",
      " [ 0.04417746]]\n",
      "the iterations of  124400 the loss is 0.36347260431904393 theta= [[-5.57540715]\n",
      " [ 0.05107297]\n",
      " [ 0.04420054]]\n",
      "the iterations of  124500 the loss is 0.3633867620303553 theta= [[-5.57833705]\n",
      " [ 0.0510953 ]\n",
      " [ 0.04422361]]\n",
      "the iterations of  124600 the loss is 0.3633010065503459 theta= [[-5.58126548]\n",
      " [ 0.05111762]\n",
      " [ 0.04424667]]\n",
      "the iterations of  124700 the loss is 0.3632153377493338 theta= [[-5.58419242]\n",
      " [ 0.05113993]\n",
      " [ 0.04426972]]\n",
      "the iterations of  124800 the loss is 0.3631297554978859 theta= [[-5.58711788]\n",
      " [ 0.05116223]\n",
      " [ 0.04429275]]\n",
      "the iterations of  124900 the loss is 0.36304425966681714 theta= [[-5.59004187]\n",
      " [ 0.05118451]\n",
      " [ 0.04431577]]\n",
      "the iterations of  125000 the loss is 0.3629588501271912 theta= [[-5.59296438]\n",
      " [ 0.05120679]\n",
      " [ 0.04433879]]\n",
      "the iterations of  125100 the loss is 0.36287352675031775 theta= [[-5.59588541]\n",
      " [ 0.05122906]\n",
      " [ 0.04436179]]\n",
      "the iterations of  125200 the loss is 0.36278828940775426 theta= [[-5.59880497]\n",
      " [ 0.05125131]\n",
      " [ 0.04438478]]\n",
      "the iterations of  125300 the loss is 0.36270313797130344 theta= [[-5.60172306]\n",
      " [ 0.05127356]\n",
      " [ 0.04440775]]\n",
      "the iterations of  125400 the loss is 0.36261807231301396 theta= [[-5.60463968]\n",
      " [ 0.05129579]\n",
      " [ 0.04443072]]\n",
      "the iterations of  125500 the loss is 0.3625330923051792 theta= [[-5.60755484]\n",
      " [ 0.05131802]\n",
      " [ 0.04445367]]\n",
      "the iterations of  125600 the loss is 0.36244819782033744 theta= [[-5.61046852]\n",
      " [ 0.05134023]\n",
      " [ 0.04447661]]\n",
      "the iterations of  125700 the loss is 0.36236338873127055 theta= [[-5.61338074]\n",
      " [ 0.05136244]\n",
      " [ 0.04449955]]\n",
      "the iterations of  125800 the loss is 0.3622786649110035 theta= [[-5.61629149]\n",
      " [ 0.05138463]\n",
      " [ 0.04452247]]\n",
      "the iterations of  125900 the loss is 0.3621940262328045 theta= [[-5.61920078]\n",
      " [ 0.05140681]\n",
      " [ 0.04454537]]\n",
      "the iterations of  126000 the loss is 0.3621094725701836 theta= [[-5.62210861]\n",
      " [ 0.05142899]\n",
      " [ 0.04456827]]\n",
      "the iterations of  126100 the loss is 0.3620250037968926 theta= [[-5.62501498]\n",
      " [ 0.05145115]\n",
      " [ 0.04459116]]\n",
      "the iterations of  126200 the loss is 0.36194061978692454 theta= [[-5.62791989]\n",
      " [ 0.0514733 ]\n",
      " [ 0.04461403]]\n",
      "the iterations of  126300 the loss is 0.3618563204145132 theta= [[-5.63082335]\n",
      " [ 0.05149544]\n",
      " [ 0.04463689]]\n",
      "the iterations of  126400 the loss is 0.36177210555413203 theta= [[-5.63372534]\n",
      " [ 0.05151758]\n",
      " [ 0.04465974]]\n",
      "the iterations of  126500 the loss is 0.36168797508049394 theta= [[-5.63662589]\n",
      " [ 0.0515397 ]\n",
      " [ 0.04468258]]\n",
      "the iterations of  126600 the loss is 0.36160392886855147 theta= [[-5.63952498]\n",
      " [ 0.05156181]\n",
      " [ 0.04470541]]\n",
      "the iterations of  126700 the loss is 0.36151996679349474 theta= [[-5.64242262]\n",
      " [ 0.05158391]\n",
      " [ 0.04472823]]\n",
      "the iterations of  126800 the loss is 0.3614360887307521 theta= [[-5.64531881]\n",
      " [ 0.051606  ]\n",
      " [ 0.04475104]]\n",
      "the iterations of  126900 the loss is 0.36135229455598916 theta= [[-5.64821355]\n",
      " [ 0.05162808]\n",
      " [ 0.04477383]]\n",
      "the iterations of  127000 the loss is 0.36126858414510854 theta= [[-5.65110684]\n",
      " [ 0.05165015]\n",
      " [ 0.04479661]]\n",
      "the iterations of  127100 the loss is 0.36118495737424927 theta= [[-5.65399869]\n",
      " [ 0.05167221]\n",
      " [ 0.04481939]]\n",
      "the iterations of  127200 the loss is 0.36110141411978525 theta= [[-5.65688909]\n",
      " [ 0.05169426]\n",
      " [ 0.04484215]]\n",
      "the iterations of  127300 the loss is 0.3610179542583264 theta= [[-5.65977805]\n",
      " [ 0.0517163 ]\n",
      " [ 0.0448649 ]]\n",
      "the iterations of  127400 the loss is 0.36093457766671727 theta= [[-5.66266557]\n",
      " [ 0.05173832]\n",
      " [ 0.04488764]]\n",
      "the iterations of  127500 the loss is 0.3608512842220361 theta= [[-5.66555164]\n",
      " [ 0.05176034]\n",
      " [ 0.04491036]]\n",
      "the iterations of  127600 the loss is 0.3607680738015949 theta= [[-5.66843628]\n",
      " [ 0.05178235]\n",
      " [ 0.04493308]]\n",
      "the iterations of  127700 the loss is 0.36068494628293934 theta= [[-5.67131948]\n",
      " [ 0.05180435]\n",
      " [ 0.04495578]]\n",
      "the iterations of  127800 the loss is 0.36060190154384636 theta= [[-5.67420125]\n",
      " [ 0.05182634]\n",
      " [ 0.04497848]]\n",
      "the iterations of  127900 the loss is 0.36051893946232644 theta= [[-5.67708158]\n",
      " [ 0.05184832]\n",
      " [ 0.04500116]]\n",
      "the iterations of  128000 the loss is 0.3604360599166205 theta= [[-5.67996048]\n",
      " [ 0.05187028]\n",
      " [ 0.04502383]]\n",
      "the iterations of  128100 the loss is 0.36035326278520075 theta= [[-5.68283794]\n",
      " [ 0.05189224]\n",
      " [ 0.04504649]]\n",
      "the iterations of  128200 the loss is 0.36027054794677027 theta= [[-5.68571398]\n",
      " [ 0.05191419]\n",
      " [ 0.04506914]]\n",
      "the iterations of  128300 the loss is 0.3601879152802616 theta= [[-5.68858859]\n",
      " [ 0.05193612]\n",
      " [ 0.04509177]]\n",
      "the iterations of  128400 the loss is 0.36010536466483684 theta= [[-5.69146177]\n",
      " [ 0.05195805]\n",
      " [ 0.0451144 ]]\n",
      "the iterations of  128500 the loss is 0.36002289597988674 theta= [[-5.69433352]\n",
      " [ 0.05197997]\n",
      " [ 0.04513702]]\n",
      "the iterations of  128600 the loss is 0.3599405091050313 theta= [[-5.69720385]\n",
      " [ 0.05200187]\n",
      " [ 0.04515962]]\n",
      "the iterations of  128700 the loss is 0.35985820392011747 theta= [[-5.70007275]\n",
      " [ 0.05202377]\n",
      " [ 0.04518221]]\n",
      "the iterations of  128800 the loss is 0.35977598030522 theta= [[-5.70294024]\n",
      " [ 0.05204566]\n",
      " [ 0.04520479]]\n",
      "the iterations of  128900 the loss is 0.3596938381406406 theta= [[-5.7058063 ]\n",
      " [ 0.05206753]\n",
      " [ 0.04522736]]\n",
      "the iterations of  129000 the loss is 0.35961177730690713 theta= [[-5.70867094]\n",
      " [ 0.0520894 ]\n",
      " [ 0.04524992]]\n",
      "the iterations of  129100 the loss is 0.35952979768477356 theta= [[-5.71153417]\n",
      " [ 0.05211126]\n",
      " [ 0.04527247]]\n",
      "the iterations of  129200 the loss is 0.3594478991552188 theta= [[-5.71439597]\n",
      " [ 0.0521331 ]\n",
      " [ 0.04529501]]\n",
      "the iterations of  129300 the loss is 0.3593660815994468 theta= [[-5.71725637]\n",
      " [ 0.05215494]\n",
      " [ 0.04531754]]\n",
      "the iterations of  129400 the loss is 0.35928434489888583 theta= [[-5.72011535]\n",
      " [ 0.05217677]\n",
      " [ 0.04534005]]\n",
      "the iterations of  129500 the loss is 0.3592026889351881 theta= [[-5.72297291]\n",
      " [ 0.05219858]\n",
      " [ 0.04536255]]\n",
      "the iterations of  129600 the loss is 0.35912111359022886 theta= [[-5.72582907]\n",
      " [ 0.05222039]\n",
      " [ 0.04538505]]\n",
      "the iterations of  129700 the loss is 0.35903961874610657 theta= [[-5.72868382]\n",
      " [ 0.05224218]\n",
      " [ 0.04540753]]\n",
      "the iterations of  129800 the loss is 0.3589582042851416 theta= [[-5.73153716]\n",
      " [ 0.05226397]\n",
      " [ 0.04543   ]]\n",
      "the iterations of  129900 the loss is 0.3588768700898766 theta= [[-5.73438909]\n",
      " [ 0.05228574]\n",
      " [ 0.04545246]]\n",
      "the iterations of  130000 the loss is 0.35879561604307497 theta= [[-5.73723961]\n",
      " [ 0.05230751]\n",
      " [ 0.04547491]]\n",
      "the iterations of  130100 the loss is 0.3587144420277219 theta= [[-5.74008873]\n",
      " [ 0.05232927]\n",
      " [ 0.04549735]]\n",
      "the iterations of  130200 the loss is 0.3586333479270216 theta= [[-5.74293645]\n",
      " [ 0.05235101]\n",
      " [ 0.04551978]]\n",
      "the iterations of  130300 the loss is 0.3585523336243991 theta= [[-5.74578277]\n",
      " [ 0.05237275]\n",
      " [ 0.04554219]]\n",
      "the iterations of  130400 the loss is 0.35847139900349845 theta= [[-5.74862769]\n",
      " [ 0.05239447]\n",
      " [ 0.0455646 ]]\n",
      "the iterations of  130500 the loss is 0.35839054394818276 theta= [[-5.75147121]\n",
      " [ 0.05241619]\n",
      " [ 0.04558699]]\n",
      "the iterations of  130600 the loss is 0.35830976834253314 theta= [[-5.75431333]\n",
      " [ 0.0524379 ]\n",
      " [ 0.04560938]]\n",
      "the iterations of  130700 the loss is 0.35822907207084903 theta= [[-5.75715405]\n",
      " [ 0.05245959]\n",
      " [ 0.04563175]]\n",
      "the iterations of  130800 the loss is 0.35814845501764664 theta= [[-5.75999338]\n",
      " [ 0.05248128]\n",
      " [ 0.04565411]]\n",
      "the iterations of  130900 the loss is 0.35806791706765995 theta= [[-5.76283132]\n",
      " [ 0.05250296]\n",
      " [ 0.04567646]]\n",
      "the iterations of  131000 the loss is 0.35798745810583843 theta= [[-5.76566786]\n",
      " [ 0.05252462]\n",
      " [ 0.0456988 ]]\n",
      "the iterations of  131100 the loss is 0.35790707801734817 theta= [[-5.76850302]\n",
      " [ 0.05254628]\n",
      " [ 0.04572113]]\n",
      "the iterations of  131200 the loss is 0.3578267766875705 theta= [[-5.77133678]\n",
      " [ 0.05256792]\n",
      " [ 0.04574345]]\n",
      "the iterations of  131300 the loss is 0.3577465540021018 theta= [[-5.77416916]\n",
      " [ 0.05258956]\n",
      " [ 0.04576576]]\n",
      "the iterations of  131400 the loss is 0.3576664098467529 theta= [[-5.77700015]\n",
      " [ 0.05261119]\n",
      " [ 0.04578805]]\n",
      "the iterations of  131500 the loss is 0.3575863441075484 theta= [[-5.77982975]\n",
      " [ 0.05263281]\n",
      " [ 0.04581034]]\n",
      "the iterations of  131600 the loss is 0.357506356670727 theta= [[-5.78265797]\n",
      " [ 0.05265441]\n",
      " [ 0.04583261]]\n",
      "the iterations of  131700 the loss is 0.3574264474227399 theta= [[-5.78548481]\n",
      " [ 0.05267601]\n",
      " [ 0.04585488]]\n",
      "the iterations of  131800 the loss is 0.3573466162502512 theta= [[-5.78831027]\n",
      " [ 0.0526976 ]\n",
      " [ 0.04587713]]\n",
      "the iterations of  131900 the loss is 0.3572668630401369 theta= [[-5.79113434]\n",
      " [ 0.05271917]\n",
      " [ 0.04589937]]\n",
      "the iterations of  132000 the loss is 0.35718718767948504 theta= [[-5.79395704]\n",
      " [ 0.05274074]\n",
      " [ 0.04592161]]\n",
      "the iterations of  132100 the loss is 0.3571075900555942 theta= [[-5.79677836]\n",
      " [ 0.0527623 ]\n",
      " [ 0.04594383]]\n",
      "the iterations of  132200 the loss is 0.35702807005597464 theta= [[-5.79959831]\n",
      " [ 0.05278385]\n",
      " [ 0.04596604]]\n",
      "the iterations of  132300 the loss is 0.35694862756834556 theta= [[-5.80241688]\n",
      " [ 0.05280539]\n",
      " [ 0.04598824]]\n",
      "the iterations of  132400 the loss is 0.35686926248063683 theta= [[-5.80523407]\n",
      " [ 0.05282692]\n",
      " [ 0.04601042]]\n",
      "the iterations of  132500 the loss is 0.35678997468098755 theta= [[-5.8080499 ]\n",
      " [ 0.05284843]\n",
      " [ 0.0460326 ]]\n",
      "the iterations of  132600 the loss is 0.35671076405774527 theta= [[-5.81086435]\n",
      " [ 0.05286994]\n",
      " [ 0.04605477]]\n",
      "the iterations of  132700 the loss is 0.35663163049946633 theta= [[-5.81367743]\n",
      " [ 0.05289144]\n",
      " [ 0.04607693]]\n",
      "the iterations of  132800 the loss is 0.35655257389491474 theta= [[-5.81648915]\n",
      " [ 0.05291293]\n",
      " [ 0.04609907]]\n",
      "the iterations of  132900 the loss is 0.3564735941330619 theta= [[-5.8192995 ]\n",
      " [ 0.05293441]\n",
      " [ 0.04612121]]\n",
      "the iterations of  133000 the loss is 0.35639469110308625 theta= [[-5.82210848]\n",
      " [ 0.05295588]\n",
      " [ 0.04614333]]\n",
      "the iterations of  133100 the loss is 0.3563158646943728 theta= [[-5.8249161 ]\n",
      " [ 0.05297734]\n",
      " [ 0.04616545]]\n",
      "the iterations of  133200 the loss is 0.3562371147965127 theta= [[-5.82772235]\n",
      " [ 0.05299879]\n",
      " [ 0.04618755]]\n",
      "the iterations of  133300 the loss is 0.3561584412993024 theta= [[-5.83052725]\n",
      " [ 0.05302023]\n",
      " [ 0.04620964]]\n",
      "the iterations of  133400 the loss is 0.35607984409274374 theta= [[-5.83333078]\n",
      " [ 0.05304166]\n",
      " [ 0.04623173]]\n",
      "the iterations of  133500 the loss is 0.3560013230670434 theta= [[-5.83613296]\n",
      " [ 0.05306309]\n",
      " [ 0.0462538 ]]\n",
      "the iterations of  133600 the loss is 0.35592287811261186 theta= [[-5.83893378]\n",
      " [ 0.0530845 ]\n",
      " [ 0.04627586]]\n",
      "the iterations of  133700 the loss is 0.3558445091200638 theta= [[-5.84173324]\n",
      " [ 0.0531059 ]\n",
      " [ 0.04629791]]\n",
      "the iterations of  133800 the loss is 0.3557662159802168 theta= [[-5.84453134]\n",
      " [ 0.05312729]\n",
      " [ 0.04631995]]\n",
      "the iterations of  133900 the loss is 0.3556879985840918 theta= [[-5.84732809]\n",
      " [ 0.05314868]\n",
      " [ 0.04634198]]\n",
      "the iterations of  134000 the loss is 0.35560985682291196 theta= [[-5.85012349]\n",
      " [ 0.05317005]\n",
      " [ 0.046364  ]]\n",
      "the iterations of  134100 the loss is 0.35553179058810214 theta= [[-5.85291754]\n",
      " [ 0.05319141]\n",
      " [ 0.046386  ]]\n",
      "the iterations of  134200 the loss is 0.3554537997712891 theta= [[-5.85571024]\n",
      " [ 0.05321277]\n",
      " [ 0.046408  ]]\n",
      "the iterations of  134300 the loss is 0.3553758842643009 theta= [[-5.85850159]\n",
      " [ 0.05323411]\n",
      " [ 0.04642999]]\n",
      "the iterations of  134400 the loss is 0.3552980439591657 theta= [[-5.86129159]\n",
      " [ 0.05325544]\n",
      " [ 0.04645196]]\n",
      "the iterations of  134500 the loss is 0.35522027874811196 theta= [[-5.86408024]\n",
      " [ 0.05327677]\n",
      " [ 0.04647393]]\n",
      "the iterations of  134600 the loss is 0.35514258852356817 theta= [[-5.86686756]\n",
      " [ 0.05329809]\n",
      " [ 0.04649589]]\n",
      "the iterations of  134700 the loss is 0.3550649731781619 theta= [[-5.86965352]\n",
      " [ 0.05331939]\n",
      " [ 0.04651783]]\n",
      "the iterations of  134800 the loss is 0.3549874326047196 theta= [[-5.87243815]\n",
      " [ 0.05334069]\n",
      " [ 0.04653977]]\n",
      "the iterations of  134900 the loss is 0.35490996669626645 theta= [[-5.87522143]\n",
      " [ 0.05336197]\n",
      " [ 0.04656169]]\n",
      "the iterations of  135000 the loss is 0.3548325753460254 theta= [[-5.87800338]\n",
      " [ 0.05338325]\n",
      " [ 0.0465836 ]]\n",
      "the iterations of  135100 the loss is 0.3547552584474172 theta= [[-5.88078398]\n",
      " [ 0.05340452]\n",
      " [ 0.04660551]]\n",
      "the iterations of  135200 the loss is 0.354678015894059 theta= [[-5.88356325]\n",
      " [ 0.05342578]\n",
      " [ 0.0466274 ]]\n",
      "the iterations of  135300 the loss is 0.35460084757976523 theta= [[-5.88634118]\n",
      " [ 0.05344702]\n",
      " [ 0.04664928]]\n",
      "the iterations of  135400 the loss is 0.3545237533985469 theta= [[-5.88911778]\n",
      " [ 0.05346826]\n",
      " [ 0.04667115]]\n",
      "the iterations of  135500 the loss is 0.35444673324460974 theta= [[-5.89189304]\n",
      " [ 0.05348949]\n",
      " [ 0.04669301]]\n",
      "the iterations of  135600 the loss is 0.3543697870123561 theta= [[-5.89466698]\n",
      " [ 0.05351071]\n",
      " [ 0.04671487]]\n",
      "the iterations of  135700 the loss is 0.3542929145963825 theta= [[-5.89743958]\n",
      " [ 0.05353192]\n",
      " [ 0.04673671]]\n",
      "the iterations of  135800 the loss is 0.35421611589148033 theta= [[-5.90021085]\n",
      " [ 0.05355312]\n",
      " [ 0.04675854]]\n",
      "the iterations of  135900 the loss is 0.3541393907926348 theta= [[-5.90298079]\n",
      " [ 0.05357432]\n",
      " [ 0.04678036]]\n",
      "the iterations of  136000 the loss is 0.35406273919502484 theta= [[-5.90574941]\n",
      " [ 0.0535955 ]\n",
      " [ 0.04680217]]\n",
      "the iterations of  136100 the loss is 0.353986160994023 theta= [[-5.9085167 ]\n",
      " [ 0.05361667]\n",
      " [ 0.04682396]]\n",
      "the iterations of  136200 the loss is 0.3539096560851942 theta= [[-5.91128266]\n",
      " [ 0.05363783]\n",
      " [ 0.04684575]]\n",
      "the iterations of  136300 the loss is 0.3538332243642959 theta= [[-5.9140473 ]\n",
      " [ 0.05365899]\n",
      " [ 0.04686753]]\n",
      "the iterations of  136400 the loss is 0.3537568657272776 theta= [[-5.91681062]\n",
      " [ 0.05368013]\n",
      " [ 0.0468893 ]]\n",
      "the iterations of  136500 the loss is 0.35368058007027997 theta= [[-5.91957262]\n",
      " [ 0.05370126]\n",
      " [ 0.04691106]]\n",
      "the iterations of  136600 the loss is 0.3536043672896356 theta= [[-5.9223333 ]\n",
      " [ 0.05372239]\n",
      " [ 0.04693281]]\n",
      "the iterations of  136700 the loss is 0.3535282272818672 theta= [[-5.92509266]\n",
      " [ 0.05374351]\n",
      " [ 0.04695454]]\n",
      "the iterations of  136800 the loss is 0.353452159943688 theta= [[-5.92785071]\n",
      " [ 0.05376461]\n",
      " [ 0.04697627]]\n",
      "the iterations of  136900 the loss is 0.3533761651720005 theta= [[-5.93060744]\n",
      " [ 0.05378571]\n",
      " [ 0.04699799]]\n",
      "the iterations of  137000 the loss is 0.3533002428638975 theta= [[-5.93336285]\n",
      " [ 0.0538068 ]\n",
      " [ 0.04701969]]\n",
      "the iterations of  137100 the loss is 0.3532243929166605 theta= [[-5.93611695]\n",
      " [ 0.05382787]\n",
      " [ 0.04704139]]\n",
      "the iterations of  137200 the loss is 0.35314861522775937 theta= [[-5.93886974]\n",
      " [ 0.05384894]\n",
      " [ 0.04706307]]\n",
      "the iterations of  137300 the loss is 0.35307290969485217 theta= [[-5.94162121]\n",
      " [ 0.05387   ]\n",
      " [ 0.04708475]]\n",
      "the iterations of  137400 the loss is 0.3529972762157854 theta= [[-5.94437138]\n",
      " [ 0.05389105]\n",
      " [ 0.04710642]]\n",
      "the iterations of  137500 the loss is 0.35292171468859235 theta= [[-5.94712024]\n",
      " [ 0.05391209]\n",
      " [ 0.04712807]]\n",
      "the iterations of  137600 the loss is 0.35284622501149343 theta= [[-5.94986779]\n",
      " [ 0.05393312]\n",
      " [ 0.04714972]]\n",
      "the iterations of  137700 the loss is 0.3527708070828956 theta= [[-5.95261404]\n",
      " [ 0.05395415]\n",
      " [ 0.04717135]]\n",
      "the iterations of  137800 the loss is 0.352695460801392 theta= [[-5.95535898]\n",
      " [ 0.05397516]\n",
      " [ 0.04719298]]\n",
      "the iterations of  137900 the loss is 0.3526201860657611 theta= [[-5.95810261]\n",
      " [ 0.05399616]\n",
      " [ 0.04721459]]\n",
      "the iterations of  138000 the loss is 0.3525449827749676 theta= [[-5.96084495]\n",
      " [ 0.05401716]\n",
      " [ 0.04723619]]\n",
      "the iterations of  138100 the loss is 0.35246985082816024 theta= [[-5.96358598]\n",
      " [ 0.05403814]\n",
      " [ 0.04725779]]\n",
      "the iterations of  138200 the loss is 0.35239479012467284 theta= [[-5.96632571]\n",
      " [ 0.05405912]\n",
      " [ 0.04727937]]\n",
      "the iterations of  138300 the loss is 0.352319800564023 theta= [[-5.96906415]\n",
      " [ 0.05408008]\n",
      " [ 0.04730095]]\n",
      "the iterations of  138400 the loss is 0.35224488204591237 theta= [[-5.97180129]\n",
      " [ 0.05410104]\n",
      " [ 0.04732251]]\n",
      "the iterations of  138500 the loss is 0.35217003447022543 theta= [[-5.97453713]\n",
      " [ 0.05412199]\n",
      " [ 0.04734406]]\n",
      "the iterations of  138600 the loss is 0.35209525773703015 theta= [[-5.97727167]\n",
      " [ 0.05414292]\n",
      " [ 0.04736561]]\n",
      "the iterations of  138700 the loss is 0.35202055174657665 theta= [[-5.98000493]\n",
      " [ 0.05416385]\n",
      " [ 0.04738714]]\n",
      "the iterations of  138800 the loss is 0.35194591639929684 theta= [[-5.98273688]\n",
      " [ 0.05418477]\n",
      " [ 0.04740866]]\n",
      "the iterations of  138900 the loss is 0.3518713515958052 theta= [[-5.98546755]\n",
      " [ 0.05420568]\n",
      " [ 0.04743018]]\n",
      "the iterations of  139000 the loss is 0.3517968572368968 theta= [[-5.98819693]\n",
      " [ 0.05422658]\n",
      " [ 0.04745168]]\n",
      "the iterations of  139100 the loss is 0.35172243322354807 theta= [[-5.99092502]\n",
      " [ 0.05424748]\n",
      " [ 0.04747317]]\n",
      "the iterations of  139200 the loss is 0.3516480794569156 theta= [[-5.99365182]\n",
      " [ 0.05426836]\n",
      " [ 0.04749466]]\n",
      "the iterations of  139300 the loss is 0.3515737958383366 theta= [[-5.99637733]\n",
      " [ 0.05428923]\n",
      " [ 0.04751613]]\n",
      "the iterations of  139400 the loss is 0.35149958226932726 theta= [[-5.99910156]\n",
      " [ 0.0543101 ]\n",
      " [ 0.04753759]]\n",
      "the iterations of  139500 the loss is 0.35142543865158393 theta= [[-6.00182451]\n",
      " [ 0.05433095]\n",
      " [ 0.04755905]]\n",
      "the iterations of  139600 the loss is 0.35135136488698115 theta= [[-6.00454617]\n",
      " [ 0.0543518 ]\n",
      " [ 0.04758049]]\n",
      "the iterations of  139700 the loss is 0.3512773608775727 theta= [[-6.00726655]\n",
      " [ 0.05437263]\n",
      " [ 0.04760192]]\n",
      "the iterations of  139800 the loss is 0.3512034265255902 theta= [[-6.00998565]\n",
      " [ 0.05439346]\n",
      " [ 0.04762334]]\n",
      "the iterations of  139900 the loss is 0.35112956173344323 theta= [[-6.01270347]\n",
      " [ 0.05441428]\n",
      " [ 0.04764476]]\n",
      "the iterations of  140000 the loss is 0.35105576640371866 theta= [[-6.01542001]\n",
      " [ 0.05443509]\n",
      " [ 0.04766616]]\n",
      "the iterations of  140100 the loss is 0.3509820404391799 theta= [[-6.01813527]\n",
      " [ 0.05445589]\n",
      " [ 0.04768755]]\n",
      "the iterations of  140200 the loss is 0.3509083837427681 theta= [[-6.02084926]\n",
      " [ 0.05447668]\n",
      " [ 0.04770894]]\n",
      "the iterations of  140300 the loss is 0.35083479621759944 theta= [[-6.02356197]\n",
      " [ 0.05449746]\n",
      " [ 0.04773031]]\n",
      "the iterations of  140400 the loss is 0.35076127776696686 theta= [[-6.02627341]\n",
      " [ 0.05451823]\n",
      " [ 0.04775167]]\n",
      "the iterations of  140500 the loss is 0.35068782829433814 theta= [[-6.02898358]\n",
      " [ 0.054539  ]\n",
      " [ 0.04777303]]\n",
      "the iterations of  140600 the loss is 0.3506144477033568 theta= [[-6.03169248]\n",
      " [ 0.05455975]\n",
      " [ 0.04779437]]\n",
      "the iterations of  140700 the loss is 0.35054113589784075 theta= [[-6.03440011]\n",
      " [ 0.0545805 ]\n",
      " [ 0.0478157 ]]\n",
      "the iterations of  140800 the loss is 0.3504678927817817 theta= [[-6.03710646]\n",
      " [ 0.05460123]\n",
      " [ 0.04783703]]\n",
      "the iterations of  140900 the loss is 0.3503947182593462 theta= [[-6.03981155]\n",
      " [ 0.05462196]\n",
      " [ 0.04785834]]\n",
      "the iterations of  141000 the loss is 0.3503216122348739 theta= [[-6.04251538]\n",
      " [ 0.05464268]\n",
      " [ 0.04787964]]\n",
      "the iterations of  141100 the loss is 0.35024857461287795 theta= [[-6.04521794]\n",
      " [ 0.05466339]\n",
      " [ 0.04790094]]\n",
      "the iterations of  141200 the loss is 0.350175605298044 theta= [[-6.04791923]\n",
      " [ 0.05468409]\n",
      " [ 0.04792222]]\n",
      "the iterations of  141300 the loss is 0.3501027041952299 theta= [[-6.05061926]\n",
      " [ 0.05470478]\n",
      " [ 0.0479435 ]]\n",
      "the iterations of  141400 the loss is 0.3500298712094665 theta= [[-6.05331803]\n",
      " [ 0.05472546]\n",
      " [ 0.04796476]]\n",
      "the iterations of  141500 the loss is 0.34995710624595533 theta= [[-6.05601554]\n",
      " [ 0.05474613]\n",
      " [ 0.04798601]]\n",
      "the iterations of  141600 the loss is 0.3498844092100699 theta= [[-6.05871179]\n",
      " [ 0.05476679]\n",
      " [ 0.04800726]]\n",
      "the iterations of  141700 the loss is 0.34981178000735447 theta= [[-6.06140678]\n",
      " [ 0.05478745]\n",
      " [ 0.04802849]]\n",
      "the iterations of  141800 the loss is 0.3497392185435239 theta= [[-6.06410052]\n",
      " [ 0.05480809]\n",
      " [ 0.04804972]]\n",
      "the iterations of  141900 the loss is 0.3496667247244635 theta= [[-6.066793  ]\n",
      " [ 0.05482873]\n",
      " [ 0.04807093]]\n",
      "the iterations of  142000 the loss is 0.3495942984562279 theta= [[-6.06948422]\n",
      " [ 0.05484936]\n",
      " [ 0.04809214]]\n",
      "the iterations of  142100 the loss is 0.3495219396450415 theta= [[-6.07217419]\n",
      " [ 0.05486998]\n",
      " [ 0.04811333]]\n",
      "the iterations of  142200 the loss is 0.34944964819729796 theta= [[-6.07486291]\n",
      " [ 0.05489059]\n",
      " [ 0.04813452]]\n",
      "the iterations of  142300 the loss is 0.3493774240195595 theta= [[-6.07755038]\n",
      " [ 0.05491119]\n",
      " [ 0.0481557 ]]\n",
      "the iterations of  142400 the loss is 0.3493052670185569 theta= [[-6.08023659]\n",
      " [ 0.05493178]\n",
      " [ 0.04817686]]\n",
      "the iterations of  142500 the loss is 0.3492331771011887 theta= [[-6.08292156]\n",
      " [ 0.05495236]\n",
      " [ 0.04819802]]\n",
      "the iterations of  142600 the loss is 0.3491611541745212 theta= [[-6.08560528]\n",
      " [ 0.05497294]\n",
      " [ 0.04821917]]\n",
      "the iterations of  142700 the loss is 0.3490891981457881 theta= [[-6.08828775]\n",
      " [ 0.0549935 ]\n",
      " [ 0.0482403 ]]\n",
      "the iterations of  142800 the loss is 0.3490173089223898 theta= [[-6.09096898]\n",
      " [ 0.05501406]\n",
      " [ 0.04826143]]\n",
      "the iterations of  142900 the loss is 0.34894548641189366 theta= [[-6.09364896]\n",
      " [ 0.0550346 ]\n",
      " [ 0.04828255]]\n",
      "the iterations of  143000 the loss is 0.34887373052203274 theta= [[-6.0963277 ]\n",
      " [ 0.05505514]\n",
      " [ 0.04830366]]\n",
      "the iterations of  143100 the loss is 0.34880204116070634 theta= [[-6.0990052 ]\n",
      " [ 0.05507567]\n",
      " [ 0.04832475]]\n",
      "the iterations of  143200 the loss is 0.3487304182359792 theta= [[-6.10168146]\n",
      " [ 0.05509619]\n",
      " [ 0.04834584]]\n",
      "the iterations of  143300 the loss is 0.3486588616560811 theta= [[-6.10435647]\n",
      " [ 0.0551167 ]\n",
      " [ 0.04836692]]\n",
      "the iterations of  143400 the loss is 0.3485873713294065 theta= [[-6.10703025]\n",
      " [ 0.0551372 ]\n",
      " [ 0.04838799]]\n",
      "the iterations of  143500 the loss is 0.3485159471645144 theta= [[-6.10970279]\n",
      " [ 0.0551577 ]\n",
      " [ 0.04840905]]\n",
      "the iterations of  143600 the loss is 0.3484445890701281 theta= [[-6.11237409]\n",
      " [ 0.05517818]\n",
      " [ 0.0484301 ]]\n",
      "the iterations of  143700 the loss is 0.3483732969551342 theta= [[-6.11504416]\n",
      " [ 0.05519866]\n",
      " [ 0.04845114]]\n",
      "the iterations of  143800 the loss is 0.34830207072858294 theta= [[-6.117713  ]\n",
      " [ 0.05521912]\n",
      " [ 0.04847217]]\n",
      "the iterations of  143900 the loss is 0.34823091029968745 theta= [[-6.1203806 ]\n",
      " [ 0.05523958]\n",
      " [ 0.04849319]]\n",
      "the iterations of  144000 the loss is 0.3481598155778239 theta= [[-6.12304697]\n",
      " [ 0.05526003]\n",
      " [ 0.0485142 ]]\n",
      "the iterations of  144100 the loss is 0.3480887864725297 theta= [[-6.12571211]\n",
      " [ 0.05528047]\n",
      " [ 0.04853521]]\n",
      "the iterations of  144200 the loss is 0.3480178228935057 theta= [[-6.12837602]\n",
      " [ 0.0553009 ]\n",
      " [ 0.0485562 ]]\n",
      "the iterations of  144300 the loss is 0.34794692475061306 theta= [[-6.1310387 ]\n",
      " [ 0.05532132]\n",
      " [ 0.04857718]]\n",
      "the iterations of  144400 the loss is 0.34787609195387503 theta= [[-6.13370015]\n",
      " [ 0.05534174]\n",
      " [ 0.04859815]]\n",
      "the iterations of  144500 the loss is 0.3478053244134754 theta= [[-6.13636038]\n",
      " [ 0.05536214]\n",
      " [ 0.04861912]]\n",
      "the iterations of  144600 the loss is 0.3477346220397587 theta= [[-6.13901938]\n",
      " [ 0.05538254]\n",
      " [ 0.04864007]]\n",
      "the iterations of  144700 the loss is 0.34766398474322974 theta= [[-6.14167716]\n",
      " [ 0.05540293]\n",
      " [ 0.04866102]]\n",
      "the iterations of  144800 the loss is 0.34759341243455294 theta= [[-6.14433371]\n",
      " [ 0.0554233 ]\n",
      " [ 0.04868195]]\n",
      "the iterations of  144900 the loss is 0.3475229050245524 theta= [[-6.14698904]\n",
      " [ 0.05544367]\n",
      " [ 0.04870288]]\n",
      "the iterations of  145000 the loss is 0.34745246242421146 theta= [[-6.14964316]\n",
      " [ 0.05546403]\n",
      " [ 0.04872379]]\n",
      "the iterations of  145100 the loss is 0.34738208454467245 theta= [[-6.15229605]\n",
      " [ 0.05548439]\n",
      " [ 0.0487447 ]]\n",
      "the iterations of  145200 the loss is 0.3473117712972359 theta= [[-6.15494772]\n",
      " [ 0.05550473]\n",
      " [ 0.04876559]]\n",
      "the iterations of  145300 the loss is 0.3472415225933608 theta= [[-6.15759818]\n",
      " [ 0.05552506]\n",
      " [ 0.04878648]]\n",
      "the iterations of  145400 the loss is 0.34717133834466357 theta= [[-6.16024742]\n",
      " [ 0.05554539]\n",
      " [ 0.04880736]]\n",
      "the iterations of  145500 the loss is 0.3471012184629185 theta= [[-6.16289545]\n",
      " [ 0.05556571]\n",
      " [ 0.04882823]]\n",
      "the iterations of  145600 the loss is 0.34703116286005714 theta= [[-6.16554226]\n",
      " [ 0.05558602]\n",
      " [ 0.04884909]]\n",
      "the iterations of  145700 the loss is 0.3469611714481677 theta= [[-6.16818786]\n",
      " [ 0.05560632]\n",
      " [ 0.04886994]]\n",
      "the iterations of  145800 the loss is 0.3468912441394945 theta= [[-6.17083224]\n",
      " [ 0.05562661]\n",
      " [ 0.04889078]]\n",
      "the iterations of  145900 the loss is 0.3468213808464386 theta= [[-6.17347542]\n",
      " [ 0.05564689]\n",
      " [ 0.04891161]]\n",
      "the iterations of  146000 the loss is 0.34675158148155644 theta= [[-6.17611739]\n",
      " [ 0.05566716]\n",
      " [ 0.04893243]]\n",
      "the iterations of  146100 the loss is 0.34668184595756 theta= [[-6.17875814]\n",
      " [ 0.05568743]\n",
      " [ 0.04895324]]\n",
      "the iterations of  146200 the loss is 0.34661217418731705 theta= [[-6.18139769]\n",
      " [ 0.05570768]\n",
      " [ 0.04897404]]\n",
      "the iterations of  146300 the loss is 0.34654256608384926 theta= [[-6.18403604]\n",
      " [ 0.05572793]\n",
      " [ 0.04899483]]\n",
      "the iterations of  146400 the loss is 0.3464730215603335 theta= [[-6.18667318]\n",
      " [ 0.05574817]\n",
      " [ 0.04901562]]\n",
      "the iterations of  146500 the loss is 0.34640354053010014 theta= [[-6.18930911]\n",
      " [ 0.0557684 ]\n",
      " [ 0.04903639]]\n",
      "the iterations of  146600 the loss is 0.3463341229066341 theta= [[-6.19194384]\n",
      " [ 0.05578862]\n",
      " [ 0.04905716]]\n",
      "the iterations of  146700 the loss is 0.3462647686035731 theta= [[-6.19457737]\n",
      " [ 0.05580883]\n",
      " [ 0.04907791]]\n",
      "the iterations of  146800 the loss is 0.3461954775347084 theta= [[-6.1972097 ]\n",
      " [ 0.05582904]\n",
      " [ 0.04909866]]\n",
      "the iterations of  146900 the loss is 0.3461262496139842 theta= [[-6.19984083]\n",
      " [ 0.05584923]\n",
      " [ 0.04911939]]\n",
      "the iterations of  147000 the loss is 0.34605708475549724 theta= [[-6.20247076]\n",
      " [ 0.05586942]\n",
      " [ 0.04914012]]\n",
      "the iterations of  147100 the loss is 0.34598798287349586 theta= [[-6.20509949]\n",
      " [ 0.0558896 ]\n",
      " [ 0.04916084]]\n",
      "the iterations of  147200 the loss is 0.3459189438823811 theta= [[-6.20772703]\n",
      " [ 0.05590977]\n",
      " [ 0.04918155]]\n",
      "the iterations of  147300 the loss is 0.34584996769670484 theta= [[-6.21035337]\n",
      " [ 0.05592993]\n",
      " [ 0.04920225]]\n",
      "the iterations of  147400 the loss is 0.3457810542311707 theta= [[-6.21297852]\n",
      " [ 0.05595008]\n",
      " [ 0.04922294]]\n",
      "the iterations of  147500 the loss is 0.34571220340063297 theta= [[-6.21560247]\n",
      " [ 0.05597022]\n",
      " [ 0.04924362]]\n",
      "the iterations of  147600 the loss is 0.34564341512009655 theta= [[-6.21822523]\n",
      " [ 0.05599036]\n",
      " [ 0.04926429]]\n",
      "the iterations of  147700 the loss is 0.3455746893047167 theta= [[-6.2208468 ]\n",
      " [ 0.05601049]\n",
      " [ 0.04928495]]\n",
      "the iterations of  147800 the loss is 0.3455060258697983 theta= [[-6.22346718]\n",
      " [ 0.0560306 ]\n",
      " [ 0.0493056 ]]\n",
      "the iterations of  147900 the loss is 0.345437424730796 theta= [[-6.22608637]\n",
      " [ 0.05605071]\n",
      " [ 0.04932625]]\n",
      "the iterations of  148000 the loss is 0.34536888580331393 theta= [[-6.22870437]\n",
      " [ 0.05607081]\n",
      " [ 0.04934688]]\n",
      "the iterations of  148100 the loss is 0.34530040900310505 theta= [[-6.23132119]\n",
      " [ 0.05609091]\n",
      " [ 0.04936751]]\n",
      "the iterations of  148200 the loss is 0.345231994246071 theta= [[-6.23393682]\n",
      " [ 0.05611099]\n",
      " [ 0.04938812]]\n",
      "the iterations of  148300 the loss is 0.34516364144826184 theta= [[-6.23655126]\n",
      " [ 0.05613106]\n",
      " [ 0.04940873]]\n",
      "the iterations of  148400 the loss is 0.3450953505258756 theta= [[-6.23916453]\n",
      " [ 0.05615113]\n",
      " [ 0.04942932]]\n",
      "the iterations of  148500 the loss is 0.34502712139525793 theta= [[-6.24177661]\n",
      " [ 0.05617119]\n",
      " [ 0.04944991]]\n",
      "the iterations of  148600 the loss is 0.3449589539729018 theta= [[-6.2443875 ]\n",
      " [ 0.05619124]\n",
      " [ 0.04947049]]\n",
      "the iterations of  148700 the loss is 0.3448908481754481 theta= [[-6.24699722]\n",
      " [ 0.05621128]\n",
      " [ 0.04949106]]\n",
      "the iterations of  148800 the loss is 0.3448228039196831 theta= [[-6.24960576]\n",
      " [ 0.05623131]\n",
      " [ 0.04951162]]\n",
      "the iterations of  148900 the loss is 0.3447548211225409 theta= [[-6.25221312]\n",
      " [ 0.05625134]\n",
      " [ 0.04953217]]\n",
      "the iterations of  149000 the loss is 0.344686899701101 theta= [[-6.2548193 ]\n",
      " [ 0.05627135]\n",
      " [ 0.04955271]]\n",
      "the iterations of  149100 the loss is 0.3446190395725887 theta= [[-6.25742431]\n",
      " [ 0.05629136]\n",
      " [ 0.04957325]]\n",
      "the iterations of  149200 the loss is 0.34455124065437526 theta= [[-6.26002814]\n",
      " [ 0.05631136]\n",
      " [ 0.04959377]]\n",
      "the iterations of  149300 the loss is 0.3444835028639768 theta= [[-6.26263079]\n",
      " [ 0.05633135]\n",
      " [ 0.04961428]]\n",
      "the iterations of  149400 the loss is 0.3444158261190543 theta= [[-6.26523228]\n",
      " [ 0.05635133]\n",
      " [ 0.04963479]]\n",
      "the iterations of  149500 the loss is 0.3443482103374137 theta= [[-6.26783259]\n",
      " [ 0.0563713 ]\n",
      " [ 0.04965529]]\n",
      "the iterations of  149600 the loss is 0.34428065543700576 theta= [[-6.27043173]\n",
      " [ 0.05639127]\n",
      " [ 0.04967577]]\n",
      "the iterations of  149700 the loss is 0.3442131613359238 theta= [[-6.2730297 ]\n",
      " [ 0.05641122]\n",
      " [ 0.04969625]]\n",
      "the iterations of  149800 the loss is 0.34414572795240594 theta= [[-6.2756265 ]\n",
      " [ 0.05643117]\n",
      " [ 0.04971672]]\n",
      "the iterations of  149900 the loss is 0.3440783552048334 theta= [[-6.27822214]\n",
      " [ 0.05645111]\n",
      " [ 0.04973718]]\n",
      "the iterations of  150000 the loss is 0.3440110430117302 theta= [[-6.2808166 ]\n",
      " [ 0.05647104]\n",
      " [ 0.04975763]]\n",
      "the iterations of  150100 the loss is 0.343943791291764 theta= [[-6.2834099 ]\n",
      " [ 0.05649097]\n",
      " [ 0.04977807]]\n",
      "the iterations of  150200 the loss is 0.3438765999637441 theta= [[-6.28600204]\n",
      " [ 0.05651088]\n",
      " [ 0.0497985 ]]\n",
      "the iterations of  150300 the loss is 0.34380946894662223 theta= [[-6.28859301]\n",
      " [ 0.05653079]\n",
      " [ 0.04981893]]\n",
      "the iterations of  150400 the loss is 0.34374239815949187 theta= [[-6.29118282]\n",
      " [ 0.05655068]\n",
      " [ 0.04983934]]\n",
      "the iterations of  150500 the loss is 0.34367538752158866 theta= [[-6.29377147]\n",
      " [ 0.05657057]\n",
      " [ 0.04985975]]\n",
      "the iterations of  150600 the loss is 0.343608436952289 theta= [[-6.29635896]\n",
      " [ 0.05659045]\n",
      " [ 0.04988014]]\n",
      "the iterations of  150700 the loss is 0.34354154637111006 theta= [[-6.29894529]\n",
      " [ 0.05661033]\n",
      " [ 0.04990053]]\n",
      "the iterations of  150800 the loss is 0.34347471569771054 theta= [[-6.30153046]\n",
      " [ 0.05663019]\n",
      " [ 0.04992091]]\n",
      "the iterations of  150900 the loss is 0.3434079448518888 theta= [[-6.30411447]\n",
      " [ 0.05665005]\n",
      " [ 0.04994128]]\n",
      "the iterations of  151000 the loss is 0.34334123375358344 theta= [[-6.30669733]\n",
      " [ 0.05666989]\n",
      " [ 0.04996164]]\n",
      "the iterations of  151100 the loss is 0.34327458232287333 theta= [[-6.30927903]\n",
      " [ 0.05668973]\n",
      " [ 0.04998199]]\n",
      "the iterations of  151200 the loss is 0.3432079904799761 theta= [[-6.31185958]\n",
      " [ 0.05670956]\n",
      " [ 0.05000233]]\n",
      "the iterations of  151300 the loss is 0.3431414581452493 theta= [[-6.31443897]\n",
      " [ 0.05672938]\n",
      " [ 0.05002266]]\n",
      "the iterations of  151400 the loss is 0.3430749852391887 theta= [[-6.31701722]\n",
      " [ 0.0567492 ]\n",
      " [ 0.05004299]]\n",
      "the iterations of  151500 the loss is 0.34300857168242943 theta= [[-6.31959431]\n",
      " [ 0.056769  ]\n",
      " [ 0.0500633 ]]\n",
      "the iterations of  151600 the loss is 0.3429422173957443 theta= [[-6.32217025]\n",
      " [ 0.0567888 ]\n",
      " [ 0.05008361]]\n",
      "the iterations of  151700 the loss is 0.3428759223000448 theta= [[-6.32474504]\n",
      " [ 0.05680859]\n",
      " [ 0.0501039 ]]\n",
      "the iterations of  151800 the loss is 0.3428096863163798 theta= [[-6.32731868]\n",
      " [ 0.05682837]\n",
      " [ 0.05012419]]\n",
      "the iterations of  151900 the loss is 0.34274350936593545 theta= [[-6.32989117]\n",
      " [ 0.05684814]\n",
      " [ 0.05014447]]\n",
      "the iterations of  152000 the loss is 0.34267739137003567 theta= [[-6.33246252]\n",
      " [ 0.05686791]\n",
      " [ 0.05016474]]\n",
      "the iterations of  152100 the loss is 0.3426113322501406 theta= [[-6.33503273]\n",
      " [ 0.05688766]\n",
      " [ 0.050185  ]]\n",
      "the iterations of  152200 the loss is 0.3425453319278474 theta= [[-6.33760179]\n",
      " [ 0.05690741]\n",
      " [ 0.05020526]]\n",
      "the iterations of  152300 the loss is 0.34247939032488917 theta= [[-6.3401697 ]\n",
      " [ 0.05692715]\n",
      " [ 0.0502255 ]]\n",
      "the iterations of  152400 the loss is 0.3424135073631358 theta= [[-6.34273648]\n",
      " [ 0.05694688]\n",
      " [ 0.05024573]]\n",
      "the iterations of  152500 the loss is 0.342347682964592 theta= [[-6.34530211]\n",
      " [ 0.0569666 ]\n",
      " [ 0.05026596]]\n",
      "the iterations of  152600 the loss is 0.34228191705139877 theta= [[-6.34786661]\n",
      " [ 0.05698632]\n",
      " [ 0.05028618]]\n",
      "the iterations of  152700 the loss is 0.3422162095458316 theta= [[-6.35042996]\n",
      " [ 0.05700602]\n",
      " [ 0.05030638]]\n",
      "the iterations of  152800 the loss is 0.34215056037030117 theta= [[-6.35299218]\n",
      " [ 0.05702572]\n",
      " [ 0.05032658]]\n",
      "the iterations of  152900 the loss is 0.3420849694473529 theta= [[-6.35555326]\n",
      " [ 0.05704541]\n",
      " [ 0.05034677]]\n",
      "the iterations of  153000 the loss is 0.34201943669966633 theta= [[-6.3581132 ]\n",
      " [ 0.05706509]\n",
      " [ 0.05036696]]\n",
      "the iterations of  153100 the loss is 0.34195396205005496 theta= [[-6.36067201]\n",
      " [ 0.05708477]\n",
      " [ 0.05038713]]\n",
      "the iterations of  153200 the loss is 0.34188854542146624 theta= [[-6.36322968]\n",
      " [ 0.05710443]\n",
      " [ 0.05040729]]\n",
      "the iterations of  153300 the loss is 0.34182318673698114 theta= [[-6.36578622]\n",
      " [ 0.05712409]\n",
      " [ 0.05042745]]\n",
      "the iterations of  153400 the loss is 0.3417578859198136 theta= [[-6.36834163]\n",
      " [ 0.05714374]\n",
      " [ 0.05044759]]\n",
      "the iterations of  153500 the loss is 0.3416926428933109 theta= [[-6.37089591]\n",
      " [ 0.05716338]\n",
      " [ 0.05046773]]\n",
      "the iterations of  153600 the loss is 0.3416274575809522 theta= [[-6.37344906]\n",
      " [ 0.05718301]\n",
      " [ 0.05048786]]\n",
      "the iterations of  153700 the loss is 0.34156232990635027 theta= [[-6.37600108]\n",
      " [ 0.05720264]\n",
      " [ 0.05050798]]\n",
      "the iterations of  153800 the loss is 0.3414972597932486 theta= [[-6.37855197]\n",
      " [ 0.05722225]\n",
      " [ 0.05052809]]\n",
      "the iterations of  153900 the loss is 0.3414322471655231 theta= [[-6.38110173]\n",
      " [ 0.05724186]\n",
      " [ 0.05054819]]\n",
      "the iterations of  154000 the loss is 0.34136729194718174 theta= [[-6.38365037]\n",
      " [ 0.05726146]\n",
      " [ 0.05056828]]\n",
      "the iterations of  154100 the loss is 0.34130239406236307 theta= [[-6.38619788]\n",
      " [ 0.05728105]\n",
      " [ 0.05058837]]\n",
      "the iterations of  154200 the loss is 0.34123755343533657 theta= [[-6.38874427]\n",
      " [ 0.05730064]\n",
      " [ 0.05060844]]\n",
      "the iterations of  154300 the loss is 0.3411727699905029 theta= [[-6.39128954]\n",
      " [ 0.05732021]\n",
      " [ 0.05062851]]\n",
      "the iterations of  154400 the loss is 0.34110804365239306 theta= [[-6.39383368]\n",
      " [ 0.05733978]\n",
      " [ 0.05064857]]\n",
      "the iterations of  154500 the loss is 0.34104337434566795 theta= [[-6.3963767 ]\n",
      " [ 0.05735934]\n",
      " [ 0.05066862]]\n",
      "the iterations of  154600 the loss is 0.34097876199511856 theta= [[-6.39891861]\n",
      " [ 0.05737889]\n",
      " [ 0.05068866]]\n",
      "the iterations of  154700 the loss is 0.34091420652566545 theta= [[-6.40145939]\n",
      " [ 0.05739843]\n",
      " [ 0.05070869]]\n",
      "the iterations of  154800 the loss is 0.3408497078623587 theta= [[-6.40399906]\n",
      " [ 0.05741797]\n",
      " [ 0.05072871]]\n",
      "the iterations of  154900 the loss is 0.3407852659303775 theta= [[-6.4065376 ]\n",
      " [ 0.05743749]\n",
      " [ 0.05074873]]\n",
      "the iterations of  155000 the loss is 0.34072088065502953 theta= [[-6.40907503]\n",
      " [ 0.05745701]\n",
      " [ 0.05076873]]\n",
      "the iterations of  155100 the loss is 0.34065655196175115 theta= [[-6.41161135]\n",
      " [ 0.05747652]\n",
      " [ 0.05078873]]\n",
      "the iterations of  155200 the loss is 0.34059227977610734 theta= [[-6.41414655]\n",
      " [ 0.05749603]\n",
      " [ 0.05080872]]\n",
      "the iterations of  155300 the loss is 0.3405280640237906 theta= [[-6.41668064]\n",
      " [ 0.05751552]\n",
      " [ 0.0508287 ]]\n",
      "the iterations of  155400 the loss is 0.3404639046306216 theta= [[-6.41921362]\n",
      " [ 0.05753501]\n",
      " [ 0.05084867]]\n",
      "the iterations of  155500 the loss is 0.34039980152254856 theta= [[-6.42174548]\n",
      " [ 0.05755448]\n",
      " [ 0.05086863]]\n",
      "the iterations of  155600 the loss is 0.34033575462564614 theta= [[-6.42427624]\n",
      " [ 0.05757396]\n",
      " [ 0.05088859]]\n",
      "the iterations of  155700 the loss is 0.3402717638661168 theta= [[-6.42680588]\n",
      " [ 0.05759342]\n",
      " [ 0.05090853]]\n",
      "the iterations of  155800 the loss is 0.3402078291702893 theta= [[-6.42933442]\n",
      " [ 0.05761287]\n",
      " [ 0.05092847]]\n",
      "the iterations of  155900 the loss is 0.3401439504646191 theta= [[-6.43186185]\n",
      " [ 0.05763232]\n",
      " [ 0.0509484 ]]\n",
      "the iterations of  156000 the loss is 0.3400801276756875 theta= [[-6.43438817]\n",
      " [ 0.05765176]\n",
      " [ 0.05096832]]\n",
      "the iterations of  156100 the loss is 0.340016360730202 theta= [[-6.43691339]\n",
      " [ 0.05767119]\n",
      " [ 0.05098823]]\n",
      "the iterations of  156200 the loss is 0.3399526495549955 theta= [[-6.4394375 ]\n",
      " [ 0.05769061]\n",
      " [ 0.05100813]]\n",
      "the iterations of  156300 the loss is 0.33988899407702666 theta= [[-6.44196051]\n",
      " [ 0.05771002]\n",
      " [ 0.05102802]]\n",
      "the iterations of  156400 the loss is 0.3398253942233788 theta= [[-6.44448242]\n",
      " [ 0.05772943]\n",
      " [ 0.05104791]]\n",
      "the iterations of  156500 the loss is 0.33976184992126024 theta= [[-6.44700323]\n",
      " [ 0.05774883]\n",
      " [ 0.05106778]]\n",
      "the iterations of  156600 the loss is 0.33969836109800405 theta= [[-6.44952293]\n",
      " [ 0.05776822]\n",
      " [ 0.05108765]]\n",
      "the iterations of  156700 the loss is 0.3396349276810676 theta= [[-6.45204154]\n",
      " [ 0.0577876 ]\n",
      " [ 0.05110751]]\n",
      "the iterations of  156800 the loss is 0.33957154959803226 theta= [[-6.45455904]\n",
      " [ 0.05780697]\n",
      " [ 0.05112736]]\n",
      "the iterations of  156900 the loss is 0.3395082267766034 theta= [[-6.45707545]\n",
      " [ 0.05782634]\n",
      " [ 0.0511472 ]]\n",
      "the iterations of  157000 the loss is 0.3394449591446098 theta= [[-6.45959076]\n",
      " [ 0.0578457 ]\n",
      " [ 0.05116703]]\n",
      "the iterations of  157100 the loss is 0.3393817466300036 theta= [[-6.46210498]\n",
      " [ 0.05786505]\n",
      " [ 0.05118686]]\n",
      "the iterations of  157200 the loss is 0.3393185891608603 theta= [[-6.4646181 ]\n",
      " [ 0.05788439]\n",
      " [ 0.05120668]]\n",
      "the iterations of  157300 the loss is 0.33925548666537764 theta= [[-6.46713012]\n",
      " [ 0.05790373]\n",
      " [ 0.05122648]]\n",
      "the iterations of  157400 the loss is 0.3391924390718765 theta= [[-6.46964106]\n",
      " [ 0.05792305]\n",
      " [ 0.05124628]]\n",
      "the iterations of  157500 the loss is 0.33912944630879954 theta= [[-6.4721509 ]\n",
      " [ 0.05794237]\n",
      " [ 0.05126607]]\n",
      "the iterations of  157600 the loss is 0.3390665083047118 theta= [[-6.47465965]\n",
      " [ 0.05796168]\n",
      " [ 0.05128586]]\n",
      "the iterations of  157700 the loss is 0.33900362498830006 theta= [[-6.47716731]\n",
      " [ 0.05798099]\n",
      " [ 0.05130563]]\n",
      "the iterations of  157800 the loss is 0.33894079628837254 theta= [[-6.47967388]\n",
      " [ 0.05800028]\n",
      " [ 0.05132539]]\n",
      "the iterations of  157900 the loss is 0.33887802213385904 theta= [[-6.48217936]\n",
      " [ 0.05801957]\n",
      " [ 0.05134515]]\n",
      "the iterations of  158000 the loss is 0.3388153024538103 theta= [[-6.48468376]\n",
      " [ 0.05803885]\n",
      " [ 0.0513649 ]]\n",
      "the iterations of  158100 the loss is 0.3387526371773975 theta= [[-6.48718706]\n",
      " [ 0.05805812]\n",
      " [ 0.05138464]]\n",
      "the iterations of  158200 the loss is 0.33869002623391276 theta= [[-6.48968929]\n",
      " [ 0.05807738]\n",
      " [ 0.05140437]]\n",
      "the iterations of  158300 the loss is 0.33862746955276835 theta= [[-6.49219043]\n",
      " [ 0.05809664]\n",
      " [ 0.05142409]]\n",
      "the iterations of  158400 the loss is 0.3385649670634966 theta= [[-6.49469048]\n",
      " [ 0.05811589]\n",
      " [ 0.05144381]]\n",
      "the iterations of  158500 the loss is 0.3385025186957498 theta= [[-6.49718945]\n",
      " [ 0.05813513]\n",
      " [ 0.05146351]]\n",
      "the iterations of  158600 the loss is 0.3384401243792997 theta= [[-6.49968734]\n",
      " [ 0.05815436]\n",
      " [ 0.05148321]]\n",
      "the iterations of  158700 the loss is 0.3383777840440371 theta= [[-6.50218415]\n",
      " [ 0.05817358]\n",
      " [ 0.0515029 ]]\n",
      "the iterations of  158800 the loss is 0.3383154976199726 theta= [[-6.50467988]\n",
      " [ 0.0581928 ]\n",
      " [ 0.05152258]]\n",
      "the iterations of  158900 the loss is 0.3382532650372345 theta= [[-6.50717453]\n",
      " [ 0.05821201]\n",
      " [ 0.05154225]]\n",
      "the iterations of  159000 the loss is 0.3381910862260704 theta= [[-6.50966811]\n",
      " [ 0.05823121]\n",
      " [ 0.05156192]]\n",
      "the iterations of  159100 the loss is 0.3381289611168466 theta= [[-6.5121606 ]\n",
      " [ 0.0582504 ]\n",
      " [ 0.05158157]]\n",
      "the iterations of  159200 the loss is 0.33806688964004666 theta= [[-6.51465202]\n",
      " [ 0.05826959]\n",
      " [ 0.05160122]]\n",
      "the iterations of  159300 the loss is 0.3380048717262723 theta= [[-6.51714237]\n",
      " [ 0.05828876]\n",
      " [ 0.05162086]]\n",
      "the iterations of  159400 the loss is 0.337942907306243 theta= [[-6.51963164]\n",
      " [ 0.05830793]\n",
      " [ 0.05164049]]\n",
      "the iterations of  159500 the loss is 0.33788099631079577 theta= [[-6.52211983]\n",
      " [ 0.0583271 ]\n",
      " [ 0.05166011]]\n",
      "the iterations of  159600 the loss is 0.33781913867088403 theta= [[-6.52460696]\n",
      " [ 0.05834625]\n",
      " [ 0.05167973]]\n",
      "the iterations of  159700 the loss is 0.3377573343175786 theta= [[-6.52709301]\n",
      " [ 0.0583654 ]\n",
      " [ 0.05169933]]\n",
      "the iterations of  159800 the loss is 0.33769558318206694 theta= [[-6.52957799]\n",
      " [ 0.05838453]\n",
      " [ 0.05171893]]\n",
      "the iterations of  159900 the loss is 0.3376338851956525 theta= [[-6.53206191]\n",
      " [ 0.05840366]\n",
      " [ 0.05173852]]\n",
      "the iterations of  160000 the loss is 0.337572240289756 theta= [[-6.53454475]\n",
      " [ 0.05842279]\n",
      " [ 0.0517581 ]]\n",
      "the iterations of  160100 the loss is 0.3375106483959126 theta= [[-6.53702653]\n",
      " [ 0.0584419 ]\n",
      " [ 0.05177767]]\n",
      "the iterations of  160200 the loss is 0.33744910944577405 theta= [[-6.53950723]\n",
      " [ 0.05846101]\n",
      " [ 0.05179723]]\n",
      "the iterations of  160300 the loss is 0.3373876233711078 theta= [[-6.54198688]\n",
      " [ 0.05848011]\n",
      " [ 0.05181679]]\n",
      "the iterations of  160400 the loss is 0.3373261901037954 theta= [[-6.54446546]\n",
      " [ 0.0584992 ]\n",
      " [ 0.05183633]]\n",
      "the iterations of  160500 the loss is 0.33726480957583427 theta= [[-6.54694297]\n",
      " [ 0.05851829]\n",
      " [ 0.05185587]]\n",
      "the iterations of  160600 the loss is 0.3372034817193365 theta= [[-6.54941942]\n",
      " [ 0.05853736]\n",
      " [ 0.0518754 ]]\n",
      "the iterations of  160700 the loss is 0.33714220646652854 theta= [[-6.55189481]\n",
      " [ 0.05855643]\n",
      " [ 0.05189493]]\n",
      "the iterations of  160800 the loss is 0.3370809837497509 theta= [[-6.55436914]\n",
      " [ 0.05857549]\n",
      " [ 0.05191444]]\n",
      "the iterations of  160900 the loss is 0.33701981350145865 theta= [[-6.5568424 ]\n",
      " [ 0.05859454]\n",
      " [ 0.05193395]]\n",
      "the iterations of  161000 the loss is 0.33695869565422 theta= [[-6.55931461]\n",
      " [ 0.05861359]\n",
      " [ 0.05195344]]\n",
      "the iterations of  161100 the loss is 0.3368976301407174 theta= [[-6.56178576]\n",
      " [ 0.05863263]\n",
      " [ 0.05197293]]\n",
      "the iterations of  161200 the loss is 0.3368366168937464 theta= [[-6.56425585]\n",
      " [ 0.05865166]\n",
      " [ 0.05199241]]\n",
      "the iterations of  161300 the loss is 0.33677565584621516 theta= [[-6.56672488]\n",
      " [ 0.05867068]\n",
      " [ 0.05201189]]\n",
      "the iterations of  161400 the loss is 0.3367147469311454 theta= [[-6.56919286]\n",
      " [ 0.0586897 ]\n",
      " [ 0.05203135]]\n",
      "the iterations of  161500 the loss is 0.3366538900816715 theta= [[-6.57165978]\n",
      " [ 0.0587087 ]\n",
      " [ 0.05205081]]\n",
      "the iterations of  161600 the loss is 0.33659308523103965 theta= [[-6.57412565]\n",
      " [ 0.0587277 ]\n",
      " [ 0.05207025]]\n",
      "the iterations of  161700 the loss is 0.33653233231260893 theta= [[-6.57659046]\n",
      " [ 0.05874669]\n",
      " [ 0.05208969]]\n",
      "the iterations of  161800 the loss is 0.33647163125984947 theta= [[-6.57905422]\n",
      " [ 0.05876568]\n",
      " [ 0.05210913]]\n",
      "the iterations of  161900 the loss is 0.33641098200634434 theta= [[-6.58151694]\n",
      " [ 0.05878465]\n",
      " [ 0.05212855]]\n",
      "the iterations of  162000 the loss is 0.33635038448578713 theta= [[-6.5839786 ]\n",
      " [ 0.05880362]\n",
      " [ 0.05214796]]\n",
      "the iterations of  162100 the loss is 0.33628983863198303 theta= [[-6.58643921]\n",
      " [ 0.05882259]\n",
      " [ 0.05216737]]\n",
      "the iterations of  162200 the loss is 0.33622934437884844 theta= [[-6.58889877]\n",
      " [ 0.05884154]\n",
      " [ 0.05218677]]\n",
      "the iterations of  162300 the loss is 0.3361689016604107 theta= [[-6.59135728]\n",
      " [ 0.05886048]\n",
      " [ 0.05220616]]\n",
      "the iterations of  162400 the loss is 0.33610851041080686 theta= [[-6.59381475]\n",
      " [ 0.05887942]\n",
      " [ 0.05222554]]\n",
      "the iterations of  162500 the loss is 0.3360481705642853 theta= [[-6.59627117]\n",
      " [ 0.05889835]\n",
      " [ 0.05224492]]\n",
      "the iterations of  162600 the loss is 0.3359878820552043 theta= [[-6.59872655]\n",
      " [ 0.05891728]\n",
      " [ 0.05226428]]\n",
      "the iterations of  162700 the loss is 0.33592764481803217 theta= [[-6.60118088]\n",
      " [ 0.05893619]\n",
      " [ 0.05228364]]\n",
      "the iterations of  162800 the loss is 0.33586745878734625 theta= [[-6.60363417]\n",
      " [ 0.0589551 ]\n",
      " [ 0.05230299]]\n",
      "the iterations of  162900 the loss is 0.3358073238978347 theta= [[-6.60608641]\n",
      " [ 0.058974  ]\n",
      " [ 0.05232233]]\n",
      "the iterations of  163000 the loss is 0.33574724008429313 theta= [[-6.60853761]\n",
      " [ 0.05899289]\n",
      " [ 0.05234167]]\n",
      "the iterations of  163100 the loss is 0.3356872072816278 theta= [[-6.61098778]\n",
      " [ 0.05901178]\n",
      " [ 0.05236099]]\n",
      "the iterations of  163200 the loss is 0.33562722542485285 theta= [[-6.6134369 ]\n",
      " [ 0.05903066]\n",
      " [ 0.05238031]]\n",
      "the iterations of  163300 the loss is 0.3355672944490914 theta= [[-6.61588498]\n",
      " [ 0.05904953]\n",
      " [ 0.05239962]]\n",
      "the iterations of  163400 the loss is 0.3355074142895748 theta= [[-6.61833203]\n",
      " [ 0.05906839]\n",
      " [ 0.05241892]]\n",
      "the iterations of  163500 the loss is 0.3354475848816427 theta= [[-6.62077804]\n",
      " [ 0.05908724]\n",
      " [ 0.05243821]]\n",
      "the iterations of  163600 the loss is 0.3353878061607423 theta= [[-6.62322301]\n",
      " [ 0.05910609]\n",
      " [ 0.0524575 ]]\n",
      "the iterations of  163700 the loss is 0.33532807806242926 theta= [[-6.62566695]\n",
      " [ 0.05912493]\n",
      " [ 0.05247678]]\n",
      "the iterations of  163800 the loss is 0.33526840052236623 theta= [[-6.62810985]\n",
      " [ 0.05914376]\n",
      " [ 0.05249605]]\n",
      "the iterations of  163900 the loss is 0.3352087734763228 theta= [[-6.63055172]\n",
      " [ 0.05916259]\n",
      " [ 0.05251531]]\n",
      "the iterations of  164000 the loss is 0.3351491968601762 theta= [[-6.63299255]\n",
      " [ 0.05918141]\n",
      " [ 0.05253456]]\n",
      "the iterations of  164100 the loss is 0.33508967060991024 theta= [[-6.63543236]\n",
      " [ 0.05920022]\n",
      " [ 0.0525538 ]]\n",
      "the iterations of  164200 the loss is 0.3350301946616158 theta= [[-6.63787113]\n",
      " [ 0.05921902]\n",
      " [ 0.05257304]]\n",
      "the iterations of  164300 the loss is 0.3349707689514889 theta= [[-6.64030887]\n",
      " [ 0.05923781]\n",
      " [ 0.05259227]]\n",
      "the iterations of  164400 the loss is 0.3349113934158335 theta= [[-6.64274559]\n",
      " [ 0.0592566 ]\n",
      " [ 0.05261149]]\n",
      "the iterations of  164500 the loss is 0.33485206799105854 theta= [[-6.64518127]\n",
      " [ 0.05927538]\n",
      " [ 0.0526307 ]]\n",
      "the iterations of  164600 the loss is 0.33479279261367856 theta= [[-6.64761593]\n",
      " [ 0.05929415]\n",
      " [ 0.05264991]]\n",
      "the iterations of  164700 the loss is 0.33473356722031444 theta= [[-6.65004956]\n",
      " [ 0.05931292]\n",
      " [ 0.05266911]]\n",
      "the iterations of  164800 the loss is 0.3346743917476915 theta= [[-6.65248216]\n",
      " [ 0.05933168]\n",
      " [ 0.05268829]]\n",
      "the iterations of  164900 the loss is 0.33461526613264087 theta= [[-6.65491374]\n",
      " [ 0.05935043]\n",
      " [ 0.05270748]]\n",
      "the iterations of  165000 the loss is 0.3345561903120984 theta= [[-6.65734429]\n",
      " [ 0.05936917]\n",
      " [ 0.05272665]]\n",
      "the iterations of  165100 the loss is 0.3344971642231042 theta= [[-6.65977383]\n",
      " [ 0.0593879 ]\n",
      " [ 0.05274581]]\n",
      "the iterations of  165200 the loss is 0.33443818780280377 theta= [[-6.66220234]\n",
      " [ 0.05940663]\n",
      " [ 0.05276497]]\n",
      "the iterations of  165300 the loss is 0.3343792609884461 theta= [[-6.66462982]\n",
      " [ 0.05942535]\n",
      " [ 0.05278412]]\n",
      "the iterations of  165400 the loss is 0.33432038371738476 theta= [[-6.66705629]\n",
      " [ 0.05944406]\n",
      " [ 0.05280326]]\n",
      "the iterations of  165500 the loss is 0.33426155592707657 theta= [[-6.66948174]\n",
      " [ 0.05946277]\n",
      " [ 0.05282239]]\n",
      "the iterations of  165600 the loss is 0.3342027775550829 theta= [[-6.67190616]\n",
      " [ 0.05948147]\n",
      " [ 0.05284152]]\n",
      "the iterations of  165700 the loss is 0.3341440485390678 theta= [[-6.67432957]\n",
      " [ 0.05950016]\n",
      " [ 0.05286063]]\n",
      "the iterations of  165800 the loss is 0.3340853688167988 theta= [[-6.67675197]\n",
      " [ 0.05951884]\n",
      " [ 0.05287974]]\n",
      "the iterations of  165900 the loss is 0.3340267383261468 theta= [[-6.67917334]\n",
      " [ 0.05953752]\n",
      " [ 0.05289885]]\n",
      "the iterations of  166000 the loss is 0.33396815700508453 theta= [[-6.6815937 ]\n",
      " [ 0.05955618]\n",
      " [ 0.05291794]]\n",
      "the iterations of  166100 the loss is 0.3339096247916886 theta= [[-6.68401305]\n",
      " [ 0.05957485]\n",
      " [ 0.05293702]]\n",
      "the iterations of  166200 the loss is 0.3338511416241371 theta= [[-6.68643138]\n",
      " [ 0.0595935 ]\n",
      " [ 0.0529561 ]]\n",
      "the iterations of  166300 the loss is 0.3337927074407107 theta= [[-6.6888487 ]\n",
      " [ 0.05961214]\n",
      " [ 0.05297517]]\n",
      "the iterations of  166400 the loss is 0.33373432217979226 theta= [[-6.69126501]\n",
      " [ 0.05963078]\n",
      " [ 0.05299423]]\n",
      "the iterations of  166500 the loss is 0.33367598577986585 theta= [[-6.6936803 ]\n",
      " [ 0.05964941]\n",
      " [ 0.05301329]]\n",
      "the iterations of  166600 the loss is 0.3336176981795178 theta= [[-6.69609459]\n",
      " [ 0.05966804]\n",
      " [ 0.05303233]]\n",
      "the iterations of  166700 the loss is 0.33355945931743536 theta= [[-6.69850786]\n",
      " [ 0.05968666]\n",
      " [ 0.05305137]]\n",
      "the iterations of  166800 the loss is 0.33350126913240674 theta= [[-6.70092013]\n",
      " [ 0.05970526]\n",
      " [ 0.0530704 ]]\n",
      "the iterations of  166900 the loss is 0.3334431275633217 theta= [[-6.70333139]\n",
      " [ 0.05972387]\n",
      " [ 0.05308942]]\n",
      "the iterations of  167000 the loss is 0.3333850345491707 theta= [[-6.70574164]\n",
      " [ 0.05974246]\n",
      " [ 0.05310844]]\n",
      "the iterations of  167100 the loss is 0.3333269900290443 theta= [[-6.70815089]\n",
      " [ 0.05976105]\n",
      " [ 0.05312745]]\n",
      "the iterations of  167200 the loss is 0.3332689939421342 theta= [[-6.71055913]\n",
      " [ 0.05977963]\n",
      " [ 0.05314644]]\n",
      "the iterations of  167300 the loss is 0.3332110462277313 theta= [[-6.71296636]\n",
      " [ 0.0597982 ]\n",
      " [ 0.05316544]]\n",
      "the iterations of  167400 the loss is 0.3331531468252271 theta= [[-6.7153726 ]\n",
      " [ 0.05981677]\n",
      " [ 0.05318442]]\n",
      "the iterations of  167500 the loss is 0.3330952956741129 theta= [[-6.71777782]\n",
      " [ 0.05983532]\n",
      " [ 0.05320339]]\n",
      "the iterations of  167600 the loss is 0.33303749271397926 theta= [[-6.72018205]\n",
      " [ 0.05985387]\n",
      " [ 0.05322236]]\n",
      "the iterations of  167700 the loss is 0.33297973788451657 theta= [[-6.72258528]\n",
      " [ 0.05987242]\n",
      " [ 0.05324132]]\n",
      "the iterations of  167800 the loss is 0.33292203112551355 theta= [[-6.7249875 ]\n",
      " [ 0.05989095]\n",
      " [ 0.05326027]]\n",
      "the iterations of  167900 the loss is 0.3328643723768588 theta= [[-6.72738873]\n",
      " [ 0.05990948]\n",
      " [ 0.05327922]]\n",
      "the iterations of  168000 the loss is 0.3328067615785391 theta= [[-6.72978896]\n",
      " [ 0.059928  ]\n",
      " [ 0.05329816]]\n",
      "the iterations of  168100 the loss is 0.33274919867064023 theta= [[-6.73218818]\n",
      " [ 0.05994652]\n",
      " [ 0.05331708]]\n",
      "the iterations of  168200 the loss is 0.33269168359334594 theta= [[-6.73458642]\n",
      " [ 0.05996502]\n",
      " [ 0.053336  ]]\n",
      "the iterations of  168300 the loss is 0.3326342162869383 theta= [[-6.73698365]\n",
      " [ 0.05998352]\n",
      " [ 0.05335492]]\n",
      "the iterations of  168400 the loss is 0.33257679669179774 theta= [[-6.73937989]\n",
      " [ 0.06000202]\n",
      " [ 0.05337382]]\n",
      "the iterations of  168500 the loss is 0.33251942474840185 theta= [[-6.74177514]\n",
      " [ 0.0600205 ]\n",
      " [ 0.05339272]]\n",
      "the iterations of  168600 the loss is 0.33246210039732627 theta= [[-6.74416939]\n",
      " [ 0.06003898]\n",
      " [ 0.05341161]]\n",
      "the iterations of  168700 the loss is 0.3324048235792436 theta= [[-6.74656265]\n",
      " [ 0.06005745]\n",
      " [ 0.05343049]]\n",
      "the iterations of  168800 the loss is 0.3323475942349243 theta= [[-6.74895492]\n",
      " [ 0.06007591]\n",
      " [ 0.05344937]]\n",
      "the iterations of  168900 the loss is 0.3322904123052354 theta= [[-6.7513462 ]\n",
      " [ 0.06009437]\n",
      " [ 0.05346823]]\n",
      "the iterations of  169000 the loss is 0.33223327773114064 theta= [[-6.75373648]\n",
      " [ 0.06011282]\n",
      " [ 0.05348709]]\n",
      "the iterations of  169100 the loss is 0.3321761904537005 theta= [[-6.75612578]\n",
      " [ 0.06013126]\n",
      " [ 0.05350595]]\n",
      "the iterations of  169200 the loss is 0.33211915041407225 theta= [[-6.75851409]\n",
      " [ 0.06014969]\n",
      " [ 0.05352479]]\n",
      "the iterations of  169300 the loss is 0.3320621575535091 theta= [[-6.76090141]\n",
      " [ 0.06016812]\n",
      " [ 0.05354362]]\n",
      "the iterations of  169400 the loss is 0.33200521181335985 theta= [[-6.76328774]\n",
      " [ 0.06018654]\n",
      " [ 0.05356245]]\n",
      "the iterations of  169500 the loss is 0.3319483131350702 theta= [[-6.76567308]\n",
      " [ 0.06020495]\n",
      " [ 0.05358127]]\n",
      "the iterations of  169600 the loss is 0.3318914614601806 theta= [[-6.76805745]\n",
      " [ 0.06022336]\n",
      " [ 0.05360009]]\n",
      "the iterations of  169700 the loss is 0.3318346567303274 theta= [[-6.77044082]\n",
      " [ 0.06024176]\n",
      " [ 0.05361889]]\n",
      "the iterations of  169800 the loss is 0.33177789888724235 theta= [[-6.77282321]\n",
      " [ 0.06026015]\n",
      " [ 0.05363769]]\n",
      "the iterations of  169900 the loss is 0.3317211878727517 theta= [[-6.77520462]\n",
      " [ 0.06027853]\n",
      " [ 0.05365648]]\n",
      "the iterations of  170000 the loss is 0.3316645236287773 theta= [[-6.77758505]\n",
      " [ 0.06029691]\n",
      " [ 0.05367526]]\n",
      "the iterations of  170100 the loss is 0.3316079060973355 theta= [[-6.7799645 ]\n",
      " [ 0.06031528]\n",
      " [ 0.05369404]]\n",
      "the iterations of  170200 the loss is 0.3315513352205366 theta= [[-6.78234296]\n",
      " [ 0.06033364]\n",
      " [ 0.0537128 ]]\n",
      "the iterations of  170300 the loss is 0.3314948109405862 theta= [[-6.78472045]\n",
      " [ 0.060352  ]\n",
      " [ 0.05373156]]\n",
      "the iterations of  170400 the loss is 0.3314383331997836 theta= [[-6.78709695]\n",
      " [ 0.06037034]\n",
      " [ 0.05375031]]\n",
      "the iterations of  170500 the loss is 0.33138190194052186 theta= [[-6.78947248]\n",
      " [ 0.06038869]\n",
      " [ 0.05376906]]\n",
      "the iterations of  170600 the loss is 0.3313255171052882 theta= [[-6.79184703]\n",
      " [ 0.06040702]\n",
      " [ 0.0537878 ]]\n",
      "the iterations of  170700 the loss is 0.33126917863666344 theta= [[-6.7942206 ]\n",
      " [ 0.06042535]\n",
      " [ 0.05380652]]\n",
      "the iterations of  170800 the loss is 0.33121288647732106 theta= [[-6.7965932 ]\n",
      " [ 0.06044367]\n",
      " [ 0.05382525]]\n",
      "the iterations of  170900 the loss is 0.331156640570029 theta= [[-6.79896483]\n",
      " [ 0.06046198]\n",
      " [ 0.05384396]]\n",
      "the iterations of  171000 the loss is 0.33110044085764734 theta= [[-6.80133548]\n",
      " [ 0.06048028]\n",
      " [ 0.05386267]]\n",
      "the iterations of  171100 the loss is 0.3310442872831289 theta= [[-6.80370515]\n",
      " [ 0.06049858]\n",
      " [ 0.05388136]]\n",
      "the iterations of  171200 the loss is 0.3309881797895201 theta= [[-6.80607385]\n",
      " [ 0.06051687]\n",
      " [ 0.05390005]]\n",
      "the iterations of  171300 the loss is 0.330932118319959 theta= [[-6.80844159]\n",
      " [ 0.06053516]\n",
      " [ 0.05391874]]\n",
      "the iterations of  171400 the loss is 0.3308761028176764 theta= [[-6.81080835]\n",
      " [ 0.06055343]\n",
      " [ 0.05393741]]\n",
      "the iterations of  171500 the loss is 0.33082013322599507 theta= [[-6.81317414]\n",
      " [ 0.0605717 ]\n",
      " [ 0.05395608]]\n",
      "the iterations of  171600 the loss is 0.3307642094883297 theta= [[-6.81553896]\n",
      " [ 0.06058997]\n",
      " [ 0.05397474]]\n",
      "the iterations of  171700 the loss is 0.3307083315481866 theta= [[-6.81790281]\n",
      " [ 0.06060822]\n",
      " [ 0.0539934 ]]\n",
      "the iterations of  171800 the loss is 0.330652499349164 theta= [[-6.8202657 ]\n",
      " [ 0.06062647]\n",
      " [ 0.05401204]]\n",
      "the iterations of  171900 the loss is 0.33059671283495135 theta= [[-6.82262762]\n",
      " [ 0.06064471]\n",
      " [ 0.05403068]]\n",
      "the iterations of  172000 the loss is 0.33054097194932935 theta= [[-6.82498857]\n",
      " [ 0.06066295]\n",
      " [ 0.05404931]]\n",
      "the iterations of  172100 the loss is 0.33048527663616967 theta= [[-6.82734855]\n",
      " [ 0.06068117]\n",
      " [ 0.05406793]]\n",
      "the iterations of  172200 the loss is 0.330429626839435 theta= [[-6.82970758]\n",
      " [ 0.0606994 ]\n",
      " [ 0.05408655]]\n",
      "the iterations of  172300 the loss is 0.33037402250317854 theta= [[-6.83206563]\n",
      " [ 0.06071761]\n",
      " [ 0.05410516]]\n",
      "the iterations of  172400 the loss is 0.33031846357154393 theta= [[-6.83442273]\n",
      " [ 0.06073581]\n",
      " [ 0.05412376]]\n",
      "the iterations of  172500 the loss is 0.3302629499887652 theta= [[-6.83677886]\n",
      " [ 0.06075401]\n",
      " [ 0.05414235]]\n",
      "the iterations of  172600 the loss is 0.3302074816991669 theta= [[-6.83913403]\n",
      " [ 0.06077221]\n",
      " [ 0.05416094]]\n",
      "the iterations of  172700 the loss is 0.33015205864716285 theta= [[-6.84148825]\n",
      " [ 0.06079039]\n",
      " [ 0.05417951]]\n",
      "the iterations of  172800 the loss is 0.33009668077725707 theta= [[-6.8438415 ]\n",
      " [ 0.06080857]\n",
      " [ 0.05419808]]\n",
      "the iterations of  172900 the loss is 0.33004134803404317 theta= [[-6.84619379]\n",
      " [ 0.06082674]\n",
      " [ 0.05421665]]\n",
      "the iterations of  173000 the loss is 0.32998606036220424 theta= [[-6.84854512]\n",
      " [ 0.0608449 ]\n",
      " [ 0.0542352 ]]\n",
      "the iterations of  173100 the loss is 0.3299308177065125 theta= [[-6.8508955 ]\n",
      " [ 0.06086306]\n",
      " [ 0.05425375]]\n",
      "the iterations of  173200 the loss is 0.32987562001182946 theta= [[-6.85324492]\n",
      " [ 0.06088121]\n",
      " [ 0.05427229]]\n",
      "the iterations of  173300 the loss is 0.3298204672231051 theta= [[-6.85559338]\n",
      " [ 0.06089936]\n",
      " [ 0.05429083]]\n",
      "the iterations of  173400 the loss is 0.3297653592853784 theta= [[-6.85794089]\n",
      " [ 0.06091749]\n",
      " [ 0.05430935]]\n",
      "the iterations of  173500 the loss is 0.3297102961437773 theta= [[-6.86028745]\n",
      " [ 0.06093562]\n",
      " [ 0.05432787]]\n",
      "the iterations of  173600 the loss is 0.3296552777435175 theta= [[-6.86263305]\n",
      " [ 0.06095374]\n",
      " [ 0.05434638]]\n",
      "the iterations of  173700 the loss is 0.329600304029903 theta= [[-6.8649777 ]\n",
      " [ 0.06097186]\n",
      " [ 0.05436488]]\n",
      "the iterations of  173800 the loss is 0.3295453749483262 theta= [[-6.86732139]\n",
      " [ 0.06098997]\n",
      " [ 0.05438338]]\n",
      "the iterations of  173900 the loss is 0.3294904904442669 theta= [[-6.86966414]\n",
      " [ 0.06100807]\n",
      " [ 0.05440187]]\n",
      "the iterations of  174000 the loss is 0.32943565046329326 theta= [[-6.87200593]\n",
      " [ 0.06102616]\n",
      " [ 0.05442035]]\n",
      "the iterations of  174100 the loss is 0.32938085495106023 theta= [[-6.87434678]\n",
      " [ 0.06104425]\n",
      " [ 0.05443883]]\n",
      "the iterations of  174200 the loss is 0.32932610385331046 theta= [[-6.87668667]\n",
      " [ 0.06106233]\n",
      " [ 0.05445729]]\n",
      "the iterations of  174300 the loss is 0.3292713971158738 theta= [[-6.87902562]\n",
      " [ 0.0610804 ]\n",
      " [ 0.05447575]]\n",
      "the iterations of  174400 the loss is 0.32921673468466695 theta= [[-6.88136362]\n",
      " [ 0.06109847]\n",
      " [ 0.0544942 ]]\n",
      "the iterations of  174500 the loss is 0.3291621165056935 theta= [[-6.88370067]\n",
      " [ 0.06111653]\n",
      " [ 0.05451265]]\n",
      "the iterations of  174600 the loss is 0.32910754252504376 theta= [[-6.88603678]\n",
      " [ 0.06113458]\n",
      " [ 0.05453108]]\n",
      "the iterations of  174700 the loss is 0.3290530126888948 theta= [[-6.88837194]\n",
      " [ 0.06115263]\n",
      " [ 0.05454951]]\n",
      "the iterations of  174800 the loss is 0.32899852694350945 theta= [[-6.89070616]\n",
      " [ 0.06117067]\n",
      " [ 0.05456794]]\n",
      "the iterations of  174900 the loss is 0.3289440852352371 theta= [[-6.89303944]\n",
      " [ 0.0611887 ]\n",
      " [ 0.05458635]]\n",
      "the iterations of  175000 the loss is 0.32888968751051295 theta= [[-6.89537177]\n",
      " [ 0.06120672]\n",
      " [ 0.05460476]]\n",
      "the iterations of  175100 the loss is 0.3288353337158582 theta= [[-6.89770316]\n",
      " [ 0.06122474]\n",
      " [ 0.05462316]]\n",
      "the iterations of  175200 the loss is 0.3287810237978795 theta= [[-6.90003361]\n",
      " [ 0.06124275]\n",
      " [ 0.05464155]]\n",
      "the iterations of  175300 the loss is 0.3287267577032689 theta= [[-6.90236312]\n",
      " [ 0.06126076]\n",
      " [ 0.05465994]]\n",
      "the iterations of  175400 the loss is 0.32867253537880436 theta= [[-6.90469169]\n",
      " [ 0.06127875]\n",
      " [ 0.05467832]]\n",
      "the iterations of  175500 the loss is 0.3286183567713482 theta= [[-6.90701932]\n",
      " [ 0.06129674]\n",
      " [ 0.05469669]]\n",
      "the iterations of  175600 the loss is 0.3285642218278483 theta= [[-6.90934601]\n",
      " [ 0.06131473]\n",
      " [ 0.05471505]]\n",
      "the iterations of  175700 the loss is 0.3285101304953369 theta= [[-6.91167176]\n",
      " [ 0.06133271]\n",
      " [ 0.05473341]]\n",
      "the iterations of  175800 the loss is 0.3284560827209316 theta= [[-6.91399658]\n",
      " [ 0.06135068]\n",
      " [ 0.05475176]]\n",
      "the iterations of  175900 the loss is 0.32840207845183356 theta= [[-6.91632046]\n",
      " [ 0.06136864]\n",
      " [ 0.0547701 ]]\n",
      "the iterations of  176000 the loss is 0.32834811763532906 theta= [[-6.91864341]\n",
      " [ 0.06138659]\n",
      " [ 0.05478843]]\n",
      "the iterations of  176100 the loss is 0.3282942002187882 theta= [[-6.92096542]\n",
      " [ 0.06140454]\n",
      " [ 0.05480676]]\n",
      "the iterations of  176200 the loss is 0.328240326149665 theta= [[-6.9232865 ]\n",
      " [ 0.06142249]\n",
      " [ 0.05482508]]\n",
      "the iterations of  176300 the loss is 0.3281864953754973 theta= [[-6.92560665]\n",
      " [ 0.06144042]\n",
      " [ 0.05484339]]\n",
      "the iterations of  176400 the loss is 0.3281327078439071 theta= [[-6.92792586]\n",
      " [ 0.06145835]\n",
      " [ 0.0548617 ]]\n",
      "the iterations of  176500 the loss is 0.3280789635025992 theta= [[-6.93024415]\n",
      " [ 0.06147627]\n",
      " [ 0.05488   ]]\n",
      "the iterations of  176600 the loss is 0.3280252622993621 theta= [[-6.9325615 ]\n",
      " [ 0.06149419]\n",
      " [ 0.05489829]]\n",
      "the iterations of  176700 the loss is 0.32797160418206756 theta= [[-6.93487792]\n",
      " [ 0.0615121 ]\n",
      " [ 0.05491657]]\n",
      "the iterations of  176800 the loss is 0.3279179890986702 theta= [[-6.93719341]\n",
      " [ 0.06153   ]\n",
      " [ 0.05493485]]\n",
      "the iterations of  176900 the loss is 0.3278644169972076 theta= [[-6.93950798]\n",
      " [ 0.06154789]\n",
      " [ 0.05495312]]\n",
      "the iterations of  177000 the loss is 0.3278108878257999 theta= [[-6.94182161]\n",
      " [ 0.06156578]\n",
      " [ 0.05497138]]\n",
      "the iterations of  177100 the loss is 0.32775740153265 theta= [[-6.94413432]\n",
      " [ 0.06158366]\n",
      " [ 0.05498963]]\n",
      "the iterations of  177200 the loss is 0.3277039580660429 theta= [[-6.94644611]\n",
      " [ 0.06160154]\n",
      " [ 0.05500788]]\n",
      "the iterations of  177300 the loss is 0.32765055737434595 theta= [[-6.94875697]\n",
      " [ 0.0616194 ]\n",
      " [ 0.05502612]]\n",
      "the iterations of  177400 the loss is 0.3275971994060085 theta= [[-6.9510669 ]\n",
      " [ 0.06163727]\n",
      " [ 0.05504436]]\n",
      "the iterations of  177500 the loss is 0.32754388410956214 theta= [[-6.95337591]\n",
      " [ 0.06165512]\n",
      " [ 0.05506258]]\n",
      "the iterations of  177600 the loss is 0.32749061143361985 theta= [[-6.955684  ]\n",
      " [ 0.06167297]\n",
      " [ 0.0550808 ]]\n",
      "the iterations of  177700 the loss is 0.3274373813268759 theta= [[-6.95799116]\n",
      " [ 0.06169081]\n",
      " [ 0.05509901]]\n",
      "the iterations of  177800 the loss is 0.3273841937381068 theta= [[-6.96029741]\n",
      " [ 0.06170864]\n",
      " [ 0.05511722]]\n",
      "the iterations of  177900 the loss is 0.3273310486161696 theta= [[-6.96260273]\n",
      " [ 0.06172647]\n",
      " [ 0.05513541]]\n",
      "the iterations of  178000 the loss is 0.3272779459100027 theta= [[-6.96490713]\n",
      " [ 0.06174429]\n",
      " [ 0.0551536 ]]\n",
      "the iterations of  178100 the loss is 0.3272248855686255 theta= [[-6.96721061]\n",
      " [ 0.0617621 ]\n",
      " [ 0.05517179]]\n",
      "the iterations of  178200 the loss is 0.3271718675411382 theta= [[-6.96951318]\n",
      " [ 0.06177991]\n",
      " [ 0.05518996]]\n",
      "the iterations of  178300 the loss is 0.32711889177672154 theta= [[-6.97181482]\n",
      " [ 0.06179771]\n",
      " [ 0.05520813]]\n",
      "the iterations of  178400 the loss is 0.3270659582246369 theta= [[-6.97411555]\n",
      " [ 0.0618155 ]\n",
      " [ 0.05522629]]\n",
      "the iterations of  178500 the loss is 0.3270130668342255 theta= [[-6.97641537]\n",
      " [ 0.06183329]\n",
      " [ 0.05524445]]\n",
      "the iterations of  178600 the loss is 0.32696021755490956 theta= [[-6.97871426]\n",
      " [ 0.06185107]\n",
      " [ 0.05526259]]\n",
      "the iterations of  178700 the loss is 0.32690741033619053 theta= [[-6.98101224]\n",
      " [ 0.06186884]\n",
      " [ 0.05528074]]\n",
      "the iterations of  178800 the loss is 0.3268546451276502 theta= [[-6.98330931]\n",
      " [ 0.06188661]\n",
      " [ 0.05529887]]\n",
      "the iterations of  178900 the loss is 0.3268019218789501 theta= [[-6.98560546]\n",
      " [ 0.06190437]\n",
      " [ 0.05531699]]\n",
      "the iterations of  179000 the loss is 0.3267492405398307 theta= [[-6.98790071]\n",
      " [ 0.06192212]\n",
      " [ 0.05533511]]\n",
      "the iterations of  179100 the loss is 0.32669660106011233 theta= [[-6.99019503]\n",
      " [ 0.06193987]\n",
      " [ 0.05535322]]\n",
      "the iterations of  179200 the loss is 0.326644003389695 theta= [[-6.99248845]\n",
      " [ 0.06195761]\n",
      " [ 0.05537133]]\n",
      "the iterations of  179300 the loss is 0.326591447478557 theta= [[-6.99478096]\n",
      " [ 0.06197534]\n",
      " [ 0.05538943]]\n",
      "the iterations of  179400 the loss is 0.32653893327675576 theta= [[-6.99707256]\n",
      " [ 0.06199307]\n",
      " [ 0.05540752]]\n",
      "the iterations of  179500 the loss is 0.3264864607344277 theta= [[-6.99936324]\n",
      " [ 0.06201079]\n",
      " [ 0.0554256 ]]\n",
      "the iterations of  179600 the loss is 0.3264340298017877 theta= [[-7.00165302]\n",
      " [ 0.0620285 ]\n",
      " [ 0.05544368]]\n",
      "the iterations of  179700 the loss is 0.3263816404291295 theta= [[-7.0039419 ]\n",
      " [ 0.06204621]\n",
      " [ 0.05546175]]\n",
      "the iterations of  179800 the loss is 0.3263292925668246 theta= [[-7.00622986]\n",
      " [ 0.0620639 ]\n",
      " [ 0.05547981]]\n",
      "the iterations of  179900 the loss is 0.3262769861653231 theta= [[-7.00851692]\n",
      " [ 0.0620816 ]\n",
      " [ 0.05549786]]\n",
      "the iterations of  180000 the loss is 0.326224721175153 theta= [[-7.01080307]\n",
      " [ 0.06209928]\n",
      " [ 0.05551591]]\n",
      "the iterations of  180100 the loss is 0.3261724975469201 theta= [[-7.01308832]\n",
      " [ 0.06211696]\n",
      " [ 0.05553395]]\n",
      "the iterations of  180200 the loss is 0.326120315231308 theta= [[-7.01537266]\n",
      " [ 0.06213464]\n",
      " [ 0.05555199]]\n",
      "the iterations of  180300 the loss is 0.32606817417907813 theta= [[-7.0176561 ]\n",
      " [ 0.0621523 ]\n",
      " [ 0.05557001]]\n",
      "the iterations of  180400 the loss is 0.32601607434106883 theta= [[-7.01993864]\n",
      " [ 0.06216996]\n",
      " [ 0.05558803]]\n",
      "the iterations of  180500 the loss is 0.325964015668196 theta= [[-7.02222028]\n",
      " [ 0.06218762]\n",
      " [ 0.05560605]]\n",
      "the iterations of  180600 the loss is 0.3259119981114528 theta= [[-7.02450101]\n",
      " [ 0.06220526]\n",
      " [ 0.05562405]]\n",
      "the iterations of  180700 the loss is 0.325860021621909 theta= [[-7.02678085]\n",
      " [ 0.0622229 ]\n",
      " [ 0.05564205]]\n",
      "the iterations of  180800 the loss is 0.3258080861507116 theta= [[-7.02905978]\n",
      " [ 0.06224054]\n",
      " [ 0.05566004]]\n",
      "the iterations of  180900 the loss is 0.32575619164908437 theta= [[-7.03133782]\n",
      " [ 0.06225816]\n",
      " [ 0.05567803]]\n",
      "the iterations of  181000 the loss is 0.32570433806832694 theta= [[-7.03361495]\n",
      " [ 0.06227578]\n",
      " [ 0.05569601]]\n",
      "the iterations of  181100 the loss is 0.3256525253598159 theta= [[-7.03589119]\n",
      " [ 0.0622934 ]\n",
      " [ 0.05571398]]\n",
      "the iterations of  181200 the loss is 0.3256007534750042 theta= [[-7.03816654]\n",
      " [ 0.062311  ]\n",
      " [ 0.05573194]]\n",
      "the iterations of  181300 the loss is 0.32554902236542044 theta= [[-7.04044098]\n",
      " [ 0.0623286 ]\n",
      " [ 0.0557499 ]]\n",
      "the iterations of  181400 the loss is 0.32549733198266945 theta= [[-7.04271453]\n",
      " [ 0.0623462 ]\n",
      " [ 0.05576785]]\n",
      "the iterations of  181500 the loss is 0.32544568227843185 theta= [[-7.04498719]\n",
      " [ 0.06236378]\n",
      " [ 0.05578579]]\n",
      "the iterations of  181600 the loss is 0.325394073204464 theta= [[-7.04725895]\n",
      " [ 0.06238136]\n",
      " [ 0.05580373]]\n",
      "the iterations of  181700 the loss is 0.3253425047125974 theta= [[-7.04952982]\n",
      " [ 0.06239894]\n",
      " [ 0.05582165]]\n",
      "the iterations of  181800 the loss is 0.3252909767547395 theta= [[-7.0517998 ]\n",
      " [ 0.06241651]\n",
      " [ 0.05583958]]\n",
      "the iterations of  181900 the loss is 0.32523948928287255 theta= [[-7.05406888]\n",
      " [ 0.06243407]\n",
      " [ 0.05585749]]\n",
      "the iterations of  182000 the loss is 0.3251880422490539 theta= [[-7.05633707]\n",
      " [ 0.06245162]\n",
      " [ 0.0558754 ]]\n",
      "the iterations of  182100 the loss is 0.3251366356054162 theta= [[-7.05860437]\n",
      " [ 0.06246917]\n",
      " [ 0.0558933 ]]\n",
      "the iterations of  182200 the loss is 0.3250852693041665 theta= [[-7.06087079]\n",
      " [ 0.06248671]\n",
      " [ 0.0559112 ]]\n",
      "the iterations of  182300 the loss is 0.3250339432975868 theta= [[-7.06313631]\n",
      " [ 0.06250425]\n",
      " [ 0.05592908]]\n",
      "the iterations of  182400 the loss is 0.3249826575380332 theta= [[-7.06540094]\n",
      " [ 0.06252177]\n",
      " [ 0.05594696]]\n",
      "the iterations of  182500 the loss is 0.32493141197793685 theta= [[-7.06766469]\n",
      " [ 0.0625393 ]\n",
      " [ 0.05596484]]\n",
      "the iterations of  182600 the loss is 0.32488020656980254 theta= [[-7.06992755]\n",
      " [ 0.06255681]\n",
      " [ 0.0559827 ]]\n",
      "the iterations of  182700 the loss is 0.3248290412662093 theta= [[-7.07218952]\n",
      " [ 0.06257432]\n",
      " [ 0.05600056]]\n",
      "the iterations of  182800 the loss is 0.3247779160198101 theta= [[-7.07445061]\n",
      " [ 0.06259182]\n",
      " [ 0.05601842]]\n",
      "the iterations of  182900 the loss is 0.3247268307833322 theta= [[-7.07671081]\n",
      " [ 0.06260932]\n",
      " [ 0.05603626]]\n",
      "the iterations of  183000 the loss is 0.3246757855095755 theta= [[-7.07897013]\n",
      " [ 0.06262681]\n",
      " [ 0.0560541 ]]\n",
      "the iterations of  183100 the loss is 0.32462478015141427 theta= [[-7.08122857]\n",
      " [ 0.06264429]\n",
      " [ 0.05607193]]\n",
      "the iterations of  183200 the loss is 0.32457381466179613 theta= [[-7.08348612]\n",
      " [ 0.06266176]\n",
      " [ 0.05608976]]\n",
      "the iterations of  183300 the loss is 0.3245228889937413 theta= [[-7.08574279]\n",
      " [ 0.06267923]\n",
      " [ 0.05610758]]\n",
      "the iterations of  183400 the loss is 0.32447200310034396 theta= [[-7.08799858]\n",
      " [ 0.0626967 ]\n",
      " [ 0.05612539]]\n",
      "the iterations of  183500 the loss is 0.32442115693477036 theta= [[-7.09025349]\n",
      " [ 0.06271415]\n",
      " [ 0.05614319]]\n",
      "the iterations of  183600 the loss is 0.3243703504502606 theta= [[-7.09250751]\n",
      " [ 0.0627316 ]\n",
      " [ 0.05616099]]\n",
      "the iterations of  183700 the loss is 0.32431958360012664 theta= [[-7.09476066]\n",
      " [ 0.06274905]\n",
      " [ 0.05617878]]\n",
      "the iterations of  183800 the loss is 0.32426885633775315 theta= [[-7.09701293]\n",
      " [ 0.06276649]\n",
      " [ 0.05619657]]\n",
      "the iterations of  183900 the loss is 0.32421816861659747 theta= [[-7.09926432]\n",
      " [ 0.06278392]\n",
      " [ 0.05621434]]\n",
      "the iterations of  184000 the loss is 0.32416752039018926 theta= [[-7.10151484]\n",
      " [ 0.06280134]\n",
      " [ 0.05623211]]\n",
      "the iterations of  184100 the loss is 0.3241169116121298 theta= [[-7.10376448]\n",
      " [ 0.06281876]\n",
      " [ 0.05624988]]\n",
      "the iterations of  184200 the loss is 0.3240663422360928 theta= [[-7.10601324]\n",
      " [ 0.06283617]\n",
      " [ 0.05626764]]\n",
      "the iterations of  184300 the loss is 0.32401581221582376 theta= [[-7.10826112]\n",
      " [ 0.06285358]\n",
      " [ 0.05628539]]\n",
      "the iterations of  184400 the loss is 0.32396532150513996 theta= [[-7.11050814]\n",
      " [ 0.06287097]\n",
      " [ 0.05630313]]\n",
      "the iterations of  184500 the loss is 0.3239148700579301 theta= [[-7.11275427]\n",
      " [ 0.06288837]\n",
      " [ 0.05632087]]\n",
      "the iterations of  184600 the loss is 0.3238644578281546 theta= [[-7.11499954]\n",
      " [ 0.06290575]\n",
      " [ 0.05633859]]\n",
      "the iterations of  184700 the loss is 0.32381408476984447 theta= [[-7.11724393]\n",
      " [ 0.06292313]\n",
      " [ 0.05635632]]\n",
      "the iterations of  184800 the loss is 0.3237637508371031 theta= [[-7.11948745]\n",
      " [ 0.0629405 ]\n",
      " [ 0.05637403]]\n",
      "the iterations of  184900 the loss is 0.32371345598410406 theta= [[-7.1217301 ]\n",
      " [ 0.06295787]\n",
      " [ 0.05639174]]\n",
      "the iterations of  185000 the loss is 0.32366320016509215 theta= [[-7.12397188]\n",
      " [ 0.06297523]\n",
      " [ 0.05640945]]\n",
      "the iterations of  185100 the loss is 0.32361298333438254 theta= [[-7.12621279]\n",
      " [ 0.06299258]\n",
      " [ 0.05642714]]\n",
      "the iterations of  185200 the loss is 0.3235628054463616 theta= [[-7.12845283]\n",
      " [ 0.06300993]\n",
      " [ 0.05644483]]\n",
      "the iterations of  185300 the loss is 0.32351266645548615 theta= [[-7.130692  ]\n",
      " [ 0.06302727]\n",
      " [ 0.05646251]]\n",
      "the iterations of  185400 the loss is 0.32346256631628295 theta= [[-7.13293031]\n",
      " [ 0.06304461]\n",
      " [ 0.05648019]]\n",
      "the iterations of  185500 the loss is 0.3234125049833493 theta= [[-7.13516774]\n",
      " [ 0.06306193]\n",
      " [ 0.05649786]]\n",
      "the iterations of  185600 the loss is 0.3233624824113527 theta= [[-7.13740431]\n",
      " [ 0.06307926]\n",
      " [ 0.05651552]]\n",
      "the iterations of  185700 the loss is 0.32331249855503025 theta= [[-7.13964002]\n",
      " [ 0.06309657]\n",
      " [ 0.05653317]]\n",
      "the iterations of  185800 the loss is 0.3232625533691894 theta= [[-7.14187486]\n",
      " [ 0.06311388]\n",
      " [ 0.05655082]]\n",
      "the iterations of  185900 the loss is 0.3232126468087073 theta= [[-7.14410883]\n",
      " [ 0.06313118]\n",
      " [ 0.05656846]]\n",
      "the iterations of  186000 the loss is 0.32316277882852995 theta= [[-7.14634194]\n",
      " [ 0.06314848]\n",
      " [ 0.0565861 ]]\n",
      "the iterations of  186100 the loss is 0.32311294938367374 theta= [[-7.14857419]\n",
      " [ 0.06316577]\n",
      " [ 0.05660373]]\n",
      "the iterations of  186200 the loss is 0.3230631584292236 theta= [[-7.15080558]\n",
      " [ 0.06318305]\n",
      " [ 0.05662135]]\n",
      "the iterations of  186300 the loss is 0.3230134059203341 theta= [[-7.1530361 ]\n",
      " [ 0.06320033]\n",
      " [ 0.05663896]]\n",
      "the iterations of  186400 the loss is 0.32296369181222906 theta= [[-7.15526577]\n",
      " [ 0.0632176 ]\n",
      " [ 0.05665657]]\n",
      "the iterations of  186500 the loss is 0.32291401606020054 theta= [[-7.15749457]\n",
      " [ 0.06323487]\n",
      " [ 0.05667417]]\n",
      "the iterations of  186600 the loss is 0.32286437861961004 theta= [[-7.15972252]\n",
      " [ 0.06325212]\n",
      " [ 0.05669177]]\n",
      "the iterations of  186700 the loss is 0.32281477944588743 theta= [[-7.1619496 ]\n",
      " [ 0.06326938]\n",
      " [ 0.05670936]]\n",
      "the iterations of  186800 the loss is 0.32276521849453116 theta= [[-7.16417583]\n",
      " [ 0.06328662]\n",
      " [ 0.05672694]]\n",
      "the iterations of  186900 the loss is 0.322715695721108 theta= [[-7.1664012 ]\n",
      " [ 0.06330386]\n",
      " [ 0.05674451]]\n",
      "the iterations of  187000 the loss is 0.3226662110812535 theta= [[-7.16862571]\n",
      " [ 0.06332109]\n",
      " [ 0.05676208]]\n",
      "the iterations of  187100 the loss is 0.3226167645306704 theta= [[-7.17084936]\n",
      " [ 0.06333832]\n",
      " [ 0.05677964]]\n",
      "the iterations of  187200 the loss is 0.3225673560251303 theta= [[-7.17307216]\n",
      " [ 0.06335554]\n",
      " [ 0.0567972 ]]\n",
      "the iterations of  187300 the loss is 0.32251798552047284 theta= [[-7.17529411]\n",
      " [ 0.06337275]\n",
      " [ 0.05681475]]\n",
      "the iterations of  187400 the loss is 0.3224686529726042 theta= [[-7.1775152 ]\n",
      " [ 0.06338996]\n",
      " [ 0.05683229]]\n",
      "the iterations of  187500 the loss is 0.3224193583374997 theta= [[-7.17973544]\n",
      " [ 0.06340716]\n",
      " [ 0.05684982]]\n",
      "the iterations of  187600 the loss is 0.3223701015712011 theta= [[-7.18195482]\n",
      " [ 0.06342436]\n",
      " [ 0.05686735]]\n",
      "the iterations of  187700 the loss is 0.3223208826298183 theta= [[-7.18417335]\n",
      " [ 0.06344155]\n",
      " [ 0.05688487]]\n",
      "the iterations of  187800 the loss is 0.3222717014695277 theta= [[-7.18639103]\n",
      " [ 0.06345873]\n",
      " [ 0.05690239]]\n",
      "the iterations of  187900 the loss is 0.3222225580465736 theta= [[-7.18860786]\n",
      " [ 0.06347591]\n",
      " [ 0.05691989]]\n",
      "the iterations of  188000 the loss is 0.32217345231726713 theta= [[-7.19082384]\n",
      " [ 0.06349308]\n",
      " [ 0.0569374 ]]\n",
      "the iterations of  188100 the loss is 0.3221243842379857 theta= [[-7.19303897]\n",
      " [ 0.06351024]\n",
      " [ 0.05695489]]\n",
      "the iterations of  188200 the loss is 0.32207535376517377 theta= [[-7.19525325]\n",
      " [ 0.0635274 ]\n",
      " [ 0.05697238]]\n",
      "the iterations of  188300 the loss is 0.3220263608553431 theta= [[-7.19746668]\n",
      " [ 0.06354455]\n",
      " [ 0.05698986]]\n",
      "the iterations of  188400 the loss is 0.321977405465071 theta= [[-7.19967926]\n",
      " [ 0.06356169]\n",
      " [ 0.05700734]]\n",
      "the iterations of  188500 the loss is 0.32192848755100184 theta= [[-7.201891  ]\n",
      " [ 0.06357883]\n",
      " [ 0.05702481]]\n",
      "the iterations of  188600 the loss is 0.3218796070698458 theta= [[-7.20410189]\n",
      " [ 0.06359596]\n",
      " [ 0.05704227]]\n",
      "the iterations of  188700 the loss is 0.3218307639783795 theta= [[-7.20631194]\n",
      " [ 0.06361309]\n",
      " [ 0.05705972]]\n",
      "the iterations of  188800 the loss is 0.3217819582334455 theta= [[-7.20852113]\n",
      " [ 0.06363021]\n",
      " [ 0.05707717]]\n",
      "the iterations of  188900 the loss is 0.32173318979195203 theta= [[-7.21072949]\n",
      " [ 0.06364732]\n",
      " [ 0.05709461]]\n",
      "the iterations of  189000 the loss is 0.32168445861087347 theta= [[-7.212937  ]\n",
      " [ 0.06366443]\n",
      " [ 0.05711205]]\n",
      "the iterations of  189100 the loss is 0.32163576464724936 theta= [[-7.21514367]\n",
      " [ 0.06368153]\n",
      " [ 0.05712948]]\n",
      "the iterations of  189200 the loss is 0.321587107858185 theta= [[-7.21734949]\n",
      " [ 0.06369863]\n",
      " [ 0.0571469 ]]\n",
      "the iterations of  189300 the loss is 0.32153848820085146 theta= [[-7.21955448]\n",
      " [ 0.06371572]\n",
      " [ 0.05716432]]\n",
      "the iterations of  189400 the loss is 0.3214899056324844 theta= [[-7.22175862]\n",
      " [ 0.0637328 ]\n",
      " [ 0.05718173]]\n",
      "the iterations of  189500 the loss is 0.3214413601103852 theta= [[-7.22396192]\n",
      " [ 0.06374988]\n",
      " [ 0.05719913]]\n",
      "the iterations of  189600 the loss is 0.32139285159191994 theta= [[-7.22616438]\n",
      " [ 0.06376695]\n",
      " [ 0.05721653]]\n",
      "the iterations of  189700 the loss is 0.3213443800345197 theta= [[-7.22836601]\n",
      " [ 0.06378401]\n",
      " [ 0.05723392]]\n",
      "the iterations of  189800 the loss is 0.3212959453956802 theta= [[-7.23056679]\n",
      " [ 0.06380107]\n",
      " [ 0.0572513 ]]\n",
      "the iterations of  189900 the loss is 0.32124754763296226 theta= [[-7.23276674]\n",
      " [ 0.06381812]\n",
      " [ 0.05726868]]\n",
      "the iterations of  190000 the loss is 0.3211991867039906 theta= [[-7.23496585]\n",
      " [ 0.06383517]\n",
      " [ 0.05728605]]\n",
      "the iterations of  190100 the loss is 0.32115086256645503 theta= [[-7.23716412]\n",
      " [ 0.06385221]\n",
      " [ 0.05730341]]\n",
      "the iterations of  190200 the loss is 0.3211025751781092 theta= [[-7.23936155]\n",
      " [ 0.06386924]\n",
      " [ 0.05732077]]\n",
      "the iterations of  190300 the loss is 0.3210543244967712 theta= [[-7.24155816]\n",
      " [ 0.06388627]\n",
      " [ 0.05733812]]\n",
      "the iterations of  190400 the loss is 0.3210061104803227 theta= [[-7.24375392]\n",
      " [ 0.06390329]\n",
      " [ 0.05735546]]\n",
      "the iterations of  190500 the loss is 0.3209579330867101 theta= [[-7.24594885]\n",
      " [ 0.0639203 ]\n",
      " [ 0.0573728 ]]\n",
      "the iterations of  190600 the loss is 0.32090979227394323 theta= [[-7.24814295]\n",
      " [ 0.06393731]\n",
      " [ 0.05739013]]\n",
      "the iterations of  190700 the loss is 0.3208616880000954 theta= [[-7.25033622]\n",
      " [ 0.06395431]\n",
      " [ 0.05740746]]\n",
      "the iterations of  190800 the loss is 0.3208136202233036 theta= [[-7.25252865]\n",
      " [ 0.06397131]\n",
      " [ 0.05742478]]\n",
      "the iterations of  190900 the loss is 0.3207655889017687 theta= [[-7.25472025]\n",
      " [ 0.0639883 ]\n",
      " [ 0.05744209]]\n",
      "the iterations of  191000 the loss is 0.3207175939937546 theta= [[-7.25691102]\n",
      " [ 0.06400528]\n",
      " [ 0.05745939]]\n",
      "the iterations of  191100 the loss is 0.32066963545758836 theta= [[-7.25910096]\n",
      " [ 0.06402226]\n",
      " [ 0.05747669]]\n",
      "the iterations of  191200 the loss is 0.3206217132516604 theta= [[-7.26129007]\n",
      " [ 0.06403923]\n",
      " [ 0.05749398]]\n",
      "the iterations of  191300 the loss is 0.3205738273344238 theta= [[-7.26347836]\n",
      " [ 0.0640562 ]\n",
      " [ 0.05751127]]\n",
      "the iterations of  191400 the loss is 0.32052597766439495 theta= [[-7.26566581]\n",
      " [ 0.06407316]\n",
      " [ 0.05752855]]\n",
      "the iterations of  191500 the loss is 0.32047816420015274 theta= [[-7.26785243]\n",
      " [ 0.06409011]\n",
      " [ 0.05754582]]\n",
      "the iterations of  191600 the loss is 0.3204303869003386 theta= [[-7.27003823]\n",
      " [ 0.06410706]\n",
      " [ 0.05756309]]\n",
      "the iterations of  191700 the loss is 0.3203826457236566 theta= [[-7.27222321]\n",
      " [ 0.064124  ]\n",
      " [ 0.05758035]]\n",
      "the iterations of  191800 the loss is 0.32033494062887363 theta= [[-7.27440735]\n",
      " [ 0.06414094]\n",
      " [ 0.0575976 ]]\n",
      "the iterations of  191900 the loss is 0.32028727157481823 theta= [[-7.27659067]\n",
      " [ 0.06415786]\n",
      " [ 0.05761485]]\n",
      "the iterations of  192000 the loss is 0.32023963852038145 theta= [[-7.27877317]\n",
      " [ 0.06417479]\n",
      " [ 0.05763209]]\n",
      "the iterations of  192100 the loss is 0.3201920414245163 theta= [[-7.28095484]\n",
      " [ 0.0641917 ]\n",
      " [ 0.05764933]]\n",
      "the iterations of  192200 the loss is 0.3201444802462381 theta= [[-7.28313569]\n",
      " [ 0.06420862]\n",
      " [ 0.05766656]]\n",
      "the iterations of  192300 the loss is 0.3200969549446235 theta= [[-7.28531572]\n",
      " [ 0.06422552]\n",
      " [ 0.05768378]]\n",
      "the iterations of  192400 the loss is 0.3200494654788112 theta= [[-7.28749492]\n",
      " [ 0.06424242]\n",
      " [ 0.05770099]]\n",
      "the iterations of  192500 the loss is 0.3200020118080014 theta= [[-7.2896733 ]\n",
      " [ 0.06425931]\n",
      " [ 0.0577182 ]]\n",
      "the iterations of  192600 the loss is 0.3199545938914558 theta= [[-7.29185087]\n",
      " [ 0.0642762 ]\n",
      " [ 0.0577354 ]]\n",
      "the iterations of  192700 the loss is 0.31990721168849745 theta= [[-7.29402761]\n",
      " [ 0.06429308]\n",
      " [ 0.0577526 ]]\n",
      "the iterations of  192800 the loss is 0.31985986515851095 theta= [[-7.29620353]\n",
      " [ 0.06430995]\n",
      " [ 0.05776979]]\n",
      "the iterations of  192900 the loss is 0.31981255426094163 theta= [[-7.29837863]\n",
      " [ 0.06432682]\n",
      " [ 0.05778697]]\n",
      "the iterations of  193000 the loss is 0.31976527895529616 theta= [[-7.30055292]\n",
      " [ 0.06434368]\n",
      " [ 0.05780415]]\n",
      "the iterations of  193100 the loss is 0.3197180392011418 theta= [[-7.30272638]\n",
      " [ 0.06436054]\n",
      " [ 0.05782132]]\n",
      "the iterations of  193200 the loss is 0.3196708349581072 theta= [[-7.30489904]\n",
      " [ 0.06437739]\n",
      " [ 0.05783849]]\n",
      "the iterations of  193300 the loss is 0.31962366618588123 theta= [[-7.30707087]\n",
      " [ 0.06439423]\n",
      " [ 0.05785564]]\n",
      "the iterations of  193400 the loss is 0.31957653284421356 theta= [[-7.30924189]\n",
      " [ 0.06441107]\n",
      " [ 0.0578728 ]]\n",
      "the iterations of  193500 the loss is 0.3195294348929142 theta= [[-7.31141209]\n",
      " [ 0.0644279 ]\n",
      " [ 0.05788994]]\n",
      "the iterations of  193600 the loss is 0.3194823722918537 theta= [[-7.31358148]\n",
      " [ 0.06444473]\n",
      " [ 0.05790708]]\n",
      "the iterations of  193700 the loss is 0.31943534500096293 theta= [[-7.31575005]\n",
      " [ 0.06446155]\n",
      " [ 0.05792421]]\n",
      "the iterations of  193800 the loss is 0.3193883529802325 theta= [[-7.31791781]\n",
      " [ 0.06447836]\n",
      " [ 0.05794134]]\n",
      "the iterations of  193900 the loss is 0.3193413961897134 theta= [[-7.32008476]\n",
      " [ 0.06449517]\n",
      " [ 0.05795846]]\n",
      "the iterations of  194000 the loss is 0.31929447458951665 theta= [[-7.32225089]\n",
      " [ 0.06451197]\n",
      " [ 0.05797557]]\n",
      "the iterations of  194100 the loss is 0.3192475881398127 theta= [[-7.32441622]\n",
      " [ 0.06452877]\n",
      " [ 0.05799268]]\n",
      "the iterations of  194200 the loss is 0.3192007368008322 theta= [[-7.32658073]\n",
      " [ 0.06454556]\n",
      " [ 0.05800978]]\n",
      "the iterations of  194300 the loss is 0.3191539205328648 theta= [[-7.32874443]\n",
      " [ 0.06456234]\n",
      " [ 0.05802688]]\n",
      "the iterations of  194400 the loss is 0.31910713929626006 theta= [[-7.33090733]\n",
      " [ 0.06457912]\n",
      " [ 0.05804396]]\n",
      "the iterations of  194500 the loss is 0.3190603930514267 theta= [[-7.33306941]\n",
      " [ 0.06459589]\n",
      " [ 0.05806105]]\n",
      "the iterations of  194600 the loss is 0.31901368175883293 theta= [[-7.33523069]\n",
      " [ 0.06461266]\n",
      " [ 0.05807812]]\n",
      "the iterations of  194700 the loss is 0.3189670053790062 theta= [[-7.33739115]\n",
      " [ 0.06462942]\n",
      " [ 0.05809519]]\n",
      "the iterations of  194800 the loss is 0.3189203638725326 theta= [[-7.33955082]\n",
      " [ 0.06464617]\n",
      " [ 0.05811225]]\n",
      "the iterations of  194900 the loss is 0.3188737572000575 theta= [[-7.34170967]\n",
      " [ 0.06466292]\n",
      " [ 0.05812931]]\n",
      "the iterations of  195000 the loss is 0.31882718532228477 theta= [[-7.34386772]\n",
      " [ 0.06467967]\n",
      " [ 0.05814636]]\n",
      "the iterations of  195100 the loss is 0.31878064819997726 theta= [[-7.34602496]\n",
      " [ 0.0646964 ]\n",
      " [ 0.05816341]]\n",
      "the iterations of  195200 the loss is 0.3187341457939566 theta= [[-7.3481814 ]\n",
      " [ 0.06471313]\n",
      " [ 0.05818044]]\n",
      "the iterations of  195300 the loss is 0.3186876780651026 theta= [[-7.35033703]\n",
      " [ 0.06472986]\n",
      " [ 0.05819748]]\n",
      "the iterations of  195400 the loss is 0.31864124497435353 theta= [[-7.35249186]\n",
      " [ 0.06474657]\n",
      " [ 0.0582145 ]]\n",
      "the iterations of  195500 the loss is 0.31859484648270614 theta= [[-7.35464589]\n",
      " [ 0.06476329]\n",
      " [ 0.05823152]]\n",
      "the iterations of  195600 the loss is 0.3185484825512152 theta= [[-7.35679911]\n",
      " [ 0.06477999]\n",
      " [ 0.05824853]]\n",
      "the iterations of  195700 the loss is 0.3185021531409936 theta= [[-7.35895153]\n",
      " [ 0.06479669]\n",
      " [ 0.05826554]]\n",
      "the iterations of  195800 the loss is 0.3184558582132122 theta= [[-7.36110316]\n",
      " [ 0.06481339]\n",
      " [ 0.05828254]]\n",
      "the iterations of  195900 the loss is 0.3184095977290996 theta= [[-7.36325398]\n",
      " [ 0.06483008]\n",
      " [ 0.05829954]]\n",
      "the iterations of  196000 the loss is 0.31836337164994266 theta= [[-7.365404  ]\n",
      " [ 0.06484676]\n",
      " [ 0.05831653]]\n",
      "the iterations of  196100 the loss is 0.31831717993708536 theta= [[-7.36755322]\n",
      " [ 0.06486344]\n",
      " [ 0.05833351]]\n",
      "the iterations of  196200 the loss is 0.31827102255192935 theta= [[-7.36970164]\n",
      " [ 0.06488011]\n",
      " [ 0.05835048]]\n",
      "the iterations of  196300 the loss is 0.31822489945593396 theta= [[-7.37184927]\n",
      " [ 0.06489677]\n",
      " [ 0.05836745]]\n",
      "the iterations of  196400 the loss is 0.3181788106106156 theta= [[-7.37399609]\n",
      " [ 0.06491343]\n",
      " [ 0.05838442]]\n",
      "the iterations of  196500 the loss is 0.31813275597754787 theta= [[-7.37614212]\n",
      " [ 0.06493009]\n",
      " [ 0.05840137]]\n",
      "the iterations of  196600 the loss is 0.31808673551836175 theta= [[-7.37828736]\n",
      " [ 0.06494673]\n",
      " [ 0.05841832]]\n",
      "the iterations of  196700 the loss is 0.31804074919474545 theta= [[-7.38043179]\n",
      " [ 0.06496338]\n",
      " [ 0.05843527]]\n",
      "the iterations of  196800 the loss is 0.31799479696844335 theta= [[-7.38257544]\n",
      " [ 0.06498001]\n",
      " [ 0.05845221]]\n",
      "the iterations of  196900 the loss is 0.31794887880125744 theta= [[-7.38471829]\n",
      " [ 0.06499664]\n",
      " [ 0.05846914]]\n",
      "the iterations of  197000 the loss is 0.317902994655046 theta= [[-7.38686034]\n",
      " [ 0.06501327]\n",
      " [ 0.05848607]]\n",
      "the iterations of  197100 the loss is 0.31785714449172375 theta= [[-7.3890016 ]\n",
      " [ 0.06502988]\n",
      " [ 0.05850299]]\n",
      "the iterations of  197200 the loss is 0.3178113282732624 theta= [[-7.39114207]\n",
      " [ 0.0650465 ]\n",
      " [ 0.0585199 ]]\n",
      "the iterations of  197300 the loss is 0.31776554596168993 theta= [[-7.39328174]\n",
      " [ 0.0650631 ]\n",
      " [ 0.05853681]]\n",
      "the iterations of  197400 the loss is 0.31771979751909035 theta= [[-7.39542063]\n",
      " [ 0.0650797 ]\n",
      " [ 0.05855371]]\n",
      "the iterations of  197500 the loss is 0.31767408290760424 theta= [[-7.39755872]\n",
      " [ 0.0650963 ]\n",
      " [ 0.0585706 ]]\n",
      "the iterations of  197600 the loss is 0.3176284020894281 theta= [[-7.39969602]\n",
      " [ 0.06511289]\n",
      " [ 0.05858749]]\n",
      "the iterations of  197700 the loss is 0.31758275502681405 theta= [[-7.40183254]\n",
      " [ 0.06512947]\n",
      " [ 0.05860438]]\n",
      "the iterations of  197800 the loss is 0.31753714168207087 theta= [[-7.40396826]\n",
      " [ 0.06514605]\n",
      " [ 0.05862125]]\n",
      "the iterations of  197900 the loss is 0.31749156201756246 theta= [[-7.40610319]\n",
      " [ 0.06516262]\n",
      " [ 0.05863812]]\n",
      "the iterations of  198000 the loss is 0.317446015995709 theta= [[-7.40823734]\n",
      " [ 0.06517918]\n",
      " [ 0.05865499]]\n",
      "the iterations of  198100 the loss is 0.3174005035789857 theta= [[-7.4103707 ]\n",
      " [ 0.06519574]\n",
      " [ 0.05867185]]\n",
      "the iterations of  198200 the loss is 0.31735502472992355 theta= [[-7.41250327]\n",
      " [ 0.0652123 ]\n",
      " [ 0.0586887 ]]\n",
      "the iterations of  198300 the loss is 0.31730957941110893 theta= [[-7.41463506]\n",
      " [ 0.06522884]\n",
      " [ 0.05870555]]\n",
      "the iterations of  198400 the loss is 0.31726416758518344 theta= [[-7.41676606]\n",
      " [ 0.06524539]\n",
      " [ 0.05872239]]\n",
      "the iterations of  198500 the loss is 0.317218789214844 theta= [[-7.41889627]\n",
      " [ 0.06526192]\n",
      " [ 0.05873922]]\n",
      "the iterations of  198600 the loss is 0.3171734442628422 theta= [[-7.42102571]\n",
      " [ 0.06527845]\n",
      " [ 0.05875605]]\n",
      "the iterations of  198700 the loss is 0.31712813269198525 theta= [[-7.42315435]\n",
      " [ 0.06529498]\n",
      " [ 0.05877287]]\n",
      "the iterations of  198800 the loss is 0.3170828544651347 theta= [[-7.42528222]\n",
      " [ 0.0653115 ]\n",
      " [ 0.05878969]]\n",
      "the iterations of  198900 the loss is 0.31703760954520716 theta= [[-7.4274093 ]\n",
      " [ 0.06532801]\n",
      " [ 0.0588065 ]]\n",
      "the iterations of  199000 the loss is 0.3169923978951742 theta= [[-7.42953559]\n",
      " [ 0.06534452]\n",
      " [ 0.0588233 ]]\n",
      "the iterations of  199100 the loss is 0.3169472194780611 theta= [[-7.43166111]\n",
      " [ 0.06536102]\n",
      " [ 0.0588401 ]]\n",
      "the iterations of  199200 the loss is 0.3169020742569485 theta= [[-7.43378585]\n",
      " [ 0.06537751]\n",
      " [ 0.05885689]]\n",
      "the iterations of  199300 the loss is 0.31685696219497106 theta= [[-7.4359098 ]\n",
      " [ 0.065394  ]\n",
      " [ 0.05887367]]\n",
      "the iterations of  199400 the loss is 0.3168118832553178 theta= [[-7.43803298]\n",
      " [ 0.06541049]\n",
      " [ 0.05889045]]\n",
      "the iterations of  199500 the loss is 0.3167668374012319 theta= [[-7.44015537]\n",
      " [ 0.06542697]\n",
      " [ 0.05890723]]\n",
      "the iterations of  199600 the loss is 0.31672182459601056 theta= [[-7.44227699]\n",
      " [ 0.06544344]\n",
      " [ 0.05892399]]\n",
      "the iterations of  199700 the loss is 0.31667684480300506 theta= [[-7.44439783]\n",
      " [ 0.06545991]\n",
      " [ 0.05894075]]\n",
      "the iterations of  199800 the loss is 0.31663189798562075 theta= [[-7.44651789]\n",
      " [ 0.06547637]\n",
      " [ 0.05895751]]\n",
      "the iterations of  199900 the loss is 0.3165869841073166 theta= [[-7.44863718]\n",
      " [ 0.06549282]\n",
      " [ 0.05897426]]\n",
      "the iterations of  200000 the loss is 0.31654210313160513 theta= [[-7.45075568]\n",
      " [ 0.06550927]\n",
      " [ 0.058991  ]]\n",
      "the iterations of  200100 the loss is 0.31649725502205284 theta= [[-7.45287342]\n",
      " [ 0.06552571]\n",
      " [ 0.05900774]]\n",
      "the iterations of  200200 the loss is 0.31645243974227955 theta= [[-7.45499037]\n",
      " [ 0.06554215]\n",
      " [ 0.05902447]]\n",
      "the iterations of  200300 the loss is 0.3164076572559582 theta= [[-7.45710655]\n",
      " [ 0.06555859]\n",
      " [ 0.05904119]]\n",
      "the iterations of  200400 the loss is 0.3163629075268157 theta= [[-7.45922196]\n",
      " [ 0.06557501]\n",
      " [ 0.05905791]]\n",
      "the iterations of  200500 the loss is 0.31631819051863164 theta= [[-7.4613366 ]\n",
      " [ 0.06559143]\n",
      " [ 0.05907462]]\n",
      "the iterations of  200600 the loss is 0.3162735061952389 theta= [[-7.46345046]\n",
      " [ 0.06560785]\n",
      " [ 0.05909133]]\n",
      "the iterations of  200700 the loss is 0.3162288545205235 theta= [[-7.46556355]\n",
      " [ 0.06562426]\n",
      " [ 0.05910803]]\n",
      "the iterations of  200800 the loss is 0.3161842354584244 theta= [[-7.46767587]\n",
      " [ 0.06564066]\n",
      " [ 0.05912473]]\n",
      "the iterations of  200900 the loss is 0.31613964897293273 theta= [[-7.46978741]\n",
      " [ 0.06565706]\n",
      " [ 0.05914142]]\n",
      "the iterations of  201000 the loss is 0.3160950950280936 theta= [[-7.47189819]\n",
      " [ 0.06567345]\n",
      " [ 0.0591581 ]]\n",
      "the iterations of  201100 the loss is 0.3160505735880039 theta= [[-7.47400819]\n",
      " [ 0.06568984]\n",
      " [ 0.05917477]]\n",
      "the iterations of  201200 the loss is 0.3160060846168131 theta= [[-7.47611743]\n",
      " [ 0.06570622]\n",
      " [ 0.05919145]]\n",
      "the iterations of  201300 the loss is 0.3159616280787235 theta= [[-7.47822589]\n",
      " [ 0.06572259]\n",
      " [ 0.05920811]]\n",
      "the iterations of  201400 the loss is 0.3159172039379894 theta= [[-7.48033359]\n",
      " [ 0.06573896]\n",
      " [ 0.05922477]]\n",
      "the iterations of  201500 the loss is 0.3158728121589177 theta= [[-7.48244052]\n",
      " [ 0.06575533]\n",
      " [ 0.05924142]]\n",
      "the iterations of  201600 the loss is 0.3158284527058673 theta= [[-7.48454669]\n",
      " [ 0.06577168]\n",
      " [ 0.05925807]]\n",
      "the iterations of  201700 the loss is 0.3157841255432489 theta= [[-7.48665208]\n",
      " [ 0.06578804]\n",
      " [ 0.05927471]]\n",
      "the iterations of  201800 the loss is 0.31573983063552585 theta= [[-7.48875671]\n",
      " [ 0.06580438]\n",
      " [ 0.05929134]]\n",
      "the iterations of  201900 the loss is 0.315695567947213 theta= [[-7.49086058]\n",
      " [ 0.06582072]\n",
      " [ 0.05930797]]\n",
      "the iterations of  202000 the loss is 0.31565133744287693 theta= [[-7.49296368]\n",
      " [ 0.06583706]\n",
      " [ 0.0593246 ]]\n",
      "the iterations of  202100 the loss is 0.315607139087136 theta= [[-7.49506601]\n",
      " [ 0.06585339]\n",
      " [ 0.05934121]]\n",
      "the iterations of  202200 the loss is 0.3155629728446602 theta= [[-7.49716758]\n",
      " [ 0.06586971]\n",
      " [ 0.05935782]]\n",
      "the iterations of  202300 the loss is 0.31551883868017094 theta= [[-7.49926839]\n",
      " [ 0.06588603]\n",
      " [ 0.05937443]]\n",
      "the iterations of  202400 the loss is 0.31547473655844144 theta= [[-7.50136844]\n",
      " [ 0.06590234]\n",
      " [ 0.05939103]]\n",
      "the iterations of  202500 the loss is 0.31543066644429607 theta= [[-7.50346772]\n",
      " [ 0.06591865]\n",
      " [ 0.05940762]]\n",
      "the iterations of  202600 the loss is 0.31538662830261005 theta= [[-7.50556624]\n",
      " [ 0.06593495]\n",
      " [ 0.05942421]]\n",
      "the iterations of  202700 the loss is 0.31534262209831027 theta= [[-7.507664  ]\n",
      " [ 0.06595125]\n",
      " [ 0.05944079]]\n",
      "the iterations of  202800 the loss is 0.31529864779637445 theta= [[-7.509761  ]\n",
      " [ 0.06596754]\n",
      " [ 0.05945737]]\n",
      "the iterations of  202900 the loss is 0.3152547053618313 theta= [[-7.51185724]\n",
      " [ 0.06598382]\n",
      " [ 0.05947394]]\n",
      "the iterations of  203000 the loss is 0.3152107947597605 theta= [[-7.51395272]\n",
      " [ 0.0660001 ]\n",
      " [ 0.0594905 ]]\n",
      "the iterations of  203100 the loss is 0.31516691595529234 theta= [[-7.51604744]\n",
      " [ 0.06601637]\n",
      " [ 0.05950706]]\n",
      "the iterations of  203200 the loss is 0.31512306891360803 theta= [[-7.5181414 ]\n",
      " [ 0.06603264]\n",
      " [ 0.05952361]]\n",
      "the iterations of  203300 the loss is 0.3150792535999392 theta= [[-7.52023461]\n",
      " [ 0.0660489 ]\n",
      " [ 0.05954016]]\n",
      "the iterations of  203400 the loss is 0.3150354699795683 theta= [[-7.52232706]\n",
      " [ 0.06606516]\n",
      " [ 0.0595567 ]]\n",
      "the iterations of  203500 the loss is 0.31499171801782766 theta= [[-7.52441875]\n",
      " [ 0.06608141]\n",
      " [ 0.05957323]]\n",
      "the iterations of  203600 the loss is 0.31494799768010034 theta= [[-7.52650968]\n",
      " [ 0.06609766]\n",
      " [ 0.05958976]]\n",
      "the iterations of  203700 the loss is 0.3149043089318197 theta= [[-7.52859986]\n",
      " [ 0.06611389]\n",
      " [ 0.05960628]]\n",
      "the iterations of  203800 the loss is 0.31486065173846906 theta= [[-7.53068929]\n",
      " [ 0.06613013]\n",
      " [ 0.0596228 ]]\n",
      "the iterations of  203900 the loss is 0.31481702606558193 theta= [[-7.53277796]\n",
      " [ 0.06614636]\n",
      " [ 0.05963931]]\n",
      "the iterations of  204000 the loss is 0.31477343187874146 theta= [[-7.53486588]\n",
      " [ 0.06616258]\n",
      " [ 0.05965581]]\n",
      "the iterations of  204100 the loss is 0.3147298691435814 theta= [[-7.53695304]\n",
      " [ 0.0661788 ]\n",
      " [ 0.05967231]]\n",
      "the iterations of  204200 the loss is 0.3146863378257846 theta= [[-7.53903945]\n",
      " [ 0.06619501]\n",
      " [ 0.0596888 ]]\n",
      "the iterations of  204300 the loss is 0.31464283789108405 theta= [[-7.54112511]\n",
      " [ 0.06621121]\n",
      " [ 0.05970529]]\n",
      "the iterations of  204400 the loss is 0.3145993693052619 theta= [[-7.54321001]\n",
      " [ 0.06622741]\n",
      " [ 0.05972177]]\n",
      "the iterations of  204500 the loss is 0.3145559320341505 theta= [[-7.54529417]\n",
      " [ 0.06624361]\n",
      " [ 0.05973825]]\n",
      "the iterations of  204600 the loss is 0.3145125260436311 theta= [[-7.54737757]\n",
      " [ 0.0662598 ]\n",
      " [ 0.05975472]]\n",
      "the iterations of  204700 the loss is 0.3144691512996344 theta= [[-7.54946023]\n",
      " [ 0.06627598]\n",
      " [ 0.05977118]]\n",
      "the iterations of  204800 the loss is 0.31442580776814055 theta= [[-7.55154213]\n",
      " [ 0.06629216]\n",
      " [ 0.05978764]]\n",
      "the iterations of  204900 the loss is 0.3143824954151788 theta= [[-7.55362329]\n",
      " [ 0.06630833]\n",
      " [ 0.05980409]]\n",
      "the iterations of  205000 the loss is 0.3143392142068274 theta= [[-7.5557037 ]\n",
      " [ 0.0663245 ]\n",
      " [ 0.05982054]]\n",
      "the iterations of  205100 the loss is 0.31429596410921395 theta= [[-7.55778336]\n",
      " [ 0.06634066]\n",
      " [ 0.05983698]]\n",
      "the iterations of  205200 the loss is 0.31425274508851436 theta= [[-7.55986227]\n",
      " [ 0.06635682]\n",
      " [ 0.05985341]]\n",
      "the iterations of  205300 the loss is 0.3142095571109539 theta= [[-7.56194044]\n",
      " [ 0.06637297]\n",
      " [ 0.05986984]]\n",
      "the iterations of  205400 the loss is 0.31416640014280633 theta= [[-7.56401786]\n",
      " [ 0.06638911]\n",
      " [ 0.05988627]]\n",
      "the iterations of  205500 the loss is 0.31412327415039404 theta= [[-7.56609453]\n",
      " [ 0.06640525]\n",
      " [ 0.05990268]]\n",
      "the iterations of  205600 the loss is 0.3140801791000883 theta= [[-7.56817046]\n",
      " [ 0.06642138]\n",
      " [ 0.05991909]]\n",
      "the iterations of  205700 the loss is 0.3140371149583087 theta= [[-7.57024564]\n",
      " [ 0.06643751]\n",
      " [ 0.0599355 ]]\n",
      "the iterations of  205800 the loss is 0.313994081691523 theta= [[-7.57232008]\n",
      " [ 0.06645363]\n",
      " [ 0.0599519 ]]\n",
      "the iterations of  205900 the loss is 0.31395107926624755 theta= [[-7.57439378]\n",
      " [ 0.06646975]\n",
      " [ 0.05996829]]\n",
      "the iterations of  206000 the loss is 0.31390810764904664 theta= [[-7.57646673]\n",
      " [ 0.06648586]\n",
      " [ 0.05998468]]\n",
      "the iterations of  206100 the loss is 0.31386516680653304 theta= [[-7.57853894]\n",
      " [ 0.06650197]\n",
      " [ 0.06000107]]\n",
      "the iterations of  206200 the loss is 0.31382225670536745 theta= [[-7.58061041]\n",
      " [ 0.06651807]\n",
      " [ 0.06001744]]\n",
      "the iterations of  206300 the loss is 0.3137793773122586 theta= [[-7.58268114]\n",
      " [ 0.06653416]\n",
      " [ 0.06003381]]\n",
      "the iterations of  206400 the loss is 0.3137365285939629 theta= [[-7.58475113]\n",
      " [ 0.06655025]\n",
      " [ 0.06005018]]\n",
      "the iterations of  206500 the loss is 0.3136937105172846 theta= [[-7.58682038]\n",
      " [ 0.06656634]\n",
      " [ 0.06006654]]\n",
      "the iterations of  206600 the loss is 0.31365092304907594 theta= [[-7.58888888]\n",
      " [ 0.06658242]\n",
      " [ 0.06008289]]\n",
      "the iterations of  206700 the loss is 0.3136081661562365 theta= [[-7.59095665]\n",
      " [ 0.06659849]\n",
      " [ 0.06009924]]\n",
      "the iterations of  206800 the loss is 0.3135654398057136 theta= [[-7.59302368]\n",
      " [ 0.06661456]\n",
      " [ 0.06011558]]\n",
      "the iterations of  206900 the loss is 0.3135227439645021 theta= [[-7.59508997]\n",
      " [ 0.06663062]\n",
      " [ 0.06013192]]\n",
      "the iterations of  207000 the loss is 0.3134800785996438 theta= [[-7.59715552]\n",
      " [ 0.06664668]\n",
      " [ 0.06014825]]\n",
      "the iterations of  207100 the loss is 0.31343744367822823 theta= [[-7.59922034]\n",
      " [ 0.06666273]\n",
      " [ 0.06016457]]\n",
      "the iterations of  207200 the loss is 0.31339483916739197 theta= [[-7.60128442]\n",
      " [ 0.06667877]\n",
      " [ 0.06018089]]\n",
      "the iterations of  207300 the loss is 0.3133522650343187 theta= [[-7.60334776]\n",
      " [ 0.06669481]\n",
      " [ 0.06019721]]\n",
      "the iterations of  207400 the loss is 0.31330972124623924 theta= [[-7.60541037]\n",
      " [ 0.06671085]\n",
      " [ 0.06021352]]\n",
      "the iterations of  207500 the loss is 0.31326720777043116 theta= [[-7.60747225]\n",
      " [ 0.06672688]\n",
      " [ 0.06022982]]\n",
      "the iterations of  207600 the loss is 0.31322472457421935 theta= [[-7.60953338]\n",
      " [ 0.0667429 ]\n",
      " [ 0.06024611]]\n",
      "the iterations of  207700 the loss is 0.313182271624975 theta= [[-7.61159379]\n",
      " [ 0.06675892]\n",
      " [ 0.0602624 ]]\n",
      "the iterations of  207800 the loss is 0.31313984889011626 theta= [[-7.61365346]\n",
      " [ 0.06677493]\n",
      " [ 0.06027869]]\n",
      "the iterations of  207900 the loss is 0.31309745633710795 theta= [[-7.6157124 ]\n",
      " [ 0.06679094]\n",
      " [ 0.06029497]]\n",
      "the iterations of  208000 the loss is 0.31305509393346126 theta= [[-7.61777061]\n",
      " [ 0.06680694]\n",
      " [ 0.06031124]]\n",
      "the iterations of  208100 the loss is 0.3130127616467341 theta= [[-7.61982808]\n",
      " [ 0.06682294]\n",
      " [ 0.06032751]]\n",
      "the iterations of  208200 the loss is 0.31297045944453056 theta= [[-7.62188482]\n",
      " [ 0.06683893]\n",
      " [ 0.06034377]]\n",
      "the iterations of  208300 the loss is 0.31292818729450106 theta= [[-7.62394084]\n",
      " [ 0.06685491]\n",
      " [ 0.06036003]]\n",
      "the iterations of  208400 the loss is 0.3128859451643422 theta= [[-7.62599612]\n",
      " [ 0.06687089]\n",
      " [ 0.06037628]]\n",
      "the iterations of  208500 the loss is 0.3128437330217971 theta= [[-7.62805067]\n",
      " [ 0.06688687]\n",
      " [ 0.06039252]]\n",
      "the iterations of  208600 the loss is 0.31280155083465433 theta= [[-7.6301045 ]\n",
      " [ 0.06690284]\n",
      " [ 0.06040876]]\n",
      "the iterations of  208700 the loss is 0.31275939857074886 theta= [[-7.63215759]\n",
      " [ 0.0669188 ]\n",
      " [ 0.060425  ]]\n",
      "the iterations of  208800 the loss is 0.31271727619796164 theta= [[-7.63420996]\n",
      " [ 0.06693476]\n",
      " [ 0.06044123]]\n",
      "the iterations of  208900 the loss is 0.312675183684219 theta= [[-7.6362616 ]\n",
      " [ 0.06695071]\n",
      " [ 0.06045745]]\n",
      "the iterations of  209000 the loss is 0.3126331209974933 theta= [[-7.63831251]\n",
      " [ 0.06696666]\n",
      " [ 0.06047367]]\n",
      "the iterations of  209100 the loss is 0.3125910881058025 theta= [[-7.6403627 ]\n",
      " [ 0.0669826 ]\n",
      " [ 0.06048988]]\n",
      "the iterations of  209200 the loss is 0.3125490849772102 theta= [[-7.64241216]\n",
      " [ 0.06699854]\n",
      " [ 0.06050608]]\n",
      "the iterations of  209300 the loss is 0.3125071115798254 theta= [[-7.64446089]\n",
      " [ 0.06701447]\n",
      " [ 0.06052228]]\n",
      "the iterations of  209400 the loss is 0.31246516788180245 theta= [[-7.6465089 ]\n",
      " [ 0.0670304 ]\n",
      " [ 0.06053848]]\n",
      "the iterations of  209500 the loss is 0.3124232538513413 theta= [[-7.64855619]\n",
      " [ 0.06704632]\n",
      " [ 0.06055466]]\n",
      "the iterations of  209600 the loss is 0.312381369456687 theta= [[-7.65060275]\n",
      " [ 0.06706223]\n",
      " [ 0.06057085]]\n",
      "the iterations of  209700 the loss is 0.3123395146661297 theta= [[-7.65264858]\n",
      " [ 0.06707814]\n",
      " [ 0.06058702]]\n",
      "the iterations of  209800 the loss is 0.3122976894480047 theta= [[-7.6546937 ]\n",
      " [ 0.06709405]\n",
      " [ 0.0606032 ]]\n",
      "the iterations of  209900 the loss is 0.3122558937706923 theta= [[-7.65673809]\n",
      " [ 0.06710995]\n",
      " [ 0.06061936]]\n",
      "the iterations of  210000 the loss is 0.3122141276026179 theta= [[-7.65878176]\n",
      " [ 0.06712584]\n",
      " [ 0.06063552]]\n",
      "the iterations of  210100 the loss is 0.31217239091225163 theta= [[-7.66082471]\n",
      " [ 0.06714173]\n",
      " [ 0.06065168]]\n",
      "the iterations of  210200 the loss is 0.31213068366810837 theta= [[-7.66286694]\n",
      " [ 0.06715761]\n",
      " [ 0.06066783]]\n",
      "the iterations of  210300 the loss is 0.31208900583874777 theta= [[-7.66490845]\n",
      " [ 0.06717349]\n",
      " [ 0.06068397]]\n",
      "the iterations of  210400 the loss is 0.312047357392774 theta= [[-7.66694924]\n",
      " [ 0.06718936]\n",
      " [ 0.06070011]]\n",
      "the iterations of  210500 the loss is 0.31200573829883604 theta= [[-7.66898931]\n",
      " [ 0.06720523]\n",
      " [ 0.06071624]]\n",
      "the iterations of  210600 the loss is 0.3119641485256269 theta= [[-7.67102866]\n",
      " [ 0.06722109]\n",
      " [ 0.06073237]]\n",
      "the iterations of  210700 the loss is 0.3119225880418845 theta= [[-7.67306729]\n",
      " [ 0.06723695]\n",
      " [ 0.06074849]]\n",
      "the iterations of  210800 the loss is 0.3118810568163906 theta= [[-7.6751052]\n",
      " [ 0.0672528]\n",
      " [ 0.0607646]]\n",
      "the iterations of  210900 the loss is 0.3118395548179715 theta= [[-7.6771424 ]\n",
      " [ 0.06726864]\n",
      " [ 0.06078071]]\n",
      "the iterations of  211000 the loss is 0.3117980820154977 theta= [[-7.67917888]\n",
      " [ 0.06728448]\n",
      " [ 0.06079682]]\n",
      "the iterations of  211100 the loss is 0.31175663837788337 theta= [[-7.68121464]\n",
      " [ 0.06730032]\n",
      " [ 0.06081292]]\n",
      "the iterations of  211200 the loss is 0.3117152238740872 theta= [[-7.68324969]\n",
      " [ 0.06731615]\n",
      " [ 0.06082901]]\n",
      "the iterations of  211300 the loss is 0.31167383847311153 theta= [[-7.68528403]\n",
      " [ 0.06733197]\n",
      " [ 0.0608451 ]]\n",
      "the iterations of  211400 the loss is 0.3116324821440024 theta= [[-7.68731764]\n",
      " [ 0.06734779]\n",
      " [ 0.06086118]]\n",
      "the iterations of  211500 the loss is 0.3115911548558501 theta= [[-7.68935055]\n",
      " [ 0.0673636 ]\n",
      " [ 0.06087726]]\n",
      "the iterations of  211600 the loss is 0.3115498565777883 theta= [[-7.69138274]\n",
      " [ 0.06737941]\n",
      " [ 0.06089333]]\n",
      "the iterations of  211700 the loss is 0.3115085872789942 theta= [[-7.69341422]\n",
      " [ 0.06739522]\n",
      " [ 0.06090939]]\n",
      "the iterations of  211800 the loss is 0.3114673469286888 theta= [[-7.69544498]\n",
      " [ 0.06741101]\n",
      " [ 0.06092545]]\n",
      "the iterations of  211900 the loss is 0.31142613549613635 theta= [[-7.69747504]\n",
      " [ 0.06742681]\n",
      " [ 0.06094151]]\n",
      "the iterations of  212000 the loss is 0.3113849529506446 theta= [[-7.69950438]\n",
      " [ 0.06744259]\n",
      " [ 0.06095756]]\n",
      "the iterations of  212100 the loss is 0.3113437992615647 theta= [[-7.70153301]\n",
      " [ 0.06745837]\n",
      " [ 0.0609736 ]]\n",
      "the iterations of  212200 the loss is 0.3113026743982909 theta= [[-7.70356093]\n",
      " [ 0.06747415]\n",
      " [ 0.06098964]]\n",
      "the iterations of  212300 the loss is 0.3112615783302607 theta= [[-7.70558814]\n",
      " [ 0.06748992]\n",
      " [ 0.06100567]]\n",
      "the iterations of  212400 the loss is 0.3112205110269548 theta= [[-7.70761464]\n",
      " [ 0.06750569]\n",
      " [ 0.0610217 ]]\n",
      "the iterations of  212500 the loss is 0.3111794724578967 theta= [[-7.70964043]\n",
      " [ 0.06752145]\n",
      " [ 0.06103772]]\n",
      "the iterations of  212600 the loss is 0.3111384625926527 theta= [[-7.71166551]\n",
      " [ 0.0675372 ]\n",
      " [ 0.06105374]]\n",
      "the iterations of  212700 the loss is 0.31109748140083254 theta= [[-7.71368988]\n",
      " [ 0.06755295]\n",
      " [ 0.06106975]]\n",
      "the iterations of  212800 the loss is 0.3110565288520882 theta= [[-7.71571355]\n",
      " [ 0.0675687 ]\n",
      " [ 0.06108575]]\n",
      "the iterations of  212900 the loss is 0.3110156049161148 theta= [[-7.71773651]\n",
      " [ 0.06758444]\n",
      " [ 0.06110175]]\n",
      "the iterations of  213000 the loss is 0.3109747095626496 theta= [[-7.71975876]\n",
      " [ 0.06760017]\n",
      " [ 0.06111774]]\n",
      "the iterations of  213100 the loss is 0.31093384276147307 theta= [[-7.72178031]\n",
      " [ 0.0676159 ]\n",
      " [ 0.06113373]]\n",
      "the iterations of  213200 the loss is 0.31089300448240753 theta= [[-7.72380115]\n",
      " [ 0.06763163]\n",
      " [ 0.06114971]]\n",
      "the iterations of  213300 the loss is 0.31085219469531833 theta= [[-7.72582129]\n",
      " [ 0.06764734]\n",
      " [ 0.06116569]]\n",
      "the iterations of  213400 the loss is 0.3108114133701127 theta= [[-7.72784072]\n",
      " [ 0.06766306]\n",
      " [ 0.06118166]]\n",
      "the iterations of  213500 the loss is 0.3107706604767405 theta= [[-7.72985945]\n",
      " [ 0.06767877]\n",
      " [ 0.06119763]]\n",
      "the iterations of  213600 the loss is 0.31072993598519333 theta= [[-7.73187747]\n",
      " [ 0.06769447]\n",
      " [ 0.06121359]]\n",
      "the iterations of  213700 the loss is 0.31068923986550545 theta= [[-7.73389479]\n",
      " [ 0.06771017]\n",
      " [ 0.06122955]]\n",
      "the iterations of  213800 the loss is 0.3106485720877528 theta= [[-7.73591141]\n",
      " [ 0.06772586]\n",
      " [ 0.0612455 ]]\n",
      "the iterations of  213900 the loss is 0.3106079326220536 theta= [[-7.73792732]\n",
      " [ 0.06774155]\n",
      " [ 0.06126144]]\n",
      "the iterations of  214000 the loss is 0.3105673214385677 theta= [[-7.73994254]\n",
      " [ 0.06775723]\n",
      " [ 0.06127738]]\n",
      "the iterations of  214100 the loss is 0.31052673850749685 theta= [[-7.74195705]\n",
      " [ 0.0677729 ]\n",
      " [ 0.06129331]]\n",
      "the iterations of  214200 the loss is 0.3104861837990848 theta= [[-7.74397086]\n",
      " [ 0.06778858]\n",
      " [ 0.06130924]]\n",
      "the iterations of  214300 the loss is 0.3104456572836167 theta= [[-7.74598398]\n",
      " [ 0.06780424]\n",
      " [ 0.06132516]]\n",
      "the iterations of  214400 the loss is 0.3104051589314196 theta= [[-7.74799639]\n",
      " [ 0.0678199 ]\n",
      " [ 0.06134108]]\n",
      "the iterations of  214500 the loss is 0.31036468871286194 theta= [[-7.7500081 ]\n",
      " [ 0.06783556]\n",
      " [ 0.06135699]]\n",
      "the iterations of  214600 the loss is 0.31032424659835345 theta= [[-7.75201912]\n",
      " [ 0.06785121]\n",
      " [ 0.0613729 ]]\n",
      "the iterations of  214700 the loss is 0.3102838325583457 theta= [[-7.75402944]\n",
      " [ 0.06786685]\n",
      " [ 0.0613888 ]]\n",
      "the iterations of  214800 the loss is 0.3102434465633311 theta= [[-7.75603905]\n",
      " [ 0.06788249]\n",
      " [ 0.06140469]]\n",
      "the iterations of  214900 the loss is 0.3102030885838439 theta= [[-7.75804798]\n",
      " [ 0.06789813]\n",
      " [ 0.06142058]]\n",
      "the iterations of  215000 the loss is 0.3101627585904591 theta= [[-7.7600562 ]\n",
      " [ 0.06791376]\n",
      " [ 0.06143647]]\n",
      "the iterations of  215100 the loss is 0.3101224565537928 theta= [[-7.76206373]\n",
      " [ 0.06792938]\n",
      " [ 0.06145235]]\n",
      "the iterations of  215200 the loss is 0.3100821824445024 theta= [[-7.76407056]\n",
      " [ 0.067945  ]\n",
      " [ 0.06146822]]\n",
      "the iterations of  215300 the loss is 0.31004193623328624 theta= [[-7.7660767 ]\n",
      " [ 0.06796062]\n",
      " [ 0.06148409]]\n",
      "the iterations of  215400 the loss is 0.3100017178908836 theta= [[-7.76808215]\n",
      " [ 0.06797623]\n",
      " [ 0.06149995]]\n",
      "the iterations of  215500 the loss is 0.30996152738807436 theta= [[-7.7700869 ]\n",
      " [ 0.06799183]\n",
      " [ 0.06151581]]\n",
      "the iterations of  215600 the loss is 0.3099213646956794 theta= [[-7.77209095]\n",
      " [ 0.06800743]\n",
      " [ 0.06153166]]\n",
      "the iterations of  215700 the loss is 0.3098812297845602 theta= [[-7.77409431]\n",
      " [ 0.06802302]\n",
      " [ 0.06154751]]\n",
      "the iterations of  215800 the loss is 0.3098411226256189 theta= [[-7.77609698]\n",
      " [ 0.06803861]\n",
      " [ 0.06156335]]\n",
      "the iterations of  215900 the loss is 0.30980104318979806 theta= [[-7.77809896]\n",
      " [ 0.06805419]\n",
      " [ 0.06157919]]\n",
      "the iterations of  216000 the loss is 0.30976099144808106 theta= [[-7.78010025]\n",
      " [ 0.06806977]\n",
      " [ 0.06159502]]\n",
      "the iterations of  216100 the loss is 0.30972096737149174 theta= [[-7.78210084]\n",
      " [ 0.06808534]\n",
      " [ 0.06161084]]\n",
      "the iterations of  216200 the loss is 0.30968097093109326 theta= [[-7.78410074]\n",
      " [ 0.06810091]\n",
      " [ 0.06162666]]\n",
      "the iterations of  216300 the loss is 0.30964100209799056 theta= [[-7.78609996]\n",
      " [ 0.06811648]\n",
      " [ 0.06164248]]\n",
      "the iterations of  216400 the loss is 0.3096010608433277 theta= [[-7.78809848]\n",
      " [ 0.06813203]\n",
      " [ 0.06165829]]\n",
      "the iterations of  216500 the loss is 0.3095611471382896 theta= [[-7.79009631]\n",
      " [ 0.06814758]\n",
      " [ 0.06167409]]\n",
      "the iterations of  216600 the loss is 0.30952126095410054 theta= [[-7.79209346]\n",
      " [ 0.06816313]\n",
      " [ 0.06168989]]\n",
      "the iterations of  216700 the loss is 0.3094814022620254 theta= [[-7.79408992]\n",
      " [ 0.06817867]\n",
      " [ 0.06170568]]\n",
      "the iterations of  216800 the loss is 0.3094415710333686 theta= [[-7.79608568]\n",
      " [ 0.06819421]\n",
      " [ 0.06172147]]\n",
      "the iterations of  216900 the loss is 0.3094017672394749 theta= [[-7.79808077]\n",
      " [ 0.06820974]\n",
      " [ 0.06173725]]\n",
      "the iterations of  217000 the loss is 0.30936199085172816 theta= [[-7.80007516]\n",
      " [ 0.06822527]\n",
      " [ 0.06175303]]\n",
      "the iterations of  217100 the loss is 0.3093222418415527 theta= [[-7.80206887]\n",
      " [ 0.06824079]\n",
      " [ 0.0617688 ]]\n",
      "the iterations of  217200 the loss is 0.3092825201804121 theta= [[-7.80406189]\n",
      " [ 0.06825631]\n",
      " [ 0.06178456]]\n",
      "the iterations of  217300 the loss is 0.30924282583980955 theta= [[-7.80605422]\n",
      " [ 0.06827182]\n",
      " [ 0.06180033]]\n",
      "the iterations of  217400 the loss is 0.3092031587912879 theta= [[-7.80804588]\n",
      " [ 0.06828732]\n",
      " [ 0.06181608]]\n",
      "the iterations of  217500 the loss is 0.30916351900642913 theta= [[-7.81003684]\n",
      " [ 0.06830283]\n",
      " [ 0.06183183]]\n",
      "the iterations of  217600 the loss is 0.30912390645685517 theta= [[-7.81202712]\n",
      " [ 0.06831832]\n",
      " [ 0.06184758]]\n",
      "the iterations of  217700 the loss is 0.30908432111422696 theta= [[-7.81401672]\n",
      " [ 0.06833381]\n",
      " [ 0.06186332]]\n",
      "the iterations of  217800 the loss is 0.3090447629502444 theta= [[-7.81600564]\n",
      " [ 0.0683493 ]\n",
      " [ 0.06187905]]\n",
      "the iterations of  217900 the loss is 0.3090052319366472 theta= [[-7.81799387]\n",
      " [ 0.06836478]\n",
      " [ 0.06189478]]\n",
      "the iterations of  218000 the loss is 0.3089657280452138 theta= [[-7.81998142]\n",
      " [ 0.06838025]\n",
      " [ 0.0619105 ]]\n",
      "the iterations of  218100 the loss is 0.3089262512477616 theta= [[-7.82196829]\n",
      " [ 0.06839572]\n",
      " [ 0.06192622]]\n",
      "the iterations of  218200 the loss is 0.3088868015161473 theta= [[-7.82395448]\n",
      " [ 0.06841119]\n",
      " [ 0.06194193]]\n",
      "the iterations of  218300 the loss is 0.3088473788222664 theta= [[-7.82593999]\n",
      " [ 0.06842665]\n",
      " [ 0.06195764]]\n",
      "the iterations of  218400 the loss is 0.30880798313805324 theta= [[-7.82792481]\n",
      " [ 0.06844211]\n",
      " [ 0.06197334]]\n",
      "the iterations of  218500 the loss is 0.3087686144354807 theta= [[-7.82990896]\n",
      " [ 0.06845756]\n",
      " [ 0.06198904]]\n",
      "the iterations of  218600 the loss is 0.30872927268656086 theta= [[-7.83189243]\n",
      " [ 0.068473  ]\n",
      " [ 0.06200473]]\n",
      "the iterations of  218700 the loss is 0.30868995786334424 theta= [[-7.83387522]\n",
      " [ 0.06848844]\n",
      " [ 0.06202042]]\n",
      "the iterations of  218800 the loss is 0.30865066993791984 theta= [[-7.83585733]\n",
      " [ 0.06850388]\n",
      " [ 0.0620361 ]]\n",
      "the iterations of  218900 the loss is 0.30861140888241523 theta= [[-7.83783876]\n",
      " [ 0.06851931]\n",
      " [ 0.06205178]]\n",
      "the iterations of  219000 the loss is 0.3085721746689965 theta= [[-7.83981951]\n",
      " [ 0.06853473]\n",
      " [ 0.06206745]]\n",
      "the iterations of  219100 the loss is 0.3085329672698683 theta= [[-7.84179959]\n",
      " [ 0.06855015]\n",
      " [ 0.06208311]]\n",
      "the iterations of  219200 the loss is 0.30849378665727323 theta= [[-7.84377899]\n",
      " [ 0.06856556]\n",
      " [ 0.06209877]]\n",
      "the iterations of  219300 the loss is 0.3084546328034925 theta= [[-7.84575772]\n",
      " [ 0.06858097]\n",
      " [ 0.06211443]]\n",
      "the iterations of  219400 the loss is 0.30841550568084525 theta= [[-7.84773577]\n",
      " [ 0.06859638]\n",
      " [ 0.06213008]]\n",
      "the iterations of  219500 the loss is 0.30837640526168886 theta= [[-7.84971314]\n",
      " [ 0.06861178]\n",
      " [ 0.06214572]]\n",
      "the iterations of  219600 the loss is 0.30833733151841886 theta= [[-7.85168984]\n",
      " [ 0.06862717]\n",
      " [ 0.06216136]]\n",
      "the iterations of  219700 the loss is 0.3082982844234687 theta= [[-7.85366587]\n",
      " [ 0.06864256]\n",
      " [ 0.062177  ]]\n",
      "the iterations of  219800 the loss is 0.30825926394930975 theta= [[-7.85564122]\n",
      " [ 0.06865795]\n",
      " [ 0.06219262]]\n",
      "the iterations of  219900 the loss is 0.30822027006845143 theta= [[-7.8576159 ]\n",
      " [ 0.06867333]\n",
      " [ 0.06220825]]\n",
      "the iterations of  220000 the loss is 0.3081813027534406 theta= [[-7.8595899 ]\n",
      " [ 0.0686887 ]\n",
      " [ 0.06222387]]\n",
      "the iterations of  220100 the loss is 0.3081423619768621 theta= [[-7.86156324]\n",
      " [ 0.06870407]\n",
      " [ 0.06223948]]\n",
      "the iterations of  220200 the loss is 0.30810344771133874 theta= [[-7.8635359 ]\n",
      " [ 0.06871943]\n",
      " [ 0.06225509]]\n",
      "the iterations of  220300 the loss is 0.3080645599295303 theta= [[-7.86550789]\n",
      " [ 0.06873479]\n",
      " [ 0.06227069]]\n",
      "the iterations of  220400 the loss is 0.30802569860413465 theta= [[-7.86747921]\n",
      " [ 0.06875015]\n",
      " [ 0.06228629]]\n",
      "the iterations of  220500 the loss is 0.3079868637078867 theta= [[-7.86944986]\n",
      " [ 0.0687655 ]\n",
      " [ 0.06230188]]\n",
      "the iterations of  220600 the loss is 0.30794805521355934 theta= [[-7.87141984]\n",
      " [ 0.06878084]\n",
      " [ 0.06231747]]\n",
      "the iterations of  220700 the loss is 0.30790927309396243 theta= [[-7.87338915]\n",
      " [ 0.06879618]\n",
      " [ 0.06233305]]\n",
      "the iterations of  220800 the loss is 0.30787051732194326 theta= [[-7.87535779]\n",
      " [ 0.06881152]\n",
      " [ 0.06234862]]\n",
      "the iterations of  220900 the loss is 0.3078317878703861 theta= [[-7.87732576]\n",
      " [ 0.06882685]\n",
      " [ 0.0623642 ]]\n",
      "the iterations of  221000 the loss is 0.307793084712213 theta= [[-7.87929306]\n",
      " [ 0.06884217]\n",
      " [ 0.06237976]]\n",
      "the iterations of  221100 the loss is 0.3077544078203823 theta= [[-7.8812597 ]\n",
      " [ 0.06885749]\n",
      " [ 0.06239532]]\n",
      "the iterations of  221200 the loss is 0.30771575716789024 theta= [[-7.88322567]\n",
      " [ 0.0688728 ]\n",
      " [ 0.06241088]]\n",
      "the iterations of  221300 the loss is 0.30767713272776925 theta= [[-7.88519097]\n",
      " [ 0.06888811]\n",
      " [ 0.06242643]]\n",
      "the iterations of  221400 the loss is 0.30763853447308936 theta= [[-7.8871556 ]\n",
      " [ 0.06890342]\n",
      " [ 0.06244197]]\n",
      "the iterations of  221500 the loss is 0.30759996237695697 theta= [[-7.88911957]\n",
      " [ 0.06891872]\n",
      " [ 0.06245751]]\n",
      "the iterations of  221600 the loss is 0.30756141641251544 theta= [[-7.89108288]\n",
      " [ 0.06893401]\n",
      " [ 0.06247305]]\n",
      "the iterations of  221700 the loss is 0.30752289655294507 theta= [[-7.89304552]\n",
      " [ 0.0689493 ]\n",
      " [ 0.06248858]]\n",
      "the iterations of  221800 the loss is 0.3074844027714624 theta= [[-7.89500749]\n",
      " [ 0.06896459]\n",
      " [ 0.0625041 ]]\n",
      "the iterations of  221900 the loss is 0.307445935041321 theta= [[-7.8969688 ]\n",
      " [ 0.06897986]\n",
      " [ 0.06251962]]\n",
      "the iterations of  222000 the loss is 0.3074074933358107 theta= [[-7.89892945]\n",
      " [ 0.06899514]\n",
      " [ 0.06253514]]\n",
      "the iterations of  222100 the loss is 0.307369077628258 theta= [[-7.90088943]\n",
      " [ 0.06901041]\n",
      " [ 0.06255065]]\n",
      "the iterations of  222200 the loss is 0.3073306878920258 theta= [[-7.90284875]\n",
      " [ 0.06902567]\n",
      " [ 0.06256615]]\n",
      "the iterations of  222300 the loss is 0.3072923241005132 theta= [[-7.90480741]\n",
      " [ 0.06904093]\n",
      " [ 0.06258165]]\n",
      "the iterations of  222400 the loss is 0.30725398622715583 theta= [[-7.90676541]\n",
      " [ 0.06905619]\n",
      " [ 0.06259714]]\n",
      "the iterations of  222500 the loss is 0.30721567424542523 theta= [[-7.90872275]\n",
      " [ 0.06907144]\n",
      " [ 0.06261263]]\n",
      "the iterations of  222600 the loss is 0.3071773881288297 theta= [[-7.91067942]\n",
      " [ 0.06908668]\n",
      " [ 0.06262811]]\n",
      "the iterations of  222700 the loss is 0.30713912785091313 theta= [[-7.91263544]\n",
      " [ 0.06910192]\n",
      " [ 0.06264359]]\n",
      "the iterations of  222800 the loss is 0.30710089338525554 theta= [[-7.91459079]\n",
      " [ 0.06911716]\n",
      " [ 0.06265906]]\n",
      "the iterations of  222900 the loss is 0.3070626847054732 theta= [[-7.91654549]\n",
      " [ 0.06913239]\n",
      " [ 0.06267453]]\n",
      "the iterations of  223000 the loss is 0.307024501785218 theta= [[-7.91849952]\n",
      " [ 0.06914761]\n",
      " [ 0.06269   ]]\n",
      "the iterations of  223100 the loss is 0.306986344598178 theta= [[-7.9204529 ]\n",
      " [ 0.06916283]\n",
      " [ 0.06270545]]\n",
      "the iterations of  223200 the loss is 0.30694821311807685 theta= [[-7.92240562]\n",
      " [ 0.06917805]\n",
      " [ 0.06272091]]\n",
      "the iterations of  223300 the loss is 0.30691010731867413 theta= [[-7.92435768]\n",
      " [ 0.06919326]\n",
      " [ 0.06273635]]\n",
      "the iterations of  223400 the loss is 0.3068720271737649 theta= [[-7.92630909]\n",
      " [ 0.06920846]\n",
      " [ 0.0627518 ]]\n",
      "the iterations of  223500 the loss is 0.3068339726571801 theta= [[-7.92825984]\n",
      " [ 0.06922367]\n",
      " [ 0.06276723]]\n",
      "the iterations of  223600 the loss is 0.306795943742786 theta= [[-7.93020993]\n",
      " [ 0.06923886]\n",
      " [ 0.06278266]]\n",
      "the iterations of  223700 the loss is 0.30675794040448456 theta= [[-7.93215936]\n",
      " [ 0.06925405]\n",
      " [ 0.06279809]]\n",
      "the iterations of  223800 the loss is 0.30671996261621337 theta= [[-7.93410814]\n",
      " [ 0.06926924]\n",
      " [ 0.06281351]]\n",
      "the iterations of  223900 the loss is 0.3066820103519449 theta= [[-7.93605627]\n",
      " [ 0.06928442]\n",
      " [ 0.06282893]]\n",
      "the iterations of  224000 the loss is 0.3066440835856875 theta= [[-7.93800374]\n",
      " [ 0.06929959]\n",
      " [ 0.06284434]]\n",
      "the iterations of  224100 the loss is 0.30660618229148434 theta= [[-7.93995056]\n",
      " [ 0.06931477]\n",
      " [ 0.06285975]]\n",
      "the iterations of  224200 the loss is 0.3065683064434142 theta= [[-7.94189672]\n",
      " [ 0.06932993]\n",
      " [ 0.06287515]]\n",
      "the iterations of  224300 the loss is 0.3065304560155907 theta= [[-7.94384223]\n",
      " [ 0.06934509]\n",
      " [ 0.06289055]]\n",
      "the iterations of  224400 the loss is 0.3064926309821628 theta= [[-7.94578709]\n",
      " [ 0.06936025]\n",
      " [ 0.06290594]]\n",
      "the iterations of  224500 the loss is 0.30645483131731394 theta= [[-7.94773129]\n",
      " [ 0.0693754 ]\n",
      " [ 0.06292132]]\n",
      "the iterations of  224600 the loss is 0.30641705699526356 theta= [[-7.94967485]\n",
      " [ 0.06939055]\n",
      " [ 0.06293671]]\n",
      "the iterations of  224700 the loss is 0.3063793079902656 theta= [[-7.95161775]\n",
      " [ 0.06940569]\n",
      " [ 0.06295208]]\n",
      "the iterations of  224800 the loss is 0.30634158427660835 theta= [[-7.95356   ]\n",
      " [ 0.06942083]\n",
      " [ 0.06296745]]\n",
      "the iterations of  224900 the loss is 0.30630388582861523 theta= [[-7.9555016 ]\n",
      " [ 0.06943596]\n",
      " [ 0.06298282]]\n",
      "the iterations of  225000 the loss is 0.30626621262064474 theta= [[-7.95744255]\n",
      " [ 0.06945109]\n",
      " [ 0.06299818]]\n",
      "the iterations of  225100 the loss is 0.3062285646270896 theta= [[-7.95938285]\n",
      " [ 0.06946621]\n",
      " [ 0.06301354]]\n",
      "the iterations of  225200 the loss is 0.3061909418223774 theta= [[-7.9613225 ]\n",
      " [ 0.06948133]\n",
      " [ 0.06302889]]\n",
      "the iterations of  225300 the loss is 0.30615334418097034 theta= [[-7.9632615 ]\n",
      " [ 0.06949644]\n",
      " [ 0.06304423]]\n",
      "the iterations of  225400 the loss is 0.306115771677365 theta= [[-7.96519986]\n",
      " [ 0.06951155]\n",
      " [ 0.06305957]]\n",
      "the iterations of  225500 the loss is 0.3060782242860924 theta= [[-7.96713756]\n",
      " [ 0.06952665]\n",
      " [ 0.06307491]]\n",
      "the iterations of  225600 the loss is 0.30604070198171807 theta= [[-7.96907462]\n",
      " [ 0.06954175]\n",
      " [ 0.06309024]]\n",
      "the iterations of  225700 the loss is 0.3060032047388419 theta= [[-7.97101103]\n",
      " [ 0.06955684]\n",
      " [ 0.06310556]]\n",
      "the iterations of  225800 the loss is 0.305965732532098 theta= [[-7.9729468 ]\n",
      " [ 0.06957193]\n",
      " [ 0.06312088]]\n",
      "the iterations of  225900 the loss is 0.3059282853361548 theta= [[-7.97488191]\n",
      " [ 0.06958701]\n",
      " [ 0.0631362 ]]\n",
      "the iterations of  226000 the loss is 0.30589086312571473 theta= [[-7.97681639]\n",
      " [ 0.06960209]\n",
      " [ 0.06315151]]\n",
      "the iterations of  226100 the loss is 0.3058534658755143 theta= [[-7.97875022]\n",
      " [ 0.06961717]\n",
      " [ 0.06316682]]\n",
      "the iterations of  226200 the loss is 0.3058160935603246 theta= [[-7.9806834 ]\n",
      " [ 0.06963224]\n",
      " [ 0.06318212]]\n",
      "the iterations of  226300 the loss is 0.30577874615494993 theta= [[-7.98261594]\n",
      " [ 0.0696473 ]\n",
      " [ 0.06319741]]\n",
      "the iterations of  226400 the loss is 0.30574142363422924 theta= [[-7.98454783]\n",
      " [ 0.06966236]\n",
      " [ 0.0632127 ]]\n",
      "the iterations of  226500 the loss is 0.30570412597303476 theta= [[-7.98647908]\n",
      " [ 0.06967742]\n",
      " [ 0.06322799]]\n",
      "the iterations of  226600 the loss is 0.30566685314627295 theta= [[-7.98840969]\n",
      " [ 0.06969247]\n",
      " [ 0.06324327]]\n",
      "the iterations of  226700 the loss is 0.305629605128884 theta= [[-7.99033966]\n",
      " [ 0.06970751]\n",
      " [ 0.06325854]]\n",
      "the iterations of  226800 the loss is 0.3055923818958418 theta= [[-7.99226898]\n",
      " [ 0.06972255]\n",
      " [ 0.06327381]]\n",
      "the iterations of  226900 the loss is 0.3055551834221538 theta= [[-7.99419766]\n",
      " [ 0.06973759]\n",
      " [ 0.06328908]]\n",
      "the iterations of  227000 the loss is 0.30551800968286136 theta= [[-7.9961257 ]\n",
      " [ 0.06975262]\n",
      " [ 0.06330434]]\n",
      "the iterations of  227100 the loss is 0.3054808606530387 theta= [[-7.9980531 ]\n",
      " [ 0.06976764]\n",
      " [ 0.0633196 ]]\n",
      "the iterations of  227200 the loss is 0.30544373630779437 theta= [[-7.99997986]\n",
      " [ 0.06978266]\n",
      " [ 0.06333485]]\n",
      "the iterations of  227300 the loss is 0.3054066366222697 theta= [[-8.00190598]\n",
      " [ 0.06979768]\n",
      " [ 0.06335009]]\n",
      "the iterations of  227400 the loss is 0.30536956157163986 theta= [[-8.00383146]\n",
      " [ 0.06981269]\n",
      " [ 0.06336533]]\n",
      "the iterations of  227500 the loss is 0.3053325111311132 theta= [[-8.0057563 ]\n",
      " [ 0.0698277 ]\n",
      " [ 0.06338057]]\n",
      "the iterations of  227600 the loss is 0.30529548527593126 theta= [[-8.0076805]\n",
      " [ 0.0698427]\n",
      " [ 0.0633958]]\n",
      "the iterations of  227700 the loss is 0.30525848398136907 theta= [[-8.00960406]\n",
      " [ 0.0698577 ]\n",
      " [ 0.06341102]]\n",
      "the iterations of  227800 the loss is 0.3052215072227342 theta= [[-8.01152699]\n",
      " [ 0.06987269]\n",
      " [ 0.06342624]]\n",
      "the iterations of  227900 the loss is 0.3051845549753678 theta= [[-8.01344927]\n",
      " [ 0.06988768]\n",
      " [ 0.06344146]]\n",
      "the iterations of  228000 the loss is 0.30514762721464406 theta= [[-8.01537092]\n",
      " [ 0.06990266]\n",
      " [ 0.06345667]]\n",
      "the iterations of  228100 the loss is 0.3051107239159703 theta= [[-8.01729194]\n",
      " [ 0.06991764]\n",
      " [ 0.06347188]]\n",
      "the iterations of  228200 the loss is 0.30507384505478635 theta= [[-8.01921232]\n",
      " [ 0.06993261]\n",
      " [ 0.06348708]]\n",
      "the iterations of  228300 the loss is 0.3050369906065653 theta= [[-8.02113206]\n",
      " [ 0.06994758]\n",
      " [ 0.06350227]]\n",
      "the iterations of  228400 the loss is 0.30500016054681295 theta= [[-8.02305117]\n",
      " [ 0.06996254]\n",
      " [ 0.06351746]]\n",
      "the iterations of  228500 the loss is 0.30496335485106807 theta= [[-8.02496964]\n",
      " [ 0.0699775 ]\n",
      " [ 0.06353265]]\n",
      "the iterations of  228600 the loss is 0.30492657349490143 theta= [[-8.02688747]\n",
      " [ 0.06999246]\n",
      " [ 0.06354783]]\n",
      "the iterations of  228700 the loss is 0.3048898164539173 theta= [[-8.02880468]\n",
      " [ 0.07000741]\n",
      " [ 0.06356301]]\n",
      "the iterations of  228800 the loss is 0.30485308370375225 theta= [[-8.03072125]\n",
      " [ 0.07002235]\n",
      " [ 0.06357818]]\n",
      "the iterations of  228900 the loss is 0.30481637522007554 theta= [[-8.03263718]\n",
      " [ 0.07003729]\n",
      " [ 0.06359334]]\n",
      "the iterations of  229000 the loss is 0.3047796909785887 theta= [[-8.03455249]\n",
      " [ 0.07005223]\n",
      " [ 0.06360851]]\n",
      "the iterations of  229100 the loss is 0.304743030955026 theta= [[-8.03646716]\n",
      " [ 0.07006716]\n",
      " [ 0.06362366]]\n",
      "the iterations of  229200 the loss is 0.30470639512515363 theta= [[-8.0383812 ]\n",
      " [ 0.07008208]\n",
      " [ 0.06363881]]\n",
      "the iterations of  229300 the loss is 0.3046697834647706 theta= [[-8.04029461]\n",
      " [ 0.070097  ]\n",
      " [ 0.06365396]]\n",
      "the iterations of  229400 the loss is 0.3046331959497081 theta= [[-8.04220738]\n",
      " [ 0.07011192]\n",
      " [ 0.0636691 ]]\n",
      "the iterations of  229500 the loss is 0.3045966325558296 theta= [[-8.04411953]\n",
      " [ 0.07012683]\n",
      " [ 0.06368424]]\n",
      "the iterations of  229600 the loss is 0.3045600932590305 theta= [[-8.04603105]\n",
      " [ 0.07014174]\n",
      " [ 0.06369937]]\n",
      "the iterations of  229700 the loss is 0.30452357803523883 theta= [[-8.04794193]\n",
      " [ 0.07015664]\n",
      " [ 0.0637145 ]]\n",
      "the iterations of  229800 the loss is 0.3044870868604142 theta= [[-8.04985219]\n",
      " [ 0.07017154]\n",
      " [ 0.06372962]]\n",
      "the iterations of  229900 the loss is 0.3044506197105483 theta= [[-8.05176182]\n",
      " [ 0.07018643]\n",
      " [ 0.06374474]]\n",
      "the iterations of  230000 the loss is 0.3044141765616652 theta= [[-8.05367082]\n",
      " [ 0.07020132]\n",
      " [ 0.06375985]]\n",
      "the iterations of  230100 the loss is 0.30437775738982026 theta= [[-8.05557919]\n",
      " [ 0.0702162 ]\n",
      " [ 0.06377496]]\n",
      "the iterations of  230200 the loss is 0.3043413621711012 theta= [[-8.05748693]\n",
      " [ 0.07023108]\n",
      " [ 0.06379006]]\n",
      "the iterations of  230300 the loss is 0.30430499088162766 theta= [[-8.05939405]\n",
      " [ 0.07024596]\n",
      " [ 0.06380516]]\n",
      "the iterations of  230400 the loss is 0.3042686434975506 theta= [[-8.06130054]\n",
      " [ 0.07026083]\n",
      " [ 0.06382025]]\n",
      "the iterations of  230500 the loss is 0.3042323199950531 theta= [[-8.0632064 ]\n",
      " [ 0.07027569]\n",
      " [ 0.06383534]]\n",
      "the iterations of  230600 the loss is 0.30419602035034954 theta= [[-8.06511164]\n",
      " [ 0.07029055]\n",
      " [ 0.06385042]]\n",
      "the iterations of  230700 the loss is 0.30415974453968625 theta= [[-8.06701625]\n",
      " [ 0.0703054 ]\n",
      " [ 0.0638655 ]]\n",
      "the iterations of  230800 the loss is 0.3041234925393408 theta= [[-8.06892023]\n",
      " [ 0.07032026]\n",
      " [ 0.06388057]]\n",
      "the iterations of  230900 the loss is 0.30408726432562244 theta= [[-8.0708236 ]\n",
      " [ 0.0703351 ]\n",
      " [ 0.06389564]]\n",
      "the iterations of  231000 the loss is 0.3040510598748716 theta= [[-8.07272633]\n",
      " [ 0.07034994]\n",
      " [ 0.0639107 ]]\n",
      "the iterations of  231100 the loss is 0.30401487916346076 theta= [[-8.07462845]\n",
      " [ 0.07036478]\n",
      " [ 0.06392576]]\n",
      "the iterations of  231200 the loss is 0.3039787221677933 theta= [[-8.07652994]\n",
      " [ 0.07037961]\n",
      " [ 0.06394081]]\n",
      "the iterations of  231300 the loss is 0.30394258886430386 theta= [[-8.0784308 ]\n",
      " [ 0.07039444]\n",
      " [ 0.06395586]]\n",
      "the iterations of  231400 the loss is 0.3039064792294582 theta= [[-8.08033105]\n",
      " [ 0.07040926]\n",
      " [ 0.06397091]]\n",
      "the iterations of  231500 the loss is 0.3038703932397537 theta= [[-8.08223067]\n",
      " [ 0.07042408]\n",
      " [ 0.06398595]]\n",
      "the iterations of  231600 the loss is 0.30383433087171857 theta= [[-8.08412967]\n",
      " [ 0.07043889]\n",
      " [ 0.06400098]]\n",
      "the iterations of  231700 the loss is 0.3037982921019121 theta= [[-8.08602805]\n",
      " [ 0.0704537 ]\n",
      " [ 0.06401601]]\n",
      "the iterations of  231800 the loss is 0.30376227690692525 theta= [[-8.0879258 ]\n",
      " [ 0.0704685 ]\n",
      " [ 0.06403104]]\n",
      "the iterations of  231900 the loss is 0.30372628526337875 theta= [[-8.08982294]\n",
      " [ 0.0704833 ]\n",
      " [ 0.06404606]]\n",
      "the iterations of  232000 the loss is 0.3036903171479256 theta= [[-8.09171946]\n",
      " [ 0.0704981 ]\n",
      " [ 0.06406107]]\n",
      "the iterations of  232100 the loss is 0.30365437253724864 theta= [[-8.09361535]\n",
      " [ 0.07051289]\n",
      " [ 0.06407608]]\n",
      "the iterations of  232200 the loss is 0.30361845140806204 theta= [[-8.09551063]\n",
      " [ 0.07052767]\n",
      " [ 0.06409109]]\n",
      "the iterations of  232300 the loss is 0.30358255373711107 theta= [[-8.09740529]\n",
      " [ 0.07054245]\n",
      " [ 0.06410609]]\n",
      "the iterations of  232400 the loss is 0.3035466795011709 theta= [[-8.09929933]\n",
      " [ 0.07055723]\n",
      " [ 0.06412108]]\n",
      "the iterations of  232500 the loss is 0.3035108286770481 theta= [[-8.10119275]\n",
      " [ 0.070572  ]\n",
      " [ 0.06413607]]\n",
      "the iterations of  232600 the loss is 0.30347500124157933 theta= [[-8.10308555]\n",
      " [ 0.07058677]\n",
      " [ 0.06415106]]\n",
      "the iterations of  232700 the loss is 0.30343919717163254 theta= [[-8.10497774]\n",
      " [ 0.07060153]\n",
      " [ 0.06416604]]\n",
      "the iterations of  232800 the loss is 0.3034034164441057 theta= [[-8.10686931]\n",
      " [ 0.07061629]\n",
      " [ 0.06418102]]\n",
      "the iterations of  232900 the loss is 0.3033676590359271 theta= [[-8.10876026]\n",
      " [ 0.07063104]\n",
      " [ 0.06419599]]\n",
      "the iterations of  233000 the loss is 0.30333192492405614 theta= [[-8.1106506 ]\n",
      " [ 0.07064579]\n",
      " [ 0.06421096]]\n",
      "the iterations of  233100 the loss is 0.3032962140854819 theta= [[-8.11254032]\n",
      " [ 0.07066053]\n",
      " [ 0.06422592]]\n",
      "the iterations of  233200 the loss is 0.30326052649722435 theta= [[-8.11442943]\n",
      " [ 0.07067527]\n",
      " [ 0.06424088]]\n",
      "the iterations of  233300 the loss is 0.3032248621363334 theta= [[-8.11631792]\n",
      " [ 0.07069   ]\n",
      " [ 0.06425583]]\n",
      "the iterations of  233400 the loss is 0.30318922097988915 theta= [[-8.1182058 ]\n",
      " [ 0.07070473]\n",
      " [ 0.06427078]]\n",
      "the iterations of  233500 the loss is 0.30315360300500244 theta= [[-8.12009306]\n",
      " [ 0.07071946]\n",
      " [ 0.06428572]]\n",
      "the iterations of  233600 the loss is 0.3031180081888134 theta= [[-8.12197971]\n",
      " [ 0.07073418]\n",
      " [ 0.06430066]]\n",
      "the iterations of  233700 the loss is 0.30308243650849304 theta= [[-8.12386574]\n",
      " [ 0.07074889]\n",
      " [ 0.06431559]]\n",
      "the iterations of  233800 the loss is 0.30304688794124196 theta= [[-8.12575117]\n",
      " [ 0.07076361]\n",
      " [ 0.06433052]]\n",
      "the iterations of  233900 the loss is 0.30301136246429083 theta= [[-8.12763598]\n",
      " [ 0.07077831]\n",
      " [ 0.06434544]]\n",
      "the iterations of  234000 the loss is 0.30297586005490046 theta= [[-8.12952017]\n",
      " [ 0.07079301]\n",
      " [ 0.06436036]]\n",
      "the iterations of  234100 the loss is 0.3029403806903611 theta= [[-8.13140376]\n",
      " [ 0.07080771]\n",
      " [ 0.06437528]]\n",
      "the iterations of  234200 the loss is 0.3029049243479934 theta= [[-8.13328674]\n",
      " [ 0.0708224 ]\n",
      " [ 0.06439019]]\n",
      "the iterations of  234300 the loss is 0.3028694910051474 theta= [[-8.1351691 ]\n",
      " [ 0.07083709]\n",
      " [ 0.06440509]]\n",
      "the iterations of  234400 the loss is 0.30283408063920336 theta= [[-8.13705085]\n",
      " [ 0.07085178]\n",
      " [ 0.06441999]]\n",
      "the iterations of  234500 the loss is 0.3027986932275706 theta= [[-8.138932  ]\n",
      " [ 0.07086646]\n",
      " [ 0.06443489]]\n",
      "the iterations of  234600 the loss is 0.3027633287476886 theta= [[-8.14081253]\n",
      " [ 0.07088113]\n",
      " [ 0.06444978]]\n",
      "the iterations of  234700 the loss is 0.30272798717702615 theta= [[-8.14269246]\n",
      " [ 0.0708958 ]\n",
      " [ 0.06446466]]\n",
      "the iterations of  234800 the loss is 0.3026926684930821 theta= [[-8.14457178]\n",
      " [ 0.07091047]\n",
      " [ 0.06447954]]\n",
      "the iterations of  234900 the loss is 0.30265737267338394 theta= [[-8.14645048]\n",
      " [ 0.07092513]\n",
      " [ 0.06449442]]\n",
      "the iterations of  235000 the loss is 0.30262209969548926 theta= [[-8.14832858]\n",
      " [ 0.07093978]\n",
      " [ 0.06450929]]\n",
      "the iterations of  235100 the loss is 0.3025868495369849 theta= [[-8.15020608]\n",
      " [ 0.07095443]\n",
      " [ 0.06452416]]\n",
      "the iterations of  235200 the loss is 0.3025516221754872 theta= [[-8.15208296]\n",
      " [ 0.07096908]\n",
      " [ 0.06453902]]\n",
      "the iterations of  235300 the loss is 0.3025164175886415 theta= [[-8.15395924]\n",
      " [ 0.07098372]\n",
      " [ 0.06455388]]\n",
      "the iterations of  235400 the loss is 0.30248123575412295 theta= [[-8.15583491]\n",
      " [ 0.07099836]\n",
      " [ 0.06456873]]\n",
      "the iterations of  235500 the loss is 0.30244607664963513 theta= [[-8.15770998]\n",
      " [ 0.071013  ]\n",
      " [ 0.06458358]]\n",
      "the iterations of  235600 the loss is 0.3024109402529114 theta= [[-8.15958444]\n",
      " [ 0.07102762]\n",
      " [ 0.06459842]]\n",
      "the iterations of  235700 the loss is 0.302375826541714 theta= [[-8.16145829]\n",
      " [ 0.07104225]\n",
      " [ 0.06461326]]\n",
      "the iterations of  235800 the loss is 0.30234073549383444 theta= [[-8.16333154]\n",
      " [ 0.07105687]\n",
      " [ 0.0646281 ]]\n",
      "the iterations of  235900 the loss is 0.30230566708709317 theta= [[-8.16520419]\n",
      " [ 0.07107148]\n",
      " [ 0.06464293]]\n",
      "the iterations of  236000 the loss is 0.30227062129933946 theta= [[-8.16707623]\n",
      " [ 0.0710861 ]\n",
      " [ 0.06465775]]\n",
      "the iterations of  236100 the loss is 0.30223559810845196 theta= [[-8.16894767]\n",
      " [ 0.0711007 ]\n",
      " [ 0.06467257]]\n",
      "the iterations of  236200 the loss is 0.30220059749233763 theta= [[-8.1708185 ]\n",
      " [ 0.0711153 ]\n",
      " [ 0.06468738]]\n",
      "the iterations of  236300 the loss is 0.30216561942893283 theta= [[-8.17268874]\n",
      " [ 0.0711299 ]\n",
      " [ 0.06470219]]\n",
      "the iterations of  236400 the loss is 0.30213066389620236 theta= [[-8.17455837]\n",
      " [ 0.07114449]\n",
      " [ 0.064717  ]]\n",
      "the iterations of  236500 the loss is 0.30209573087213987 theta= [[-8.17642739]\n",
      " [ 0.07115908]\n",
      " [ 0.0647318 ]]\n",
      "the iterations of  236600 the loss is 0.30206082033476755 theta= [[-8.17829582]\n",
      " [ 0.07117367]\n",
      " [ 0.0647466 ]]\n",
      "the iterations of  236700 the loss is 0.3020259322621367 theta= [[-8.18016364]\n",
      " [ 0.07118824]\n",
      " [ 0.06476139]]\n",
      "the iterations of  236800 the loss is 0.3019910666323269 theta= [[-8.18203087]\n",
      " [ 0.07120282]\n",
      " [ 0.06477618]]\n",
      "the iterations of  236900 the loss is 0.30195622342344597 theta= [[-8.18389749]\n",
      " [ 0.07121739]\n",
      " [ 0.06479096]]\n",
      "the iterations of  237000 the loss is 0.3019214026136311 theta= [[-8.18576351]\n",
      " [ 0.07123195]\n",
      " [ 0.06480574]]\n",
      "the iterations of  237100 the loss is 0.3018866041810474 theta= [[-8.18762894]\n",
      " [ 0.07124652]\n",
      " [ 0.06482051]]\n",
      "the iterations of  237200 the loss is 0.3018518281038882 theta= [[-8.18949376]\n",
      " [ 0.07126107]\n",
      " [ 0.06483528]]\n",
      "the iterations of  237300 the loss is 0.30181707436037575 theta= [[-8.19135799]\n",
      " [ 0.07127562]\n",
      " [ 0.06485004]]\n",
      "the iterations of  237400 the loss is 0.3017823429287601 theta= [[-8.19322161]\n",
      " [ 0.07129017]\n",
      " [ 0.0648648 ]]\n",
      "the iterations of  237500 the loss is 0.3017476337873201 theta= [[-8.19508464]\n",
      " [ 0.07130472]\n",
      " [ 0.06487956]]\n",
      "the iterations of  237600 the loss is 0.3017129469143628 theta= [[-8.19694707]\n",
      " [ 0.07131925]\n",
      " [ 0.0648943 ]]\n",
      "the iterations of  237700 the loss is 0.3016782822882231 theta= [[-8.19880891]\n",
      " [ 0.07133379]\n",
      " [ 0.06490905]]\n",
      "the iterations of  237800 the loss is 0.30164363988726406 theta= [[-8.20067014]\n",
      " [ 0.07134832]\n",
      " [ 0.06492379]]\n",
      "the iterations of  237900 the loss is 0.3016090196898771 theta= [[-8.20253078]\n",
      " [ 0.07136284]\n",
      " [ 0.06493853]]\n",
      "the iterations of  238000 the loss is 0.3015744216744818 theta= [[-8.20439083]\n",
      " [ 0.07137736]\n",
      " [ 0.06495326]]\n",
      "the iterations of  238100 the loss is 0.3015398458195254 theta= [[-8.20625028]\n",
      " [ 0.07139188]\n",
      " [ 0.06496798]]\n",
      "the iterations of  238200 the loss is 0.30150529210348376 theta= [[-8.20810913]\n",
      " [ 0.07140639]\n",
      " [ 0.06498271]]\n",
      "the iterations of  238300 the loss is 0.3014707605048594 theta= [[-8.20996739]\n",
      " [ 0.0714209 ]\n",
      " [ 0.06499742]]\n",
      "the iterations of  238400 the loss is 0.30143625100218396 theta= [[-8.21182505]\n",
      " [ 0.0714354 ]\n",
      " [ 0.06501214]]\n",
      "the iterations of  238500 the loss is 0.3014017635740166 theta= [[-8.21368212]\n",
      " [ 0.0714499 ]\n",
      " [ 0.06502684]]\n",
      "the iterations of  238600 the loss is 0.3013672981989442 theta= [[-8.21553859]\n",
      " [ 0.07146439]\n",
      " [ 0.06504155]]\n",
      "the iterations of  238700 the loss is 0.30133285485558114 theta= [[-8.21739447]\n",
      " [ 0.07147888]\n",
      " [ 0.06505625]]\n",
      "the iterations of  238800 the loss is 0.30129843352256974 theta= [[-8.21924976]\n",
      " [ 0.07149337]\n",
      " [ 0.06507094]]\n",
      "the iterations of  238900 the loss is 0.30126403417858 theta= [[-8.22110446]\n",
      " [ 0.07150785]\n",
      " [ 0.06508563]]\n",
      "the iterations of  239000 the loss is 0.3012296568023093 theta= [[-8.22295856]\n",
      " [ 0.07152232]\n",
      " [ 0.06510031]]\n",
      "the iterations of  239100 the loss is 0.3011953013724828 theta= [[-8.22481207]\n",
      " [ 0.0715368 ]\n",
      " [ 0.06511499]]\n",
      "the iterations of  239200 the loss is 0.30116096786785346 theta= [[-8.22666499]\n",
      " [ 0.07155126]\n",
      " [ 0.06512967]]\n",
      "the iterations of  239300 the loss is 0.30112665626720136 theta= [[-8.22851732]\n",
      " [ 0.07156573]\n",
      " [ 0.06514434]]\n",
      "the iterations of  239400 the loss is 0.3010923665493339 theta= [[-8.23036906]\n",
      " [ 0.07158018]\n",
      " [ 0.06515901]]\n",
      "the iterations of  239500 the loss is 0.3010580986930862 theta= [[-8.2322202 ]\n",
      " [ 0.07159464]\n",
      " [ 0.06517367]]\n",
      "the iterations of  239600 the loss is 0.3010238526773204 theta= [[-8.23407076]\n",
      " [ 0.07160909]\n",
      " [ 0.06518833]]\n",
      "the iterations of  239700 the loss is 0.3009896284809266 theta= [[-8.23592073]\n",
      " [ 0.07162353]\n",
      " [ 0.06520298]]\n",
      "the iterations of  239800 the loss is 0.3009554260828214 theta= [[-8.23777011]\n",
      " [ 0.07163797]\n",
      " [ 0.06521763]]\n",
      "the iterations of  239900 the loss is 0.30092124546194865 theta= [[-8.2396189 ]\n",
      " [ 0.07165241]\n",
      " [ 0.06523227]]\n",
      "the iterations of  240000 the loss is 0.3008870865972801 theta= [[-8.2414671 ]\n",
      " [ 0.07166684]\n",
      " [ 0.06524691]]\n",
      "the iterations of  240100 the loss is 0.3008529494678141 theta= [[-8.24331471]\n",
      " [ 0.07168127]\n",
      " [ 0.06526154]]\n",
      "the iterations of  240200 the loss is 0.3008188340525761 theta= [[-8.24516174]\n",
      " [ 0.07169569]\n",
      " [ 0.06527617]]\n",
      "the iterations of  240300 the loss is 0.3007847403306188 theta= [[-8.24700817]\n",
      " [ 0.07171011]\n",
      " [ 0.0652908 ]]\n",
      "the iterations of  240400 the loss is 0.30075066828102126 theta= [[-8.24885403]\n",
      " [ 0.07172452]\n",
      " [ 0.06530542]]\n",
      "the iterations of  240500 the loss is 0.3007166178828905 theta= [[-8.25069929]\n",
      " [ 0.07173893]\n",
      " [ 0.06532004]]\n",
      "the iterations of  240600 the loss is 0.3006825891153601 theta= [[-8.25254397]\n",
      " [ 0.07175334]\n",
      " [ 0.06533465]]\n",
      "the iterations of  240700 the loss is 0.30064858195759003 theta= [[-8.25438806]\n",
      " [ 0.07176774]\n",
      " [ 0.06534925]]\n",
      "the iterations of  240800 the loss is 0.3006145963887675 theta= [[-8.25623157]\n",
      " [ 0.07178214]\n",
      " [ 0.06536386]]\n",
      "the iterations of  240900 the loss is 0.30058063238810656 theta= [[-8.25807449]\n",
      " [ 0.07179653]\n",
      " [ 0.06537845]]\n",
      "the iterations of  241000 the loss is 0.3005466899348479 theta= [[-8.25991683]\n",
      " [ 0.07181091]\n",
      " [ 0.06539305]]\n",
      "the iterations of  241100 the loss is 0.300512769008259 theta= [[-8.26175858]\n",
      " [ 0.0718253 ]\n",
      " [ 0.06540764]]\n",
      "the iterations of  241200 the loss is 0.30047886958763353 theta= [[-8.26359975]\n",
      " [ 0.07183968]\n",
      " [ 0.06542222]]\n",
      "the iterations of  241300 the loss is 0.30044499165229227 theta= [[-8.26544033]\n",
      " [ 0.07185405]\n",
      " [ 0.0654368 ]]\n",
      "the iterations of  241400 the loss is 0.30041113518158286 theta= [[-8.26728033]\n",
      " [ 0.07186842]\n",
      " [ 0.06545137]]\n",
      "the iterations of  241500 the loss is 0.30037730015487873 theta= [[-8.26911975]\n",
      " [ 0.07188279]\n",
      " [ 0.06546595]]\n",
      "the iterations of  241600 the loss is 0.3003434865515802 theta= [[-8.27095859]\n",
      " [ 0.07189715]\n",
      " [ 0.06548051]]\n",
      "the iterations of  241700 the loss is 0.3003096943511139 theta= [[-8.27279684]\n",
      " [ 0.07191151]\n",
      " [ 0.06549507]]\n",
      "the iterations of  241800 the loss is 0.3002759235329331 theta= [[-8.27463452]\n",
      " [ 0.07192586]\n",
      " [ 0.06550963]]\n",
      "the iterations of  241900 the loss is 0.30024217407651727 theta= [[-8.27647161]\n",
      " [ 0.07194021]\n",
      " [ 0.06552418]]\n",
      "the iterations of  242000 the loss is 0.3002084459613721 theta= [[-8.27830812]\n",
      " [ 0.07195455]\n",
      " [ 0.06553873]]\n",
      "the iterations of  242100 the loss is 0.30017473916702964 theta= [[-8.28014405]\n",
      " [ 0.07196889]\n",
      " [ 0.06555327]]\n",
      "the iterations of  242200 the loss is 0.3001410536730482 theta= [[-8.2819794 ]\n",
      " [ 0.07198322]\n",
      " [ 0.06556781]]\n",
      "the iterations of  242300 the loss is 0.30010738945901266 theta= [[-8.28381417]\n",
      " [ 0.07199755]\n",
      " [ 0.06558235]]\n",
      "the iterations of  242400 the loss is 0.3000737465045333 theta= [[-8.28564836]\n",
      " [ 0.07201188]\n",
      " [ 0.06559688]]\n",
      "the iterations of  242500 the loss is 0.300040124789247 theta= [[-8.28748197]\n",
      " [ 0.0720262 ]\n",
      " [ 0.0656114 ]]\n",
      "the iterations of  242600 the loss is 0.3000065242928166 theta= [[-8.28931501]\n",
      " [ 0.07204052]\n",
      " [ 0.06562592]]\n",
      "the iterations of  242700 the loss is 0.299972944994931 theta= [[-8.29114746]\n",
      " [ 0.07205483]\n",
      " [ 0.06564044]]\n",
      "the iterations of  242800 the loss is 0.29993938687530486 theta= [[-8.29297934]\n",
      " [ 0.07206914]\n",
      " [ 0.06565495]]\n",
      "the iterations of  242900 the loss is 0.29990584991367947 theta= [[-8.29481064]\n",
      " [ 0.07208345]\n",
      " [ 0.06566946]]\n",
      "the iterations of  243000 the loss is 0.2998723340898212 theta= [[-8.29664136]\n",
      " [ 0.07209775]\n",
      " [ 0.06568396]]\n",
      "the iterations of  243100 the loss is 0.2998388393835226 theta= [[-8.2984715 ]\n",
      " [ 0.07211204]\n",
      " [ 0.06569846]]\n",
      "the iterations of  243200 the loss is 0.2998053657746023 theta= [[-8.30030107]\n",
      " [ 0.07212633]\n",
      " [ 0.06571295]]\n",
      "the iterations of  243300 the loss is 0.29977191324290453 theta= [[-8.30213006]\n",
      " [ 0.07214062]\n",
      " [ 0.06572744]]\n",
      "the iterations of  243400 the loss is 0.299738481768299 theta= [[-8.30395848]\n",
      " [ 0.0721549 ]\n",
      " [ 0.06574193]]\n",
      "the iterations of  243500 the loss is 0.2997050713306814 theta= [[-8.30578632]\n",
      " [ 0.07216918]\n",
      " [ 0.06575641]]\n",
      "the iterations of  243600 the loss is 0.2996716819099733 theta= [[-8.30761359]\n",
      " [ 0.07218346]\n",
      " [ 0.06577088]]\n",
      "the iterations of  243700 the loss is 0.2996383134861213 theta= [[-8.30944028]\n",
      " [ 0.07219773]\n",
      " [ 0.06578535]]\n",
      "the iterations of  243800 the loss is 0.299604966039098 theta= [[-8.3112664 ]\n",
      " [ 0.07221199]\n",
      " [ 0.06579982]]\n",
      "the iterations of  243900 the loss is 0.29957163954890137 theta= [[-8.31309194]\n",
      " [ 0.07222625]\n",
      " [ 0.06581428]]\n",
      "the iterations of  244000 the loss is 0.2995383339955551 theta= [[-8.31491691]\n",
      " [ 0.07224051]\n",
      " [ 0.06582874]]\n",
      "the iterations of  244100 the loss is 0.299505049359108 theta= [[-8.31674131]\n",
      " [ 0.07225476]\n",
      " [ 0.0658432 ]]\n",
      "the iterations of  244200 the loss is 0.2994717856196345 theta= [[-8.31856513]\n",
      " [ 0.07226901]\n",
      " [ 0.06585764]]\n",
      "the iterations of  244300 the loss is 0.29943854275723447 theta= [[-8.32038838]\n",
      " [ 0.07228326]\n",
      " [ 0.06587209]]\n",
      "the iterations of  244400 the loss is 0.2994053207520333 theta= [[-8.32221106]\n",
      " [ 0.0722975 ]\n",
      " [ 0.06588653]]\n",
      "the iterations of  244500 the loss is 0.2993721195841812 theta= [[-8.32403317]\n",
      " [ 0.07231173]\n",
      " [ 0.06590097]]\n",
      "the iterations of  244600 the loss is 0.2993389392338536 theta= [[-8.3258547 ]\n",
      " [ 0.07232596]\n",
      " [ 0.0659154 ]]\n",
      "the iterations of  244700 the loss is 0.2993057796812519 theta= [[-8.32767567]\n",
      " [ 0.07234019]\n",
      " [ 0.06592982]]\n",
      "the iterations of  244800 the loss is 0.2992726409066016 theta= [[-8.32949606]\n",
      " [ 0.07235441]\n",
      " [ 0.06594425]]\n",
      "the iterations of  244900 the loss is 0.2992395228901545 theta= [[-8.33131589]\n",
      " [ 0.07236863]\n",
      " [ 0.06595866]]\n",
      "the iterations of  245000 the loss is 0.2992064256121866 theta= [[-8.33313514]\n",
      " [ 0.07238284]\n",
      " [ 0.06597308]]\n",
      "the iterations of  245100 the loss is 0.2991733490529993 theta= [[-8.33495383]\n",
      " [ 0.07239705]\n",
      " [ 0.06598749]]\n",
      "the iterations of  245200 the loss is 0.2991402931929189 theta= [[-8.33677194]\n",
      " [ 0.07241126]\n",
      " [ 0.06600189]]\n",
      "the iterations of  245300 the loss is 0.29910725801229693 theta= [[-8.33858949]\n",
      " [ 0.07242546]\n",
      " [ 0.06601629]]\n",
      "the iterations of  245400 the loss is 0.29907424349150963 theta= [[-8.34040647]\n",
      " [ 0.07243966]\n",
      " [ 0.06603069]]\n",
      "the iterations of  245500 the loss is 0.29904124961095846 theta= [[-8.34222288]\n",
      " [ 0.07245385]\n",
      " [ 0.06604508]]\n",
      "the iterations of  245600 the loss is 0.2990082763510692 theta= [[-8.34403872]\n",
      " [ 0.07246804]\n",
      " [ 0.06605947]]\n",
      "the iterations of  245700 the loss is 0.2989753236922928 theta= [[-8.345854  ]\n",
      " [ 0.07248222]\n",
      " [ 0.06607385]]\n",
      "the iterations of  245800 the loss is 0.298942391615105 theta= [[-8.34766871]\n",
      " [ 0.0724964 ]\n",
      " [ 0.06608823]]\n",
      "the iterations of  245900 the loss is 0.29890948010000623 theta= [[-8.34948285]\n",
      " [ 0.07251058]\n",
      " [ 0.0661026 ]]\n",
      "the iterations of  246000 the loss is 0.2988765891275217 theta= [[-8.35129643]\n",
      " [ 0.07252475]\n",
      " [ 0.06611697]]\n",
      "the iterations of  246100 the loss is 0.2988437186782011 theta= [[-8.35310944]\n",
      " [ 0.07253891]\n",
      " [ 0.06613133]]\n",
      "the iterations of  246200 the loss is 0.298810868732619 theta= [[-8.35492188]\n",
      " [ 0.07255308]\n",
      " [ 0.06614569]]\n",
      "the iterations of  246300 the loss is 0.2987780392713743 theta= [[-8.35673376]\n",
      " [ 0.07256724]\n",
      " [ 0.06616005]]\n",
      "the iterations of  246400 the loss is 0.2987452302750906 theta= [[-8.35854507]\n",
      " [ 0.07258139]\n",
      " [ 0.0661744 ]]\n",
      "the iterations of  246500 the loss is 0.29871244172441574 theta= [[-8.36035582]\n",
      " [ 0.07259554]\n",
      " [ 0.06618875]]\n",
      "the iterations of  246600 the loss is 0.29867967360002295 theta= [[-8.36216601]\n",
      " [ 0.07260968]\n",
      " [ 0.06620309]]\n",
      "the iterations of  246700 the loss is 0.2986469258826086 theta= [[-8.36397563]\n",
      " [ 0.07262383]\n",
      " [ 0.06621743]]\n",
      "the iterations of  246800 the loss is 0.2986141985528943 theta= [[-8.36578469]\n",
      " [ 0.07263796]\n",
      " [ 0.06623177]]\n",
      "the iterations of  246900 the loss is 0.2985814915916258 theta= [[-8.36759318]\n",
      " [ 0.0726521 ]\n",
      " [ 0.0662461 ]]\n",
      "the iterations of  247000 the loss is 0.2985488049795731 theta= [[-8.36940112]\n",
      " [ 0.07266622]\n",
      " [ 0.06626042]]\n",
      "the iterations of  247100 the loss is 0.2985161386975306 theta= [[-8.37120849]\n",
      " [ 0.07268035]\n",
      " [ 0.06627474]]\n",
      "the iterations of  247200 the loss is 0.2984834927263171 theta= [[-8.37301529]\n",
      " [ 0.07269447]\n",
      " [ 0.06628906]]\n",
      "the iterations of  247300 the loss is 0.29845086704677487 theta= [[-8.37482154]\n",
      " [ 0.07270858]\n",
      " [ 0.06630337]]\n",
      "the iterations of  247400 the loss is 0.2984182616397715 theta= [[-8.37662723]\n",
      " [ 0.0727227 ]\n",
      " [ 0.06631768]]\n",
      "the iterations of  247500 the loss is 0.2983856764861977 theta= [[-8.37843235]\n",
      " [ 0.0727368 ]\n",
      " [ 0.06633198]]\n",
      "the iterations of  247600 the loss is 0.298353111566969 theta= [[-8.38023691]\n",
      " [ 0.07275091]\n",
      " [ 0.06634628]]\n",
      "the iterations of  247700 the loss is 0.29832056686302416 theta= [[-8.38204092]\n",
      " [ 0.07276501]\n",
      " [ 0.06636058]]\n",
      "the iterations of  247800 the loss is 0.29828804235532674 theta= [[-8.38384436]\n",
      " [ 0.0727791 ]\n",
      " [ 0.06637487]]\n",
      "the iterations of  247900 the loss is 0.2982555380248639 theta= [[-8.38564725]\n",
      " [ 0.07279319]\n",
      " [ 0.06638915]]\n",
      "the iterations of  248000 the loss is 0.29822305385264714 theta= [[-8.38744957]\n",
      " [ 0.07280728]\n",
      " [ 0.06640343]]\n",
      "the iterations of  248100 the loss is 0.29819058981971125 theta= [[-8.38925134]\n",
      " [ 0.07282136]\n",
      " [ 0.06641771]]\n",
      "the iterations of  248200 the loss is 0.2981581459071152 theta= [[-8.39105254]\n",
      " [ 0.07283544]\n",
      " [ 0.06643198]]\n",
      "the iterations of  248300 the loss is 0.29812572209594185 theta= [[-8.39285319]\n",
      " [ 0.07284951]\n",
      " [ 0.06644625]]\n",
      "the iterations of  248400 the loss is 0.29809331836729797 theta= [[-8.39465329]\n",
      " [ 0.07286358]\n",
      " [ 0.06646052]]\n",
      "the iterations of  248500 the loss is 0.29806093470231343 theta= [[-8.39645282]\n",
      " [ 0.07287765]\n",
      " [ 0.06647478]]\n",
      "the iterations of  248600 the loss is 0.29802857108214276 theta= [[-8.3982518 ]\n",
      " [ 0.07289171]\n",
      " [ 0.06648903]]\n",
      "the iterations of  248700 the loss is 0.2979962274879635 theta= [[-8.40005022]\n",
      " [ 0.07290576]\n",
      " [ 0.06650328]]\n",
      "the iterations of  248800 the loss is 0.29796390390097705 theta= [[-8.40184808]\n",
      " [ 0.07291982]\n",
      " [ 0.06651753]]\n",
      "the iterations of  248900 the loss is 0.29793160030240845 theta= [[-8.40364539]\n",
      " [ 0.07293387]\n",
      " [ 0.06653177]]\n",
      "the iterations of  249000 the loss is 0.29789931667350605 theta= [[-8.40544215]\n",
      " [ 0.07294791]\n",
      " [ 0.06654601]]\n",
      "the iterations of  249100 the loss is 0.29786705299554234 theta= [[-8.40723834]\n",
      " [ 0.07296195]\n",
      " [ 0.06656025]]\n",
      "the iterations of  249200 the loss is 0.2978348092498125 theta= [[-8.40903399]\n",
      " [ 0.07297599]\n",
      " [ 0.06657448]]\n",
      "the iterations of  249300 the loss is 0.2978025854176359 theta= [[-8.41082907]\n",
      " [ 0.07299002]\n",
      " [ 0.0665887 ]]\n",
      "the iterations of  249400 the loss is 0.2977703814803548 theta= [[-8.41262361]\n",
      " [ 0.07300405]\n",
      " [ 0.06660292]]\n",
      "the iterations of  249500 the loss is 0.29773819741933527 theta= [[-8.41441759]\n",
      " [ 0.07301807]\n",
      " [ 0.06661714]]\n",
      "the iterations of  249600 the loss is 0.29770603321596645 theta= [[-8.41621102]\n",
      " [ 0.07303209]\n",
      " [ 0.06663135]]\n",
      "the iterations of  249700 the loss is 0.29767388885166096 theta= [[-8.41800389]\n",
      " [ 0.0730461 ]\n",
      " [ 0.06664556]]\n",
      "the iterations of  249800 the loss is 0.29764176430785444 theta= [[-8.41979621]\n",
      " [ 0.07306012]\n",
      " [ 0.06665976]]\n",
      "the iterations of  249900 the loss is 0.29760965956600594 theta= [[-8.42158798]\n",
      " [ 0.07307412]\n",
      " [ 0.06667396]]\n",
      "the iterations of  250000 the loss is 0.29757757460759804 theta= [[-8.42337919]\n",
      " [ 0.07308812]\n",
      " [ 0.06668816]]\n",
      "the iterations of  250100 the loss is 0.2975455094141357 theta= [[-8.42516986]\n",
      " [ 0.07310212]\n",
      " [ 0.06670235]]\n",
      "the iterations of  250200 the loss is 0.29751346396714773 theta= [[-8.42695997]\n",
      " [ 0.07311612]\n",
      " [ 0.06671654]]\n",
      "the iterations of  250300 the loss is 0.2974814382481859 theta= [[-8.42874953]\n",
      " [ 0.07313011]\n",
      " [ 0.06673072]]\n",
      "the iterations of  250400 the loss is 0.29744943223882486 theta= [[-8.43053854]\n",
      " [ 0.07314409]\n",
      " [ 0.0667449 ]]\n",
      "the iterations of  250500 the loss is 0.2974174459206622 theta= [[-8.432327  ]\n",
      " [ 0.07315808]\n",
      " [ 0.06675907]]\n",
      "the iterations of  250600 the loss is 0.297385479275319 theta= [[-8.43411491]\n",
      " [ 0.07317205]\n",
      " [ 0.06677324]]\n",
      "the iterations of  250700 the loss is 0.2973535322844388 theta= [[-8.43590227]\n",
      " [ 0.07318603]\n",
      " [ 0.0667874 ]]\n",
      "the iterations of  250800 the loss is 0.2973216049296882 theta= [[-8.43768908]\n",
      " [ 0.0732    ]\n",
      " [ 0.06680156]]\n",
      "the iterations of  250900 the loss is 0.2972896971927567 theta= [[-8.43947535]\n",
      " [ 0.07321396]\n",
      " [ 0.06681572]]\n",
      "the iterations of  251000 the loss is 0.29725780905535676 theta= [[-8.44126106]\n",
      " [ 0.07322792]\n",
      " [ 0.06682987]]\n",
      "the iterations of  251100 the loss is 0.2972259404992233 theta= [[-8.44304622]\n",
      " [ 0.07324188]\n",
      " [ 0.06684402]]\n",
      "the iterations of  251200 the loss is 0.2971940915061144 theta= [[-8.44483084]\n",
      " [ 0.07325583]\n",
      " [ 0.06685817]]\n",
      "the iterations of  251300 the loss is 0.29716226205781093 theta= [[-8.44661491]\n",
      " [ 0.07326978]\n",
      " [ 0.06687231]]\n",
      "the iterations of  251400 the loss is 0.2971304521361159 theta= [[-8.44839843]\n",
      " [ 0.07328373]\n",
      " [ 0.06688644]]\n",
      "the iterations of  251500 the loss is 0.2970986617228557 theta= [[-8.45018141]\n",
      " [ 0.07329767]\n",
      " [ 0.06690057]]\n",
      "the iterations of  251600 the loss is 0.2970668907998794 theta= [[-8.45196383]\n",
      " [ 0.0733116 ]\n",
      " [ 0.0669147 ]]\n",
      "the iterations of  251700 the loss is 0.2970351393490574 theta= [[-8.45374572]\n",
      " [ 0.07332553]\n",
      " [ 0.06692882]]\n",
      "the iterations of  251800 the loss is 0.29700340735228414 theta= [[-8.45552705]\n",
      " [ 0.07333946]\n",
      " [ 0.06694294]]\n",
      "the iterations of  251900 the loss is 0.296971694791476 theta= [[-8.45730784]\n",
      " [ 0.07335339]\n",
      " [ 0.06695705]]\n",
      "the iterations of  252000 the loss is 0.2969400016485718 theta= [[-8.45908809]\n",
      " [ 0.07336731]\n",
      " [ 0.06697116]]\n",
      "the iterations of  252100 the loss is 0.296908327905533 theta= [[-8.46086779]\n",
      " [ 0.07338122]\n",
      " [ 0.06698527]]\n",
      "the iterations of  252200 the loss is 0.29687667354434316 theta= [[-8.46264694]\n",
      " [ 0.07339513]\n",
      " [ 0.06699937]]\n",
      "the iterations of  252300 the loss is 0.29684503854700894 theta= [[-8.46442555]\n",
      " [ 0.07340904]\n",
      " [ 0.06701347]]\n",
      "the iterations of  252400 the loss is 0.2968134228955583 theta= [[-8.46620362]\n",
      " [ 0.07342294]\n",
      " [ 0.06702756]]\n",
      "the iterations of  252500 the loss is 0.29678182657204255 theta= [[-8.46798114]\n",
      " [ 0.07343684]\n",
      " [ 0.06704165]]\n",
      "the iterations of  252600 the loss is 0.29675024955853463 theta= [[-8.46975812]\n",
      " [ 0.07345074]\n",
      " [ 0.06705573]]\n",
      "the iterations of  252700 the loss is 0.29671869183712984 theta= [[-8.47153456]\n",
      " [ 0.07346463]\n",
      " [ 0.06706981]]\n",
      "the iterations of  252800 the loss is 0.2966871533899462 theta= [[-8.47331046]\n",
      " [ 0.07347852]\n",
      " [ 0.06708389]]\n",
      "the iterations of  252900 the loss is 0.2966556341991235 theta= [[-8.47508581]\n",
      " [ 0.0734924 ]\n",
      " [ 0.06709796]]\n",
      "the iterations of  253000 the loss is 0.2966241342468233 theta= [[-8.47686062]\n",
      " [ 0.07350628]\n",
      " [ 0.06711203]]\n",
      "the iterations of  253100 the loss is 0.2965926535152298 theta= [[-8.47863489]\n",
      " [ 0.07352015]\n",
      " [ 0.06712609]]\n",
      "the iterations of  253200 the loss is 0.2965611919865496 theta= [[-8.48040862]\n",
      " [ 0.07353403]\n",
      " [ 0.06714015]]\n",
      "the iterations of  253300 the loss is 0.29652974964301043 theta= [[-8.4821818 ]\n",
      " [ 0.07354789]\n",
      " [ 0.0671542 ]]\n",
      "the iterations of  253400 the loss is 0.29649832646686264 theta= [[-8.48395445]\n",
      " [ 0.07356175]\n",
      " [ 0.06716825]]\n",
      "the iterations of  253500 the loss is 0.29646692244037876 theta= [[-8.48572656]\n",
      " [ 0.07357561]\n",
      " [ 0.0671823 ]]\n",
      "the iterations of  253600 the loss is 0.2964355375458528 theta= [[-8.48749812]\n",
      " [ 0.07358947]\n",
      " [ 0.06719634]]\n",
      "the iterations of  253700 the loss is 0.29640417176560097 theta= [[-8.48926915]\n",
      " [ 0.07360332]\n",
      " [ 0.06721038]]\n",
      "the iterations of  253800 the loss is 0.29637282508196106 theta= [[-8.49103964]\n",
      " [ 0.07361716]\n",
      " [ 0.06722442]]\n",
      "the iterations of  253900 the loss is 0.2963414974772929 theta= [[-8.49280959]\n",
      " [ 0.07363101]\n",
      " [ 0.06723844]]\n",
      "the iterations of  254000 the loss is 0.2963101889339779 theta= [[-8.494579  ]\n",
      " [ 0.07364484]\n",
      " [ 0.06725247]]\n",
      "the iterations of  254100 the loss is 0.29627889943442 theta= [[-8.49634787]\n",
      " [ 0.07365868]\n",
      " [ 0.06726649]]\n",
      "the iterations of  254200 the loss is 0.2962476289610439 theta= [[-8.4981162 ]\n",
      " [ 0.07367251]\n",
      " [ 0.06728051]]\n",
      "the iterations of  254300 the loss is 0.2962163774962967 theta= [[-8.499884  ]\n",
      " [ 0.07368633]\n",
      " [ 0.06729452]]\n",
      "the iterations of  254400 the loss is 0.296185145022647 theta= [[-8.50165126]\n",
      " [ 0.07370016]\n",
      " [ 0.06730853]]\n",
      "the iterations of  254500 the loss is 0.2961539315225847 theta= [[-8.50341798]\n",
      " [ 0.07371397]\n",
      " [ 0.06732253]]\n",
      "the iterations of  254600 the loss is 0.296122736978622 theta= [[-8.50518416]\n",
      " [ 0.07372779]\n",
      " [ 0.06733653]]\n",
      "the iterations of  254700 the loss is 0.2960915613732921 theta= [[-8.50694981]\n",
      " [ 0.0737416 ]\n",
      " [ 0.06735053]]\n",
      "the iterations of  254800 the loss is 0.29606040468915007 theta= [[-8.50871493]\n",
      " [ 0.0737554 ]\n",
      " [ 0.06736452]]\n",
      "the iterations of  254900 the loss is 0.29602926690877224 theta= [[-8.51047951]\n",
      " [ 0.0737692 ]\n",
      " [ 0.06737851]]\n",
      "the iterations of  255000 the loss is 0.2959981480147563 theta= [[-8.51224355]\n",
      " [ 0.073783  ]\n",
      " [ 0.06739249]]\n",
      "the iterations of  255100 the loss is 0.2959670479897222 theta= [[-8.51400706]\n",
      " [ 0.0737968 ]\n",
      " [ 0.06740647]]\n",
      "the iterations of  255200 the loss is 0.2959359668163101 theta= [[-8.51577003]\n",
      " [ 0.07381059]\n",
      " [ 0.06742045]]\n",
      "the iterations of  255300 the loss is 0.29590490447718265 theta= [[-8.51753247]\n",
      " [ 0.07382437]\n",
      " [ 0.06743442]]\n",
      "the iterations of  255400 the loss is 0.2958738609550233 theta= [[-8.51929437]\n",
      " [ 0.07383815]\n",
      " [ 0.06744839]]\n",
      "the iterations of  255500 the loss is 0.29584283623253677 theta= [[-8.52105574]\n",
      " [ 0.07385193]\n",
      " [ 0.06746235]]\n",
      "the iterations of  255600 the loss is 0.2958118302924491 theta= [[-8.52281658]\n",
      " [ 0.0738657 ]\n",
      " [ 0.06747631]]\n",
      "the iterations of  255700 the loss is 0.2957808431175081 theta= [[-8.52457689]\n",
      " [ 0.07387947]\n",
      " [ 0.06749026]]\n",
      "the iterations of  255800 the loss is 0.295749874690482 theta= [[-8.52633666]\n",
      " [ 0.07389324]\n",
      " [ 0.06750421]]\n",
      "the iterations of  255900 the loss is 0.29571892499416064 theta= [[-8.5280959 ]\n",
      " [ 0.073907  ]\n",
      " [ 0.06751816]]\n",
      "the iterations of  256000 the loss is 0.29568799401135526 theta= [[-8.52985461]\n",
      " [ 0.07392076]\n",
      " [ 0.0675321 ]]\n",
      "the iterations of  256100 the loss is 0.29565708172489763 theta= [[-8.53161279]\n",
      " [ 0.07393451]\n",
      " [ 0.06754604]]\n",
      "the iterations of  256200 the loss is 0.29562618811764085 theta= [[-8.53337043]\n",
      " [ 0.07394826]\n",
      " [ 0.06755997]]\n",
      "the iterations of  256300 the loss is 0.2955953131724593 theta= [[-8.53512755]\n",
      " [ 0.073962  ]\n",
      " [ 0.0675739 ]]\n",
      "the iterations of  256400 the loss is 0.29556445687224814 theta= [[-8.53688413]\n",
      " [ 0.07397575]\n",
      " [ 0.06758783]]\n",
      "the iterations of  256500 the loss is 0.2955336191999239 theta= [[-8.53864018]\n",
      " [ 0.07398948]\n",
      " [ 0.06760175]]\n",
      "the iterations of  256600 the loss is 0.2955028001384232 theta= [[-8.54039571]\n",
      " [ 0.07400322]\n",
      " [ 0.06761567]]\n",
      "the iterations of  256700 the loss is 0.2954719996707049 theta= [[-8.5421507 ]\n",
      " [ 0.07401695]\n",
      " [ 0.06762958]]\n",
      "the iterations of  256800 the loss is 0.29544121777974736 theta= [[-8.54390517]\n",
      " [ 0.07403067]\n",
      " [ 0.06764349]]\n",
      "the iterations of  256900 the loss is 0.29541045444855085 theta= [[-8.5456591 ]\n",
      " [ 0.07404439]\n",
      " [ 0.06765739]]\n",
      "the iterations of  257000 the loss is 0.295379709660136 theta= [[-8.54741251]\n",
      " [ 0.07405811]\n",
      " [ 0.06767129]]\n",
      "the iterations of  257100 the loss is 0.2953489833975444 theta= [[-8.54916539]\n",
      " [ 0.07407182]\n",
      " [ 0.06768519]]\n",
      "the iterations of  257200 the loss is 0.2953182756438383 theta= [[-8.55091774]\n",
      " [ 0.07408553]\n",
      " [ 0.06769908]]\n",
      "the iterations of  257300 the loss is 0.29528758638210056 theta= [[-8.55266956]\n",
      " [ 0.07409924]\n",
      " [ 0.06771297]]\n",
      "the iterations of  257400 the loss is 0.29525691559543515 theta= [[-8.55442086]\n",
      " [ 0.07411294]\n",
      " [ 0.06772686]]\n",
      "the iterations of  257500 the loss is 0.2952262632669664 theta= [[-8.55617162]\n",
      " [ 0.07412664]\n",
      " [ 0.06774074]]\n",
      "the iterations of  257600 the loss is 0.2951956293798394 theta= [[-8.55792187]\n",
      " [ 0.07414033]\n",
      " [ 0.06775461]]\n",
      "the iterations of  257700 the loss is 0.29516501391721983 theta= [[-8.55967158]\n",
      " [ 0.07415402]\n",
      " [ 0.06776848]]\n",
      "the iterations of  257800 the loss is 0.29513441686229386 theta= [[-8.56142077]\n",
      " [ 0.0741677 ]\n",
      " [ 0.06778235]]\n",
      "the iterations of  257900 the loss is 0.2951038381982684 theta= [[-8.56316943]\n",
      " [ 0.07418138]\n",
      " [ 0.06779622]]\n",
      "the iterations of  258000 the loss is 0.295073277908371 theta= [[-8.56491757]\n",
      " [ 0.07419506]\n",
      " [ 0.06781007]]\n",
      "the iterations of  258100 the loss is 0.29504273597584907 theta= [[-8.56666518]\n",
      " [ 0.07420874]\n",
      " [ 0.06782393]]\n",
      "the iterations of  258200 the loss is 0.2950122123839712 theta= [[-8.56841227]\n",
      " [ 0.07422241]\n",
      " [ 0.06783778]]\n",
      "the iterations of  258300 the loss is 0.2949817071160257 theta= [[-8.57015883]\n",
      " [ 0.07423607]\n",
      " [ 0.06785163]]\n",
      "the iterations of  258400 the loss is 0.2949512201553221 theta= [[-8.57190487]\n",
      " [ 0.07424973]\n",
      " [ 0.06786547]]\n",
      "the iterations of  258500 the loss is 0.2949207514851895 theta= [[-8.57365039]\n",
      " [ 0.07426339]\n",
      " [ 0.06787931]]\n",
      "the iterations of  258600 the loss is 0.294890301088978 theta= [[-8.57539538]\n",
      " [ 0.07427704]\n",
      " [ 0.06789315]]\n",
      "the iterations of  258700 the loss is 0.2948598689500574 theta= [[-8.57713985]\n",
      " [ 0.07429069]\n",
      " [ 0.06790698]]\n",
      "the iterations of  258800 the loss is 0.2948294550518181 theta= [[-8.57888379]\n",
      " [ 0.07430434]\n",
      " [ 0.06792081]]\n",
      "the iterations of  258900 the loss is 0.294799059377671 theta= [[-8.58062722]\n",
      " [ 0.07431798]\n",
      " [ 0.06793463]]\n",
      "the iterations of  259000 the loss is 0.2947686819110469 theta= [[-8.58237012]\n",
      " [ 0.07433162]\n",
      " [ 0.06794845]]\n",
      "the iterations of  259100 the loss is 0.2947383226353963 theta= [[-8.5841125 ]\n",
      " [ 0.07434525]\n",
      " [ 0.06796226]]\n",
      "the iterations of  259200 the loss is 0.2947079815341907 theta= [[-8.58585435]\n",
      " [ 0.07435888]\n",
      " [ 0.06797607]]\n",
      "the iterations of  259300 the loss is 0.2946776585909214 theta= [[-8.58759569]\n",
      " [ 0.07437251]\n",
      " [ 0.06798988]]\n",
      "the iterations of  259400 the loss is 0.2946473537890998 theta= [[-8.58933651]\n",
      " [ 0.07438613]\n",
      " [ 0.06800368]]\n",
      "the iterations of  259500 the loss is 0.29461706711225705 theta= [[-8.5910768 ]\n",
      " [ 0.07439975]\n",
      " [ 0.06801748]]\n",
      "the iterations of  259600 the loss is 0.294586798543945 theta= [[-8.59281657]\n",
      " [ 0.07441336]\n",
      " [ 0.06803128]]\n",
      "the iterations of  259700 the loss is 0.29455654806773446 theta= [[-8.59455583]\n",
      " [ 0.07442697]\n",
      " [ 0.06804507]]\n",
      "the iterations of  259800 the loss is 0.29452631566721726 theta= [[-8.59629456]\n",
      " [ 0.07444058]\n",
      " [ 0.06805885]]\n",
      "the iterations of  259900 the loss is 0.2944961013260045 theta= [[-8.59803278]\n",
      " [ 0.07445418]\n",
      " [ 0.06807263]]\n",
      "the iterations of  260000 the loss is 0.29446590502772757 theta= [[-8.59977047]\n",
      " [ 0.07446778]\n",
      " [ 0.06808641]]\n",
      "the iterations of  260100 the loss is 0.29443572675603774 theta= [[-8.60150765]\n",
      " [ 0.07448138]\n",
      " [ 0.06810019]]\n",
      "the iterations of  260200 the loss is 0.29440556649460564 theta= [[-8.60324431]\n",
      " [ 0.07449497]\n",
      " [ 0.06811396]]\n",
      "the iterations of  260300 the loss is 0.2943754242271223 theta= [[-8.60498045]\n",
      " [ 0.07450855]\n",
      " [ 0.06812772]]\n",
      "the iterations of  260400 the loss is 0.29434529993729813 theta= [[-8.60671607]\n",
      " [ 0.07452214]\n",
      " [ 0.06814149]]\n",
      "the iterations of  260500 the loss is 0.29431519360886343 theta= [[-8.60845117]\n",
      " [ 0.07453571]\n",
      " [ 0.06815524]]\n",
      "the iterations of  260600 the loss is 0.29428510522556833 theta= [[-8.61018576]\n",
      " [ 0.07454929]\n",
      " [ 0.068169  ]]\n",
      "the iterations of  260700 the loss is 0.29425503477118253 theta= [[-8.61191983]\n",
      " [ 0.07456286]\n",
      " [ 0.06818275]]\n",
      "the iterations of  260800 the loss is 0.29422498222949556 theta= [[-8.61365339]\n",
      " [ 0.07457643]\n",
      " [ 0.06819649]]\n",
      "the iterations of  260900 the loss is 0.29419494758431625 theta= [[-8.61538643]\n",
      " [ 0.07458999]\n",
      " [ 0.06821024]]\n",
      "the iterations of  261000 the loss is 0.2941649308194733 theta= [[-8.61711895]\n",
      " [ 0.07460355]\n",
      " [ 0.06822397]]\n",
      "the iterations of  261100 the loss is 0.29413493191881507 theta= [[-8.61885095]\n",
      " [ 0.07461711]\n",
      " [ 0.06823771]]\n",
      "the iterations of  261200 the loss is 0.29410495086620925 theta= [[-8.62058245]\n",
      " [ 0.07463066]\n",
      " [ 0.06825144]]\n",
      "the iterations of  261300 the loss is 0.2940749876455432 theta= [[-8.62231342]\n",
      " [ 0.07464421]\n",
      " [ 0.06826516]]\n",
      "the iterations of  261400 the loss is 0.29404504224072375 theta= [[-8.62404388]\n",
      " [ 0.07465775]\n",
      " [ 0.06827889]]\n",
      "the iterations of  261500 the loss is 0.2940151146356768 theta= [[-8.62577383]\n",
      " [ 0.07467129]\n",
      " [ 0.0682926 ]]\n",
      "the iterations of  261600 the loss is 0.2939852048143484 theta= [[-8.62750326]\n",
      " [ 0.07468483]\n",
      " [ 0.06830632]]\n",
      "the iterations of  261700 the loss is 0.2939553127607038 theta= [[-8.62923218]\n",
      " [ 0.07469836]\n",
      " [ 0.06832003]]\n",
      "the iterations of  261800 the loss is 0.2939254384587272 theta= [[-8.63096059]\n",
      " [ 0.07471189]\n",
      " [ 0.06833373]]\n",
      "the iterations of  261900 the loss is 0.2938955818924221 theta= [[-8.63268848]\n",
      " [ 0.07472541]\n",
      " [ 0.06834744]]\n",
      "the iterations of  262000 the loss is 0.29386574304581187 theta= [[-8.63441586]\n",
      " [ 0.07473893]\n",
      " [ 0.06836113]]\n",
      "the iterations of  262100 the loss is 0.29383592190293895 theta= [[-8.63614273]\n",
      " [ 0.07475245]\n",
      " [ 0.06837483]]\n",
      "the iterations of  262200 the loss is 0.2938061184478651 theta= [[-8.63786908]\n",
      " [ 0.07476596]\n",
      " [ 0.06838852]]\n",
      "the iterations of  262300 the loss is 0.29377633266467085 theta= [[-8.63959492]\n",
      " [ 0.07477947]\n",
      " [ 0.0684022 ]]\n",
      "the iterations of  262400 the loss is 0.29374656453745673 theta= [[-8.64132026]\n",
      " [ 0.07479297]\n",
      " [ 0.06841589]]\n",
      "the iterations of  262500 the loss is 0.2937168140503418 theta= [[-8.64304508]\n",
      " [ 0.07480647]\n",
      " [ 0.06842956]]\n",
      "the iterations of  262600 the loss is 0.2936870811874643 theta= [[-8.64476939]\n",
      " [ 0.07481997]\n",
      " [ 0.06844324]]\n",
      "the iterations of  262700 the loss is 0.29365736593298214 theta= [[-8.64649318]\n",
      " [ 0.07483347]\n",
      " [ 0.06845691]]\n",
      "the iterations of  262800 the loss is 0.2936276682710717 theta= [[-8.64821647]\n",
      " [ 0.07484696]\n",
      " [ 0.06847058]]\n",
      "the iterations of  262900 the loss is 0.2935979881859284 theta= [[-8.64993925]\n",
      " [ 0.07486044]\n",
      " [ 0.06848424]]\n",
      "the iterations of  263000 the loss is 0.29356832566176705 theta= [[-8.65166152]\n",
      " [ 0.07487392]\n",
      " [ 0.0684979 ]]\n",
      "the iterations of  263100 the loss is 0.2935386806828212 theta= [[-8.65338328]\n",
      " [ 0.0748874 ]\n",
      " [ 0.06851155]]\n",
      "the iterations of  263200 the loss is 0.2935090532333437 theta= [[-8.65510453]\n",
      " [ 0.07490087]\n",
      " [ 0.0685252 ]]\n",
      "the iterations of  263300 the loss is 0.2934794432976061 theta= [[-8.65682527]\n",
      " [ 0.07491434]\n",
      " [ 0.06853885]]\n",
      "the iterations of  263400 the loss is 0.29344985085989905 theta= [[-8.6585455 ]\n",
      " [ 0.07492781]\n",
      " [ 0.06855249]]\n",
      "the iterations of  263500 the loss is 0.2934202759045316 theta= [[-8.66026523]\n",
      " [ 0.07494127]\n",
      " [ 0.06856613]]\n",
      "the iterations of  263600 the loss is 0.29339071841583225 theta= [[-8.66198444]\n",
      " [ 0.07495473]\n",
      " [ 0.06857976]]\n",
      "the iterations of  263700 the loss is 0.293361178378148 theta= [[-8.66370315]\n",
      " [ 0.07496819]\n",
      " [ 0.06859339]]\n",
      "the iterations of  263800 the loss is 0.2933316557758446 theta= [[-8.66542135]\n",
      " [ 0.07498164]\n",
      " [ 0.06860702]]\n",
      "the iterations of  263900 the loss is 0.29330215059330667 theta= [[-8.66713905]\n",
      " [ 0.07499509]\n",
      " [ 0.06862064]]\n",
      "the iterations of  264000 the loss is 0.29327266281493775 theta= [[-8.66885623]\n",
      " [ 0.07500853]\n",
      " [ 0.06863426]]\n",
      "the iterations of  264100 the loss is 0.2932431924251599 theta= [[-8.67057291]\n",
      " [ 0.07502197]\n",
      " [ 0.06864787]]\n",
      "the iterations of  264200 the loss is 0.2932137394084139 theta= [[-8.67228909]\n",
      " [ 0.0750354 ]\n",
      " [ 0.06866148]]\n",
      "the iterations of  264300 the loss is 0.29318430374915894 theta= [[-8.67400476]\n",
      " [ 0.07504884]\n",
      " [ 0.06867509]]\n",
      "the iterations of  264400 the loss is 0.29315488543187346 theta= [[-8.67571992]\n",
      " [ 0.07506226]\n",
      " [ 0.06868869]]\n",
      "the iterations of  264500 the loss is 0.2931254844410541 theta= [[-8.67743458]\n",
      " [ 0.07507569]\n",
      " [ 0.06870229]]\n",
      "the iterations of  264600 the loss is 0.29309610076121595 theta= [[-8.67914874]\n",
      " [ 0.07508911]\n",
      " [ 0.06871589]]\n",
      "the iterations of  264700 the loss is 0.293066734376893 theta= [[-8.68086239]\n",
      " [ 0.07510253]\n",
      " [ 0.06872948]]\n",
      "the iterations of  264800 the loss is 0.29303738527263745 theta= [[-8.68257553]\n",
      " [ 0.07511594]\n",
      " [ 0.06874307]]\n",
      "the iterations of  264900 the loss is 0.29300805343302017 theta= [[-8.68428817]\n",
      " [ 0.07512935]\n",
      " [ 0.06875665]]\n",
      "the iterations of  265000 the loss is 0.2929787388426302 theta= [[-8.68600031]\n",
      " [ 0.07514275]\n",
      " [ 0.06877023]]\n",
      "the iterations of  265100 the loss is 0.2929494414860756 theta= [[-8.68771194]\n",
      " [ 0.07515615]\n",
      " [ 0.0687838 ]]\n",
      "the iterations of  265200 the loss is 0.2929201613479825 theta= [[-8.68942307]\n",
      " [ 0.07516955]\n",
      " [ 0.06879738]]\n",
      "the iterations of  265300 the loss is 0.29289089841299537 theta= [[-8.6911337 ]\n",
      " [ 0.07518295]\n",
      " [ 0.06881094]]\n",
      "the iterations of  265400 the loss is 0.292861652665777 theta= [[-8.69284383]\n",
      " [ 0.07519634]\n",
      " [ 0.06882451]]\n",
      "the iterations of  265500 the loss is 0.29283242409100874 theta= [[-8.69455345]\n",
      " [ 0.07520972]\n",
      " [ 0.06883807]]\n",
      "the iterations of  265600 the loss is 0.2928032126733897 theta= [[-8.69626257]\n",
      " [ 0.0752231 ]\n",
      " [ 0.06885162]]\n",
      "the iterations of  265700 the loss is 0.29277401839763834 theta= [[-8.69797119]\n",
      " [ 0.07523648]\n",
      " [ 0.06886517]]\n",
      "the iterations of  265800 the loss is 0.2927448412484901 theta= [[-8.69967931]\n",
      " [ 0.07524986]\n",
      " [ 0.06887872]]\n",
      "the iterations of  265900 the loss is 0.2927156812106996 theta= [[-8.70138693]\n",
      " [ 0.07526323]\n",
      " [ 0.06889227]]\n",
      "the iterations of  266000 the loss is 0.2926865382690391 theta= [[-8.70309405]\n",
      " [ 0.07527659]\n",
      " [ 0.06890581]]\n",
      "the iterations of  266100 the loss is 0.29265741240829934 theta= [[-8.70480067]\n",
      " [ 0.07528996]\n",
      " [ 0.06891934]]\n",
      "the iterations of  266200 the loss is 0.2926283036132889 theta= [[-8.70650678]\n",
      " [ 0.07530332]\n",
      " [ 0.06893287]]\n",
      "the iterations of  266300 the loss is 0.292599211868835 theta= [[-8.7082124 ]\n",
      " [ 0.07531667]\n",
      " [ 0.0689464 ]]\n",
      "the iterations of  266400 the loss is 0.292570137159782 theta= [[-8.70991752]\n",
      " [ 0.07533003]\n",
      " [ 0.06895993]]\n",
      "the iterations of  266500 the loss is 0.2925410794709935 theta= [[-8.71162214]\n",
      " [ 0.07534337]\n",
      " [ 0.06897345]]\n",
      "the iterations of  266600 the loss is 0.2925120387873502 theta= [[-8.71332626]\n",
      " [ 0.07535672]\n",
      " [ 0.06898696]]\n",
      "the iterations of  266700 the loss is 0.2924830150937512 theta= [[-8.71502988]\n",
      " [ 0.07537006]\n",
      " [ 0.06900048]]\n",
      "the iterations of  266800 the loss is 0.2924540083751135 theta= [[-8.716733  ]\n",
      " [ 0.0753834 ]\n",
      " [ 0.06901399]]\n",
      "the iterations of  266900 the loss is 0.29242501861637255 theta= [[-8.71843562]\n",
      " [ 0.07539673]\n",
      " [ 0.06902749]]\n",
      "the iterations of  267000 the loss is 0.2923960458024803 theta= [[-8.72013775]\n",
      " [ 0.07541006]\n",
      " [ 0.06904099]]\n",
      "the iterations of  267100 the loss is 0.2923670899184085 theta= [[-8.72183938]\n",
      " [ 0.07542338]\n",
      " [ 0.06905449]]\n",
      "the iterations of  267200 the loss is 0.2923381509491451 theta= [[-8.72354051]\n",
      " [ 0.07543671]\n",
      " [ 0.06906798]]\n",
      "the iterations of  267300 the loss is 0.29230922887969696 theta= [[-8.72524115]\n",
      " [ 0.07545002]\n",
      " [ 0.06908147]]\n",
      "the iterations of  267400 the loss is 0.2922803236950884 theta= [[-8.72694129]\n",
      " [ 0.07546334]\n",
      " [ 0.06909496]]\n",
      "the iterations of  267500 the loss is 0.29225143538036147 theta= [[-8.72864093]\n",
      " [ 0.07547665]\n",
      " [ 0.06910844]]\n",
      "the iterations of  267600 the loss is 0.2922225639205761 theta= [[-8.73034008]\n",
      " [ 0.07548996]\n",
      " [ 0.06912192]]\n",
      "the iterations of  267700 the loss is 0.29219370930081 theta= [[-8.73203873]\n",
      " [ 0.07550326]\n",
      " [ 0.06913539]]\n",
      "the iterations of  267800 the loss is 0.29216487150615866 theta= [[-8.73373689]\n",
      " [ 0.07551656]\n",
      " [ 0.06914886]]\n",
      "the iterations of  267900 the loss is 0.29213605052173486 theta= [[-8.73543455]\n",
      " [ 0.07552985]\n",
      " [ 0.06916233]]\n",
      "the iterations of  268000 the loss is 0.2921072463326693 theta= [[-8.73713171]\n",
      " [ 0.07554314]\n",
      " [ 0.06917579]]\n",
      "the iterations of  268100 the loss is 0.29207845892411055 theta= [[-8.73882839]\n",
      " [ 0.07555643]\n",
      " [ 0.06918925]]\n",
      "the iterations of  268200 the loss is 0.29204968828122435 theta= [[-8.74052456]\n",
      " [ 0.07556972]\n",
      " [ 0.06920271]]\n",
      "the iterations of  268300 the loss is 0.29202093438919424 theta= [[-8.74222025]\n",
      " [ 0.075583  ]\n",
      " [ 0.06921616]]\n",
      "the iterations of  268400 the loss is 0.29199219723322123 theta= [[-8.74391544]\n",
      " [ 0.07559627]\n",
      " [ 0.0692296 ]]\n",
      "the iterations of  268500 the loss is 0.29196347679852414 theta= [[-8.74561014]\n",
      " [ 0.07560955]\n",
      " [ 0.06924305]]\n",
      "the iterations of  268600 the loss is 0.291934773070339 theta= [[-8.74730434]\n",
      " [ 0.07562282]\n",
      " [ 0.06925649]]\n",
      "the iterations of  268700 the loss is 0.2919060860339196 theta= [[-8.74899805]\n",
      " [ 0.07563608]\n",
      " [ 0.06926992]]\n",
      "the iterations of  268800 the loss is 0.29187741567453673 theta= [[-8.75069127]\n",
      " [ 0.07564934]\n",
      " [ 0.06928336]]\n",
      "the iterations of  268900 the loss is 0.29184876197747894 theta= [[-8.752384  ]\n",
      " [ 0.0756626 ]\n",
      " [ 0.06929678]]\n",
      "the iterations of  269000 the loss is 0.2918201249280521 theta= [[-8.75407623]\n",
      " [ 0.07567586]\n",
      " [ 0.06931021]]\n",
      "the iterations of  269100 the loss is 0.29179150451157965 theta= [[-8.75576798]\n",
      " [ 0.07568911]\n",
      " [ 0.06932363]]\n",
      "the iterations of  269200 the loss is 0.2917629007134019 theta= [[-8.75745923]\n",
      " [ 0.07570235]\n",
      " [ 0.06933704]]\n",
      "the iterations of  269300 the loss is 0.29173431351887713 theta= [[-8.75914999]\n",
      " [ 0.0757156 ]\n",
      " [ 0.06935046]]\n",
      "the iterations of  269400 the loss is 0.29170574291338036 theta= [[-8.76084026]\n",
      " [ 0.07572884]\n",
      " [ 0.06936387]]\n",
      "the iterations of  269500 the loss is 0.2916771888823042 theta= [[-8.76253004]\n",
      " [ 0.07574207]\n",
      " [ 0.06937727]]\n",
      "the iterations of  269600 the loss is 0.2916486514110584 theta= [[-8.76421933]\n",
      " [ 0.07575531]\n",
      " [ 0.06939067]]\n",
      "the iterations of  269700 the loss is 0.2916201304850701 theta= [[-8.76590813]\n",
      " [ 0.07576853]\n",
      " [ 0.06940407]]\n",
      "the iterations of  269800 the loss is 0.2915916260897833 theta= [[-8.76759644]\n",
      " [ 0.07578176]\n",
      " [ 0.06941746]]\n",
      "the iterations of  269900 the loss is 0.2915631382106591 theta= [[-8.76928427]\n",
      " [ 0.07579498]\n",
      " [ 0.06943085]]\n",
      "the iterations of  270000 the loss is 0.2915346668331767 theta= [[-8.7709716 ]\n",
      " [ 0.0758082 ]\n",
      " [ 0.06944424]]\n",
      "the iterations of  270100 the loss is 0.2915062119428313 theta= [[-8.77265844]\n",
      " [ 0.07582141]\n",
      " [ 0.06945762]]\n",
      "the iterations of  270200 the loss is 0.2914777735251359 theta= [[-8.7743448 ]\n",
      " [ 0.07583462]\n",
      " [ 0.069471  ]]\n",
      "the iterations of  270300 the loss is 0.29144935156561974 theta= [[-8.77603067]\n",
      " [ 0.07584783]\n",
      " [ 0.06948437]]\n",
      "the iterations of  270400 the loss is 0.29142094604983043 theta= [[-8.77771605]\n",
      " [ 0.07586103]\n",
      " [ 0.06949774]]\n",
      "the iterations of  270500 the loss is 0.2913925569633311 theta= [[-8.77940094]\n",
      " [ 0.07587423]\n",
      " [ 0.06951111]]\n",
      "the iterations of  270600 the loss is 0.29136418429170335 theta= [[-8.78108535]\n",
      " [ 0.07588742]\n",
      " [ 0.06952447]]\n",
      "the iterations of  270700 the loss is 0.29133582802054464 theta= [[-8.78276926]\n",
      " [ 0.07590061]\n",
      " [ 0.06953783]]\n",
      "the iterations of  270800 the loss is 0.29130748813546986 theta= [[-8.7844527 ]\n",
      " [ 0.0759138 ]\n",
      " [ 0.06955119]]\n",
      "the iterations of  270900 the loss is 0.2912791646221107 theta= [[-8.78613564]\n",
      " [ 0.07592699]\n",
      " [ 0.06956454]]\n",
      "the iterations of  271000 the loss is 0.29125085746611556 theta= [[-8.7878181 ]\n",
      " [ 0.07594017]\n",
      " [ 0.06957789]]\n",
      "the iterations of  271100 the loss is 0.2912225666531503 theta= [[-8.78950008]\n",
      " [ 0.07595334]\n",
      " [ 0.06959123]]\n",
      "the iterations of  271200 the loss is 0.2911942921688972 theta= [[-8.79118156]\n",
      " [ 0.07596652]\n",
      " [ 0.06960457]]\n",
      "the iterations of  271300 the loss is 0.2911660339990552 theta= [[-8.79286257]\n",
      " [ 0.07597968]\n",
      " [ 0.06961791]]\n",
      "the iterations of  271400 the loss is 0.29113779212934043 theta= [[-8.79454309]\n",
      " [ 0.07599285]\n",
      " [ 0.06963124]]\n",
      "the iterations of  271500 the loss is 0.29110956654548575 theta= [[-8.79622312]\n",
      " [ 0.07600601]\n",
      " [ 0.06964457]]\n",
      "the iterations of  271600 the loss is 0.2910813572332405 theta= [[-8.79790267]\n",
      " [ 0.07601917]\n",
      " [ 0.0696579 ]]\n",
      "the iterations of  271700 the loss is 0.29105316417837096 theta= [[-8.79958173]\n",
      " [ 0.07603232]\n",
      " [ 0.06967122]]\n",
      "the iterations of  271800 the loss is 0.29102498736666005 theta= [[-8.80126032]\n",
      " [ 0.07604548]\n",
      " [ 0.06968454]]\n",
      "the iterations of  271900 the loss is 0.2909968267839074 theta= [[-8.80293841]\n",
      " [ 0.07605862]\n",
      " [ 0.06969785]]\n",
      "the iterations of  272000 the loss is 0.29096868241592927 theta= [[-8.80461603]\n",
      " [ 0.07607177]\n",
      " [ 0.06971116]]\n",
      "the iterations of  272100 the loss is 0.29094055424855836 theta= [[-8.80629316]\n",
      " [ 0.07608491]\n",
      " [ 0.06972447]]\n",
      "the iterations of  272200 the loss is 0.2909124422676443 theta= [[-8.80796981]\n",
      " [ 0.07609804]\n",
      " [ 0.06973777]]\n",
      "the iterations of  272300 the loss is 0.29088434645905303 theta= [[-8.80964598]\n",
      " [ 0.07611118]\n",
      " [ 0.06975107]]\n",
      "the iterations of  272400 the loss is 0.2908562668086674 theta= [[-8.81132166]\n",
      " [ 0.0761243 ]\n",
      " [ 0.06976437]]\n",
      "the iterations of  272500 the loss is 0.29082820330238646 theta= [[-8.81299686]\n",
      " [ 0.07613743]\n",
      " [ 0.06977766]]\n",
      "the iterations of  272600 the loss is 0.2908001559261259 theta= [[-8.81467159]\n",
      " [ 0.07615055]\n",
      " [ 0.06979094]]\n",
      "the iterations of  272700 the loss is 0.2907721246658179 theta= [[-8.81634583]\n",
      " [ 0.07616367]\n",
      " [ 0.06980423]]\n",
      "the iterations of  272800 the loss is 0.290744109507411 theta= [[-8.81801959]\n",
      " [ 0.07617678]\n",
      " [ 0.06981751]]\n",
      "the iterations of  272900 the loss is 0.29071611043687 theta= [[-8.81969286]\n",
      " [ 0.07618989]\n",
      " [ 0.06983078]]\n",
      "the iterations of  273000 the loss is 0.2906881274401767 theta= [[-8.82136566]\n",
      " [ 0.076203  ]\n",
      " [ 0.06984406]]\n",
      "the iterations of  273100 the loss is 0.29066016050332877 theta= [[-8.82303798]\n",
      " [ 0.0762161 ]\n",
      " [ 0.06985733]]\n",
      "the iterations of  273200 the loss is 0.29063220961234065 theta= [[-8.82470982]\n",
      " [ 0.0762292 ]\n",
      " [ 0.06987059]]\n",
      "the iterations of  273300 the loss is 0.2906042747532427 theta= [[-8.82638118]\n",
      " [ 0.0762423 ]\n",
      " [ 0.06988385]]\n",
      "the iterations of  273400 the loss is 0.2905763559120816 theta= [[-8.82805206]\n",
      " [ 0.07625539]\n",
      " [ 0.06989711]]\n",
      "the iterations of  273500 the loss is 0.2905484530749206 theta= [[-8.82972246]\n",
      " [ 0.07626848]\n",
      " [ 0.06991037]]\n",
      "the iterations of  273600 the loss is 0.2905205662278392 theta= [[-8.83139238]\n",
      " [ 0.07628157]\n",
      " [ 0.06992362]]\n",
      "the iterations of  273700 the loss is 0.2904926953569332 theta= [[-8.83306182]\n",
      " [ 0.07629465]\n",
      " [ 0.06993686]]\n",
      "the iterations of  273800 the loss is 0.29046484044831433 theta= [[-8.83473079]\n",
      " [ 0.07630773]\n",
      " [ 0.0699501 ]]\n",
      "the iterations of  273900 the loss is 0.29043700148811075 theta= [[-8.83639928]\n",
      " [ 0.0763208 ]\n",
      " [ 0.06996334]]\n",
      "the iterations of  274000 the loss is 0.2904091784624666 theta= [[-8.83806729]\n",
      " [ 0.07633387]\n",
      " [ 0.06997658]]\n",
      "the iterations of  274100 the loss is 0.2903813713575424 theta= [[-8.83973482]\n",
      " [ 0.07634694]\n",
      " [ 0.06998981]]\n",
      "the iterations of  274200 the loss is 0.2903535801595146 theta= [[-8.84140187]\n",
      " [ 0.07636   ]\n",
      " [ 0.07000304]]\n",
      "the iterations of  274300 the loss is 0.29032580485457593 theta= [[-8.84306845]\n",
      " [ 0.07637306]\n",
      " [ 0.07001626]]\n",
      "the iterations of  274400 the loss is 0.290298045428935 theta= [[-8.84473456]\n",
      " [ 0.07638612]\n",
      " [ 0.07002948]]\n",
      "the iterations of  274500 the loss is 0.2902703018688168 theta= [[-8.84640018]\n",
      " [ 0.07639917]\n",
      " [ 0.0700427 ]]\n",
      "the iterations of  274600 the loss is 0.29024257416046195 theta= [[-8.84806533]\n",
      " [ 0.07641222]\n",
      " [ 0.07005591]]\n",
      "the iterations of  274700 the loss is 0.29021486229012716 theta= [[-8.84973001]\n",
      " [ 0.07642527]\n",
      " [ 0.07006912]]\n",
      "the iterations of  274800 the loss is 0.2901871662440854 theta= [[-8.85139421]\n",
      " [ 0.07643831]\n",
      " [ 0.07008233]]\n",
      "the iterations of  274900 the loss is 0.29015948600862546 theta= [[-8.85305793]\n",
      " [ 0.07645135]\n",
      " [ 0.07009553]]\n",
      "the iterations of  275000 the loss is 0.2901318215700523 theta= [[-8.85472118]\n",
      " [ 0.07646438]\n",
      " [ 0.07010873]]\n",
      "the iterations of  275100 the loss is 0.29010417291468593 theta= [[-8.85638395]\n",
      " [ 0.07647741]\n",
      " [ 0.07012192]]\n",
      "the iterations of  275200 the loss is 0.2900765400288636 theta= [[-8.85804625]\n",
      " [ 0.07649044]\n",
      " [ 0.07013511]]\n",
      "the iterations of  275300 the loss is 0.29004892289893736 theta= [[-8.85970808]\n",
      " [ 0.07650346]\n",
      " [ 0.0701483 ]]\n",
      "the iterations of  275400 the loss is 0.2900213215112752 theta= [[-8.86136943]\n",
      " [ 0.07651648]\n",
      " [ 0.07016149]]\n",
      "the iterations of  275500 the loss is 0.2899937358522615 theta= [[-8.86303031]\n",
      " [ 0.0765295 ]\n",
      " [ 0.07017467]]\n",
      "the iterations of  275600 the loss is 0.289966165908296 theta= [[-8.86469072]\n",
      " [ 0.07654251]\n",
      " [ 0.07018784]]\n",
      "the iterations of  275700 the loss is 0.28993861166579454 theta= [[-8.86635065]\n",
      " [ 0.07655552]\n",
      " [ 0.07020101]]\n",
      "the iterations of  275800 the loss is 0.28991107311118836 theta= [[-8.86801011]\n",
      " [ 0.07656853]\n",
      " [ 0.07021418]]\n",
      "the iterations of  275900 the loss is 0.2898835502309247 theta= [[-8.8696691 ]\n",
      " [ 0.07658153]\n",
      " [ 0.07022735]]\n",
      "the iterations of  276000 the loss is 0.2898560430114665 theta= [[-8.87132762]\n",
      " [ 0.07659453]\n",
      " [ 0.07024051]]\n",
      "the iterations of  276100 the loss is 0.28982855143929215 theta= [[-8.87298566]\n",
      " [ 0.07660752]\n",
      " [ 0.07025367]]\n",
      "the iterations of  276200 the loss is 0.28980107550089595 theta= [[-8.87464324]\n",
      " [ 0.07662051]\n",
      " [ 0.07026682]]\n",
      "the iterations of  276300 the loss is 0.2897736151827877 theta= [[-8.87630034]\n",
      " [ 0.0766335 ]\n",
      " [ 0.07027997]]\n",
      "the iterations of  276400 the loss is 0.289746170471493 theta= [[-8.87795697]\n",
      " [ 0.07664649]\n",
      " [ 0.07029312]]\n",
      "the iterations of  276500 the loss is 0.2897187413535528 theta= [[-8.87961313]\n",
      " [ 0.07665947]\n",
      " [ 0.07030626]]\n",
      "the iterations of  276600 the loss is 0.28969132781552376 theta= [[-8.88126882]\n",
      " [ 0.07667244]\n",
      " [ 0.0703194 ]]\n",
      "the iterations of  276700 the loss is 0.2896639298439784 theta= [[-8.88292404]\n",
      " [ 0.07668542]\n",
      " [ 0.07033254]]\n",
      "the iterations of  276800 the loss is 0.289636547425504 theta= [[-8.88457879]\n",
      " [ 0.07669839]\n",
      " [ 0.07034567]]\n",
      "the iterations of  276900 the loss is 0.28960918054670426 theta= [[-8.88623307]\n",
      " [ 0.07671135]\n",
      " [ 0.0703588 ]]\n",
      "the iterations of  277000 the loss is 0.2895818291941975 theta= [[-8.88788688]\n",
      " [ 0.07672432]\n",
      " [ 0.07037192]]\n",
      "the iterations of  277100 the loss is 0.2895544933546183 theta= [[-8.88954022]\n",
      " [ 0.07673728]\n",
      " [ 0.07038504]]\n",
      "the iterations of  277200 the loss is 0.28952717301461595 theta= [[-8.89119309]\n",
      " [ 0.07675023]\n",
      " [ 0.07039816]]\n",
      "the iterations of  277300 the loss is 0.2894998681608557 theta= [[-8.8928455 ]\n",
      " [ 0.07676319]\n",
      " [ 0.07041127]]\n",
      "the iterations of  277400 the loss is 0.28947257878001803 theta= [[-8.89449743]\n",
      " [ 0.07677613]\n",
      " [ 0.07042438]]\n",
      "the iterations of  277500 the loss is 0.2894453048587988 theta= [[-8.8961489 ]\n",
      " [ 0.07678908]\n",
      " [ 0.07043749]]\n",
      "the iterations of  277600 the loss is 0.289418046383909 theta= [[-8.8977999 ]\n",
      " [ 0.07680202]\n",
      " [ 0.07045059]]\n",
      "the iterations of  277700 the loss is 0.2893908033420754 theta= [[-8.89945043]\n",
      " [ 0.07681496]\n",
      " [ 0.07046369]]\n",
      "the iterations of  277800 the loss is 0.2893635757200397 theta= [[-8.9011005 ]\n",
      " [ 0.07682789]\n",
      " [ 0.07047679]]\n",
      "the iterations of  277900 the loss is 0.28933636350455916 theta= [[-8.9027501 ]\n",
      " [ 0.07684082]\n",
      " [ 0.07048988]]\n",
      "the iterations of  278000 the loss is 0.28930916668240614 theta= [[-8.90439923]\n",
      " [ 0.07685375]\n",
      " [ 0.07050297]]\n",
      "the iterations of  278100 the loss is 0.2892819852403681 theta= [[-8.90604789]\n",
      " [ 0.07686668]\n",
      " [ 0.07051605]]\n",
      "the iterations of  278200 the loss is 0.28925481916524787 theta= [[-8.90769609]\n",
      " [ 0.0768796 ]\n",
      " [ 0.07052913]]\n",
      "the iterations of  278300 the loss is 0.2892276684438637 theta= [[-8.90934383]\n",
      " [ 0.07689251]\n",
      " [ 0.07054221]]\n",
      "the iterations of  278400 the loss is 0.2892005330630486 theta= [[-8.9109911 ]\n",
      " [ 0.07690543]\n",
      " [ 0.07055528]]\n",
      "the iterations of  278500 the loss is 0.28917341300965105 theta= [[-8.9126379 ]\n",
      " [ 0.07691834]\n",
      " [ 0.07056835]]\n",
      "the iterations of  278600 the loss is 0.28914630827053484 theta= [[-8.91428424]\n",
      " [ 0.07693124]\n",
      " [ 0.07058142]]\n",
      "the iterations of  278700 the loss is 0.28911921883257824 theta= [[-8.91593011]\n",
      " [ 0.07694414]\n",
      " [ 0.07059448]]\n",
      "the iterations of  278800 the loss is 0.2890921446826751 theta= [[-8.91757552]\n",
      " [ 0.07695704]\n",
      " [ 0.07060754]]\n",
      "the iterations of  278900 the loss is 0.28906508580773416 theta= [[-8.91922046]\n",
      " [ 0.07696994]\n",
      " [ 0.0706206 ]]\n",
      "the iterations of  279000 the loss is 0.2890380421946792 theta= [[-8.92086494]\n",
      " [ 0.07698283]\n",
      " [ 0.07063365]]\n",
      "the iterations of  279100 the loss is 0.28901101383044925 theta= [[-8.92250896]\n",
      " [ 0.07699572]\n",
      " [ 0.0706467 ]]\n",
      "the iterations of  279200 the loss is 0.2889840007019979 theta= [[-8.92415251]\n",
      " [ 0.0770086 ]\n",
      " [ 0.07065974]]\n",
      "the iterations of  279300 the loss is 0.2889570027962945 theta= [[-8.9257956 ]\n",
      " [ 0.07702149]\n",
      " [ 0.07067279]]\n",
      "the iterations of  279400 the loss is 0.2889300201003224 theta= [[-8.92743823]\n",
      " [ 0.07703436]\n",
      " [ 0.07068582]]\n",
      "the iterations of  279500 the loss is 0.2889030526010806 theta= [[-8.92908039]\n",
      " [ 0.07704724]\n",
      " [ 0.07069886]]\n",
      "the iterations of  279600 the loss is 0.2888761002855826 theta= [[-8.93072209]\n",
      " [ 0.07706011]\n",
      " [ 0.07071189]]\n",
      "the iterations of  279700 the loss is 0.28884916314085696 theta= [[-8.93236333]\n",
      " [ 0.07707298]\n",
      " [ 0.07072491]]\n",
      "the iterations of  279800 the loss is 0.28882224115394733 theta= [[-8.93400411]\n",
      " [ 0.07708584]\n",
      " [ 0.07073794]]\n",
      "the iterations of  279900 the loss is 0.28879533431191184 theta= [[-8.93564443]\n",
      " [ 0.0770987 ]\n",
      " [ 0.07075096]]\n",
      "the iterations of  280000 the loss is 0.2887684426018238 theta= [[-8.93728428]\n",
      " [ 0.07711156]\n",
      " [ 0.07076397]]\n",
      "the iterations of  280100 the loss is 0.28874156601077106 theta= [[-8.93892368]\n",
      " [ 0.07712441]\n",
      " [ 0.07077698]]\n",
      "the iterations of  280200 the loss is 0.2887147045258562 theta= [[-8.94056261]\n",
      " [ 0.07713726]\n",
      " [ 0.07078999]]\n",
      "the iterations of  280300 the loss is 0.28868785813419673 theta= [[-8.94220108]\n",
      " [ 0.07715011]\n",
      " [ 0.070803  ]]\n",
      "the iterations of  280400 the loss is 0.28866102682292516 theta= [[-8.9438391 ]\n",
      " [ 0.07716295]\n",
      " [ 0.070816  ]]\n",
      "the iterations of  280500 the loss is 0.28863421057918837 theta= [[-8.94547665]\n",
      " [ 0.07717579]\n",
      " [ 0.070829  ]]\n",
      "the iterations of  280600 the loss is 0.28860740939014795 theta= [[-8.94711374]\n",
      " [ 0.07718863]\n",
      " [ 0.07084199]]\n",
      "the iterations of  280700 the loss is 0.28858062324298034 theta= [[-8.94875038]\n",
      " [ 0.07720146]\n",
      " [ 0.07085498]]\n",
      "the iterations of  280800 the loss is 0.2885538521248765 theta= [[-8.95038655]\n",
      " [ 0.07721429]\n",
      " [ 0.07086797]]\n",
      "the iterations of  280900 the loss is 0.28852709602304205 theta= [[-8.95202226]\n",
      " [ 0.07722711]\n",
      " [ 0.07088095]]\n",
      "the iterations of  281000 the loss is 0.2885003549246976 theta= [[-8.95365752]\n",
      " [ 0.07723994]\n",
      " [ 0.07089393]]\n",
      "the iterations of  281100 the loss is 0.28847362881707794 theta= [[-8.95529232]\n",
      " [ 0.07725275]\n",
      " [ 0.07090691]]\n",
      "the iterations of  281200 the loss is 0.2884469176874322 theta= [[-8.95692666]\n",
      " [ 0.07726557]\n",
      " [ 0.07091988]]\n",
      "the iterations of  281300 the loss is 0.2884202215230251 theta= [[-8.95856054]\n",
      " [ 0.07727838]\n",
      " [ 0.07093285]]\n",
      "the iterations of  281400 the loss is 0.2883935403111347 theta= [[-8.96019397]\n",
      " [ 0.07729119]\n",
      " [ 0.07094582]]\n",
      "the iterations of  281500 the loss is 0.28836687403905426 theta= [[-8.96182693]\n",
      " [ 0.07730399]\n",
      " [ 0.07095878]]\n",
      "the iterations of  281600 the loss is 0.2883402226940915 theta= [[-8.96345944]\n",
      " [ 0.07731679]\n",
      " [ 0.07097174]]\n",
      "the iterations of  281700 the loss is 0.28831358626356834 theta= [[-8.96509149]\n",
      " [ 0.07732959]\n",
      " [ 0.0709847 ]]\n",
      "the iterations of  281800 the loss is 0.2882869647348214 theta= [[-8.96672309]\n",
      " [ 0.07734239]\n",
      " [ 0.07099765]]\n",
      "the iterations of  281900 the loss is 0.28826035809520173 theta= [[-8.96835423]\n",
      " [ 0.07735518]\n",
      " [ 0.0710106 ]]\n",
      "the iterations of  282000 the loss is 0.2882337663320746 theta= [[-8.96998492]\n",
      " [ 0.07736796]\n",
      " [ 0.07102354]]\n",
      "the iterations of  282100 the loss is 0.2882071894328199 theta= [[-8.97161514]\n",
      " [ 0.07738075]\n",
      " [ 0.07103648]]\n",
      "the iterations of  282200 the loss is 0.28818062738483174 theta= [[-8.97324492]\n",
      " [ 0.07739353]\n",
      " [ 0.07104942]]\n",
      "the iterations of  282300 the loss is 0.28815408017551847 theta= [[-8.97487423]\n",
      " [ 0.07740631]\n",
      " [ 0.07106235]]\n",
      "the iterations of  282400 the loss is 0.2881275477923031 theta= [[-8.9765031 ]\n",
      " [ 0.07741908]\n",
      " [ 0.07107528]]\n",
      "the iterations of  282500 the loss is 0.28810103022262323 theta= [[-8.9781315 ]\n",
      " [ 0.07743185]\n",
      " [ 0.07108821]]\n",
      "the iterations of  282600 the loss is 0.2880745274539297 theta= [[-8.97975946]\n",
      " [ 0.07744462]\n",
      " [ 0.07110113]]\n",
      "the iterations of  282700 the loss is 0.2880480394736885 theta= [[-8.98138695]\n",
      " [ 0.07745738]\n",
      " [ 0.07111405]]\n",
      "the iterations of  282800 the loss is 0.2880215662693799 theta= [[-8.983014  ]\n",
      " [ 0.07747014]\n",
      " [ 0.07112697]]\n",
      "the iterations of  282900 the loss is 0.2879951078284975 theta= [[-8.98464059]\n",
      " [ 0.07748289]\n",
      " [ 0.07113988]]\n",
      "the iterations of  283000 the loss is 0.2879686641385501 theta= [[-8.98626673]\n",
      " [ 0.07749565]\n",
      " [ 0.07115279]]\n",
      "the iterations of  283100 the loss is 0.2879422351870605 theta= [[-8.98789241]\n",
      " [ 0.0775084 ]\n",
      " [ 0.07116569]]\n",
      "the iterations of  283200 the loss is 0.2879158209615653 theta= [[-8.98951764]\n",
      " [ 0.07752114]\n",
      " [ 0.0711786 ]]\n",
      "the iterations of  283300 the loss is 0.28788942144961527 theta= [[-8.99114242]\n",
      " [ 0.07753388]\n",
      " [ 0.07119149]]\n",
      "the iterations of  283400 the loss is 0.2878630366387756 theta= [[-8.99276675]\n",
      " [ 0.07754662]\n",
      " [ 0.07120439]]\n",
      "the iterations of  283500 the loss is 0.28783666651662565 theta= [[-8.99439062]\n",
      " [ 0.07755936]\n",
      " [ 0.07121728]]\n",
      "the iterations of  283600 the loss is 0.2878103110707588 theta= [[-8.99601404]\n",
      " [ 0.07757209]\n",
      " [ 0.07123017]]\n",
      "the iterations of  283700 the loss is 0.28778397028878205 theta= [[-8.99763701]\n",
      " [ 0.07758482]\n",
      " [ 0.07124305]]\n",
      "the iterations of  283800 the loss is 0.287757644158317 theta= [[-8.99925953]\n",
      " [ 0.07759754]\n",
      " [ 0.07125593]]\n",
      "the iterations of  283900 the loss is 0.2877313326669994 theta= [[-9.0008816 ]\n",
      " [ 0.07761027]\n",
      " [ 0.07126881]]\n",
      "the iterations of  284000 the loss is 0.28770503580247847 theta= [[-9.00250321]\n",
      " [ 0.07762298]\n",
      " [ 0.07128169]]\n",
      "the iterations of  284100 the loss is 0.28767875355241757 theta= [[-9.00412438]\n",
      " [ 0.0776357 ]\n",
      " [ 0.07129456]]\n",
      "the iterations of  284200 the loss is 0.2876524859044943 theta= [[-9.0057451 ]\n",
      " [ 0.07764841]\n",
      " [ 0.07130742]]\n",
      "the iterations of  284300 the loss is 0.2876262328464 theta= [[-9.00736536]\n",
      " [ 0.07766112]\n",
      " [ 0.07132029]]\n",
      "the iterations of  284400 the loss is 0.28759999436583983 theta= [[-9.00898518]\n",
      " [ 0.07767382]\n",
      " [ 0.07133315]]\n",
      "the iterations of  284500 the loss is 0.28757377045053323 theta= [[-9.01060454]\n",
      " [ 0.07768652]\n",
      " [ 0.071346  ]]\n",
      "the iterations of  284600 the loss is 0.28754756108821317 theta= [[-9.01222346]\n",
      " [ 0.07769922]\n",
      " [ 0.07135885]]\n",
      "the iterations of  284700 the loss is 0.2875213662666268 theta= [[-9.01384192]\n",
      " [ 0.07771192]\n",
      " [ 0.0713717 ]]\n",
      "the iterations of  284800 the loss is 0.28749518597353485 theta= [[-9.01545994]\n",
      " [ 0.07772461]\n",
      " [ 0.07138455]]\n",
      "the iterations of  284900 the loss is 0.2874690201967121 theta= [[-9.01707751]\n",
      " [ 0.07773729]\n",
      " [ 0.07139739]]\n",
      "the iterations of  285000 the loss is 0.287442868923947 theta= [[-9.01869463]\n",
      " [ 0.07774998]\n",
      " [ 0.07141023]]\n",
      "the iterations of  285100 the loss is 0.28741673214304186 theta= [[-9.02031131]\n",
      " [ 0.07776266]\n",
      " [ 0.07142307]]\n",
      "the iterations of  285200 the loss is 0.28739060984181275 theta= [[-9.02192753]\n",
      " [ 0.07777534]\n",
      " [ 0.0714359 ]]\n",
      "the iterations of  285300 the loss is 0.2873645020080897 theta= [[-9.02354331]\n",
      " [ 0.07778801]\n",
      " [ 0.07144873]]\n",
      "the iterations of  285400 the loss is 0.28733840862971627 theta= [[-9.02515864]\n",
      " [ 0.07780068]\n",
      " [ 0.07146155]]\n",
      "the iterations of  285500 the loss is 0.28731232969454945 theta= [[-9.02677352]\n",
      " [ 0.07781335]\n",
      " [ 0.07147437]]\n",
      "the iterations of  285600 the loss is 0.28728626519046047 theta= [[-9.02838796]\n",
      " [ 0.07782601]\n",
      " [ 0.07148719]]\n",
      "the iterations of  285700 the loss is 0.287260215105334 theta= [[-9.03000195]\n",
      " [ 0.07783867]\n",
      " [ 0.0715    ]]\n",
      "the iterations of  285800 the loss is 0.28723417942706836 theta= [[-9.03161549]\n",
      " [ 0.07785133]\n",
      " [ 0.07151281]]\n",
      "the iterations of  285900 the loss is 0.28720815814357564 theta= [[-9.03322859]\n",
      " [ 0.07786398]\n",
      " [ 0.07152562]]\n",
      "the iterations of  286000 the loss is 0.28718215124278124 theta= [[-9.03484124]\n",
      " [ 0.07787663]\n",
      " [ 0.07153843]]\n",
      "the iterations of  286100 the loss is 0.2871561587126245 theta= [[-9.03645344]\n",
      " [ 0.07788928]\n",
      " [ 0.07155123]]\n",
      "the iterations of  286200 the loss is 0.28713018054105804 theta= [[-9.0380652 ]\n",
      " [ 0.07790192]\n",
      " [ 0.07156402]]\n",
      "the iterations of  286300 the loss is 0.28710421671604835 theta= [[-9.03967652]\n",
      " [ 0.07791456]\n",
      " [ 0.07157682]]\n",
      "the iterations of  286400 the loss is 0.2870782672255757 theta= [[-9.04128739]\n",
      " [ 0.0779272 ]\n",
      " [ 0.07158961]]\n",
      "the iterations of  286500 the loss is 0.2870523320576332 theta= [[-9.04289782]\n",
      " [ 0.07793983]\n",
      " [ 0.07160239]]\n",
      "the iterations of  286600 the loss is 0.2870264112002276 theta= [[-9.0445078 ]\n",
      " [ 0.07795246]\n",
      " [ 0.07161518]]\n",
      "the iterations of  286700 the loss is 0.2870005046413796 theta= [[-9.04611733]\n",
      " [ 0.07796509]\n",
      " [ 0.07162796]]\n",
      "the iterations of  286800 the loss is 0.2869746123691231 theta= [[-9.04772643]\n",
      " [ 0.07797771]\n",
      " [ 0.07164073]]\n",
      "the iterations of  286900 the loss is 0.28694873437150537 theta= [[-9.04933508]\n",
      " [ 0.07799033]\n",
      " [ 0.0716535 ]]\n",
      "the iterations of  287000 the loss is 0.28692287063658717 theta= [[-9.05094328]\n",
      " [ 0.07800295]\n",
      " [ 0.07166627]]\n",
      "the iterations of  287100 the loss is 0.28689702115244264 theta= [[-9.05255104]\n",
      " [ 0.07801556]\n",
      " [ 0.07167904]]\n",
      "the iterations of  287200 the loss is 0.2868711859071597 theta= [[-9.05415837]\n",
      " [ 0.07802817]\n",
      " [ 0.0716918 ]]\n",
      "the iterations of  287300 the loss is 0.28684536488883894 theta= [[-9.05576524]\n",
      " [ 0.07804078]\n",
      " [ 0.07170456]]\n",
      "the iterations of  287400 the loss is 0.286819558085595 theta= [[-9.05737168]\n",
      " [ 0.07805338]\n",
      " [ 0.07171732]]\n",
      "the iterations of  287500 the loss is 0.28679376548555546 theta= [[-9.05897767]\n",
      " [ 0.07806598]\n",
      " [ 0.07173007]]\n",
      "the iterations of  287600 the loss is 0.2867679870768612 theta= [[-9.06058322]\n",
      " [ 0.07807858]\n",
      " [ 0.07174282]]\n",
      "the iterations of  287700 the loss is 0.28674222284766676 theta= [[-9.06218833]\n",
      " [ 0.07809117]\n",
      " [ 0.07175556]]\n",
      "the iterations of  287800 the loss is 0.28671647278613954 theta= [[-9.063793  ]\n",
      " [ 0.07810376]\n",
      " [ 0.0717683 ]]\n",
      "the iterations of  287900 the loss is 0.2866907368804603 theta= [[-9.06539723]\n",
      " [ 0.07811634]\n",
      " [ 0.07178104]]\n",
      "the iterations of  288000 the loss is 0.28666501511882336 theta= [[-9.06700101]\n",
      " [ 0.07812893]\n",
      " [ 0.07179378]]\n",
      "the iterations of  288100 the loss is 0.2866393074894356 theta= [[-9.06860436]\n",
      " [ 0.07814151]\n",
      " [ 0.07180651]]\n",
      "the iterations of  288200 the loss is 0.28661361398051816 theta= [[-9.07020726]\n",
      " [ 0.07815408]\n",
      " [ 0.07181924]]\n",
      "the iterations of  288300 the loss is 0.2865879345803043 theta= [[-9.07180973]\n",
      " [ 0.07816665]\n",
      " [ 0.07183196]]\n",
      "the iterations of  288400 the loss is 0.286562269277041 theta= [[-9.07341175]\n",
      " [ 0.07817922]\n",
      " [ 0.07184468]]\n",
      "the iterations of  288500 the loss is 0.28653661805898845 theta= [[-9.07501334]\n",
      " [ 0.07819179]\n",
      " [ 0.0718574 ]]\n",
      "the iterations of  288600 the loss is 0.28651098091441984 theta= [[-9.07661448]\n",
      " [ 0.07820435]\n",
      " [ 0.07187011]]\n",
      "the iterations of  288700 the loss is 0.28648535783162127 theta= [[-9.07821519]\n",
      " [ 0.07821691]\n",
      " [ 0.07188282]]\n",
      "the iterations of  288800 the loss is 0.2864597487988921 theta= [[-9.07981546]\n",
      " [ 0.07822947]\n",
      " [ 0.07189553]]\n",
      "the iterations of  288900 the loss is 0.2864341538045451 theta= [[-9.08141529]\n",
      " [ 0.07824202]\n",
      " [ 0.07190823]]\n",
      "the iterations of  289000 the loss is 0.2864085728369055 theta= [[-9.08301468]\n",
      " [ 0.07825457]\n",
      " [ 0.07192093]]\n",
      "the iterations of  289100 the loss is 0.2863830058843121 theta= [[-9.08461363]\n",
      " [ 0.07826711]\n",
      " [ 0.07193363]]\n",
      "the iterations of  289200 the loss is 0.28635745293511633 theta= [[-9.08621214]\n",
      " [ 0.07827966]\n",
      " [ 0.07194633]]\n",
      "the iterations of  289300 the loss is 0.2863319139776828 theta= [[-9.08781022]\n",
      " [ 0.0782922 ]\n",
      " [ 0.07195902]]\n",
      "the iterations of  289400 the loss is 0.2863063890003894 theta= [[-9.08940786]\n",
      " [ 0.07830473]\n",
      " [ 0.0719717 ]]\n",
      "the iterations of  289500 the loss is 0.28628087799162616 theta= [[-9.09100506]\n",
      " [ 0.07831726]\n",
      " [ 0.07198439]]\n",
      "the iterations of  289600 the loss is 0.2862553809397971 theta= [[-9.09260183]\n",
      " [ 0.07832979]\n",
      " [ 0.07199707]]\n",
      "the iterations of  289700 the loss is 0.2862298978333185 theta= [[-9.09419815]\n",
      " [ 0.07834232]\n",
      " [ 0.07200974]]\n",
      "the iterations of  289800 the loss is 0.2862044286606195 theta= [[-9.09579405]\n",
      " [ 0.07835484]\n",
      " [ 0.07202242]]\n",
      "the iterations of  289900 the loss is 0.28617897341014314 theta= [[-9.0973895 ]\n",
      " [ 0.07836736]\n",
      " [ 0.07203509]]\n",
      "the iterations of  290000 the loss is 0.28615353207034383 theta= [[-9.09898452]\n",
      " [ 0.07837988]\n",
      " [ 0.07204775]]\n",
      "the iterations of  290100 the loss is 0.28612810462968985 theta= [[-9.1005791 ]\n",
      " [ 0.07839239]\n",
      " [ 0.07206041]]\n",
      "the iterations of  290200 the loss is 0.28610269107666214 theta= [[-9.10217325]\n",
      " [ 0.0784049 ]\n",
      " [ 0.07207307]]\n",
      "the iterations of  290300 the loss is 0.28607729139975424 theta= [[-9.10376697]\n",
      " [ 0.0784174 ]\n",
      " [ 0.07208573]]\n",
      "the iterations of  290400 the loss is 0.28605190558747284 theta= [[-9.10536024]\n",
      " [ 0.07842991]\n",
      " [ 0.07209838]]\n",
      "the iterations of  290500 the loss is 0.28602653362833697 theta= [[-9.10695309]\n",
      " [ 0.07844241]\n",
      " [ 0.07211103]]\n",
      "the iterations of  290600 the loss is 0.28600117551087884 theta= [[-9.10854549]\n",
      " [ 0.0784549 ]\n",
      " [ 0.07212368]]\n",
      "the iterations of  290700 the loss is 0.28597583122364334 theta= [[-9.11013747]\n",
      " [ 0.0784674 ]\n",
      " [ 0.07213632]]\n",
      "the iterations of  290800 the loss is 0.28595050075518796 theta= [[-9.11172901]\n",
      " [ 0.07847988]\n",
      " [ 0.07214896]]\n",
      "the iterations of  290900 the loss is 0.2859251840940828 theta= [[-9.11332012]\n",
      " [ 0.07849237]\n",
      " [ 0.0721616 ]]\n",
      "the iterations of  291000 the loss is 0.2858998812289114 theta= [[-9.11491079]\n",
      " [ 0.07850485]\n",
      " [ 0.07217423]]\n",
      "the iterations of  291100 the loss is 0.285874592148269 theta= [[-9.11650103]\n",
      " [ 0.07851733]\n",
      " [ 0.07218686]]\n",
      "the iterations of  291200 the loss is 0.28584931684076403 theta= [[-9.11809083]\n",
      " [ 0.07852981]\n",
      " [ 0.07219948]]\n",
      "the iterations of  291300 the loss is 0.28582405529501725 theta= [[-9.11968021]\n",
      " [ 0.07854228]\n",
      " [ 0.07221211]]\n",
      "the iterations of  291400 the loss is 0.2857988074996627 theta= [[-9.12126915]\n",
      " [ 0.07855475]\n",
      " [ 0.07222473]]\n",
      "the iterations of  291500 the loss is 0.28577357344334636 theta= [[-9.12285766]\n",
      " [ 0.07856722]\n",
      " [ 0.07223734]]\n",
      "the iterations of  291600 the loss is 0.2857483531147271 theta= [[-9.12444573]\n",
      " [ 0.07857968]\n",
      " [ 0.07224995]]\n",
      "the iterations of  291700 the loss is 0.28572314650247643 theta= [[-9.12603338]\n",
      " [ 0.07859214]\n",
      " [ 0.07226256]]\n",
      "the iterations of  291800 the loss is 0.2856979535952784 theta= [[-9.12762059]\n",
      " [ 0.0786046 ]\n",
      " [ 0.07227517]]\n",
      "the iterations of  291900 the loss is 0.28567277438182936 theta= [[-9.12920737]\n",
      " [ 0.07861705]\n",
      " [ 0.07228777]]\n",
      "the iterations of  292000 the loss is 0.2856476088508385 theta= [[-9.13079372]\n",
      " [ 0.0786295 ]\n",
      " [ 0.07230037]]\n",
      "the iterations of  292100 the loss is 0.28562245699102723 theta= [[-9.13237964]\n",
      " [ 0.07864195]\n",
      " [ 0.07231296]]\n",
      "the iterations of  292200 the loss is 0.28559731879112965 theta= [[-9.13396513]\n",
      " [ 0.07865439]\n",
      " [ 0.07232556]]\n",
      "the iterations of  292300 the loss is 0.28557219423989244 theta= [[-9.13555019]\n",
      " [ 0.07866683]\n",
      " [ 0.07233814]]\n",
      "the iterations of  292400 the loss is 0.2855470833260745 theta= [[-9.13713481]\n",
      " [ 0.07867927]\n",
      " [ 0.07235073]]\n",
      "the iterations of  292500 the loss is 0.2855219860384474 theta= [[-9.13871901]\n",
      " [ 0.0786917 ]\n",
      " [ 0.07236331]]\n",
      "the iterations of  292600 the loss is 0.2854969023657948 theta= [[-9.14030278]\n",
      " [ 0.07870413]\n",
      " [ 0.07237589]]\n",
      "the iterations of  292700 the loss is 0.2854718322969132 theta= [[-9.14188612]\n",
      " [ 0.07871656]\n",
      " [ 0.07238847]]\n",
      "the iterations of  292800 the loss is 0.28544677582061095 theta= [[-9.14346903]\n",
      " [ 0.07872898]\n",
      " [ 0.07240104]]\n",
      "the iterations of  292900 the loss is 0.28542173292570955 theta= [[-9.14505151]\n",
      " [ 0.0787414 ]\n",
      " [ 0.07241361]]\n",
      "the iterations of  293000 the loss is 0.2853967036010419 theta= [[-9.14663356]\n",
      " [ 0.07875382]\n",
      " [ 0.07242617]]\n",
      "the iterations of  293100 the loss is 0.2853716878354541 theta= [[-9.14821518]\n",
      " [ 0.07876623]\n",
      " [ 0.07243873]]\n",
      "the iterations of  293200 the loss is 0.2853466856178039 theta= [[-9.14979637]\n",
      " [ 0.07877864]\n",
      " [ 0.07245129]]\n",
      "the iterations of  293300 the loss is 0.28532169693696185 theta= [[-9.15137714]\n",
      " [ 0.07879105]\n",
      " [ 0.07246385]]\n",
      "the iterations of  293400 the loss is 0.2852967217818104 theta= [[-9.15295748]\n",
      " [ 0.07880346]\n",
      " [ 0.0724764 ]]\n",
      "the iterations of  293500 the loss is 0.28527176014124506 theta= [[-9.15453739]\n",
      " [ 0.07881586]\n",
      " [ 0.07248895]]\n",
      "the iterations of  293600 the loss is 0.28524681200417223 theta= [[-9.15611687]\n",
      " [ 0.07882825]\n",
      " [ 0.07250149]]\n",
      "the iterations of  293700 the loss is 0.2852218773595119 theta= [[-9.15769593]\n",
      " [ 0.07884065]\n",
      " [ 0.07251403]]\n",
      "the iterations of  293800 the loss is 0.2851969561961954 theta= [[-9.15927456]\n",
      " [ 0.07885304]\n",
      " [ 0.07252657]]\n",
      "the iterations of  293900 the loss is 0.2851720485031667 theta= [[-9.16085276]\n",
      " [ 0.07886543]\n",
      " [ 0.07253911]]\n",
      "the iterations of  294000 the loss is 0.2851471542693817 theta= [[-9.16243053]\n",
      " [ 0.07887781]\n",
      " [ 0.07255164]]\n",
      "the iterations of  294100 the loss is 0.2851222734838087 theta= [[-9.16400788]\n",
      " [ 0.07889019]\n",
      " [ 0.07256417]]\n",
      "the iterations of  294200 the loss is 0.28509740613542794 theta= [[-9.16558481]\n",
      " [ 0.07890257]\n",
      " [ 0.07257669]]\n",
      "the iterations of  294300 the loss is 0.28507255221323197 theta= [[-9.1671613 ]\n",
      " [ 0.07891495]\n",
      " [ 0.07258922]]\n",
      "the iterations of  294400 the loss is 0.28504771170622545 theta= [[-9.16873738]\n",
      " [ 0.07892732]\n",
      " [ 0.07260174]]\n",
      "the iterations of  294500 the loss is 0.285022884603425 theta= [[-9.17031302]\n",
      " [ 0.07893969]\n",
      " [ 0.07261425]]\n",
      "the iterations of  294600 the loss is 0.2849980708938595 theta= [[-9.17188825]\n",
      " [ 0.07895205]\n",
      " [ 0.07262676]]\n",
      "the iterations of  294700 the loss is 0.2849732705665697 theta= [[-9.17346304]\n",
      " [ 0.07896441]\n",
      " [ 0.07263927]]\n",
      "the iterations of  294800 the loss is 0.28494848361060854 theta= [[-9.17503741]\n",
      " [ 0.07897677]\n",
      " [ 0.07265178]]\n",
      "the iterations of  294900 the loss is 0.2849237100150411 theta= [[-9.17661136]\n",
      " [ 0.07898913]\n",
      " [ 0.07266428]]\n",
      "the iterations of  295000 the loss is 0.28489894976894414 theta= [[-9.17818489]\n",
      " [ 0.07900148]\n",
      " [ 0.07267678]]\n",
      "the iterations of  295100 the loss is 0.2848742028614068 theta= [[-9.17975799]\n",
      " [ 0.07901383]\n",
      " [ 0.07268927]]\n",
      "the iterations of  295200 the loss is 0.2848494692815298 theta= [[-9.18133066]\n",
      " [ 0.07902617]\n",
      " [ 0.07270176]]\n",
      "the iterations of  295300 the loss is 0.2848247490184266 theta= [[-9.18290292]\n",
      " [ 0.07903852]\n",
      " [ 0.07271425]]\n",
      "the iterations of  295400 the loss is 0.28480004206122184 theta= [[-9.18447475]\n",
      " [ 0.07905085]\n",
      " [ 0.07272674]]\n",
      "the iterations of  295500 the loss is 0.28477534839905233 theta= [[-9.18604615]\n",
      " [ 0.07906319]\n",
      " [ 0.07273922]]\n",
      "the iterations of  295600 the loss is 0.2847506680210665 theta= [[-9.18761714]\n",
      " [ 0.07907552]\n",
      " [ 0.0727517 ]]\n",
      "the iterations of  295700 the loss is 0.2847260009164256 theta= [[-9.1891877 ]\n",
      " [ 0.07908785]\n",
      " [ 0.07276418]]\n",
      "the iterations of  295800 the loss is 0.28470134707430184 theta= [[-9.19075784]\n",
      " [ 0.07910018]\n",
      " [ 0.07277665]]\n",
      "the iterations of  295900 the loss is 0.2846767064838796 theta= [[-9.19232755]\n",
      " [ 0.0791125 ]\n",
      " [ 0.07278912]]\n",
      "the iterations of  296000 the loss is 0.2846520791343554 theta= [[-9.19389685]\n",
      " [ 0.07912482]\n",
      " [ 0.07280158]]\n",
      "the iterations of  296100 the loss is 0.2846274650149371 theta= [[-9.19546572]\n",
      " [ 0.07913714]\n",
      " [ 0.07281405]]\n",
      "the iterations of  296200 the loss is 0.28460286411484487 theta= [[-9.19703418]\n",
      " [ 0.07914945]\n",
      " [ 0.0728265 ]]\n",
      "the iterations of  296300 the loss is 0.28457827642331024 theta= [[-9.19860221]\n",
      " [ 0.07916176]\n",
      " [ 0.07283896]]\n",
      "the iterations of  296400 the loss is 0.28455370192957674 theta= [[-9.20016982]\n",
      " [ 0.07917407]\n",
      " [ 0.07285141]]\n",
      "the iterations of  296500 the loss is 0.28452914062289997 theta= [[-9.20173701]\n",
      " [ 0.07918637]\n",
      " [ 0.07286386]]\n",
      "the iterations of  296600 the loss is 0.28450459249254684 theta= [[-9.20330378]\n",
      " [ 0.07919867]\n",
      " [ 0.07287631]]\n",
      "the iterations of  296700 the loss is 0.2844800575277961 theta= [[-9.20487013]\n",
      " [ 0.07921097]\n",
      " [ 0.07288875]]\n",
      "the iterations of  296800 the loss is 0.28445553571793847 theta= [[-9.20643605]\n",
      " [ 0.07922327]\n",
      " [ 0.07290119]]\n",
      "the iterations of  296900 the loss is 0.2844310270522762 theta= [[-9.20800156]\n",
      " [ 0.07923556]\n",
      " [ 0.07291363]]\n",
      "the iterations of  297000 the loss is 0.28440653152012285 theta= [[-9.20956665]\n",
      " [ 0.07924785]\n",
      " [ 0.07292606]]\n",
      "the iterations of  297100 the loss is 0.2843820491108047 theta= [[-9.21113132]\n",
      " [ 0.07926013]\n",
      " [ 0.07293849]]\n",
      "the iterations of  297200 the loss is 0.2843575798136586 theta= [[-9.21269558]\n",
      " [ 0.07927241]\n",
      " [ 0.07295092]]\n",
      "the iterations of  297300 the loss is 0.28433312361803365 theta= [[-9.21425941]\n",
      " [ 0.07928469]\n",
      " [ 0.07296334]]\n",
      "the iterations of  297400 the loss is 0.28430868051329045 theta= [[-9.21582282]\n",
      " [ 0.07929696]\n",
      " [ 0.07297576]]\n",
      "the iterations of  297500 the loss is 0.284284250488801 theta= [[-9.21738582]\n",
      " [ 0.07930924]\n",
      " [ 0.07298818]]\n",
      "the iterations of  297600 the loss is 0.28425983353394946 theta= [[-9.21894839]\n",
      " [ 0.07932151]\n",
      " [ 0.07300059]]\n",
      "the iterations of  297700 the loss is 0.28423542963813103 theta= [[-9.22051055]\n",
      " [ 0.07933377]\n",
      " [ 0.073013  ]]\n",
      "the iterations of  297800 the loss is 0.2842110387907525 theta= [[-9.2220723 ]\n",
      " [ 0.07934603]\n",
      " [ 0.07302541]]\n",
      "the iterations of  297900 the loss is 0.2841866609812324 theta= [[-9.22363362]\n",
      " [ 0.07935829]\n",
      " [ 0.07303781]]\n",
      "the iterations of  298000 the loss is 0.28416229619900096 theta= [[-9.22519453]\n",
      " [ 0.07937055]\n",
      " [ 0.07305021]]\n",
      "the iterations of  298100 the loss is 0.28413794443349966 theta= [[-9.22675502]\n",
      " [ 0.0793828 ]\n",
      " [ 0.07306261]]\n",
      "the iterations of  298200 the loss is 0.2841136056741815 theta= [[-9.22831509]\n",
      " [ 0.07939505]\n",
      " [ 0.073075  ]]\n",
      "the iterations of  298300 the loss is 0.2840892799105112 theta= [[-9.22987475]\n",
      " [ 0.0794073 ]\n",
      " [ 0.07308739]]\n",
      "the iterations of  298400 the loss is 0.28406496713196455 theta= [[-9.23143399]\n",
      " [ 0.07941954]\n",
      " [ 0.07309978]]\n",
      "the iterations of  298500 the loss is 0.28404066732802924 theta= [[-9.23299281]\n",
      " [ 0.07943178]\n",
      " [ 0.07311216]]\n",
      "the iterations of  298600 the loss is 0.2840163804882041 theta= [[-9.23455122]\n",
      " [ 0.07944402]\n",
      " [ 0.07312454]]\n",
      "the iterations of  298700 the loss is 0.28399210660199964 theta= [[-9.23610921]\n",
      " [ 0.07945625]\n",
      " [ 0.07313692]]\n",
      "the iterations of  298800 the loss is 0.28396784565893735 theta= [[-9.23766679]\n",
      " [ 0.07946848]\n",
      " [ 0.0731493 ]]\n",
      "the iterations of  298900 the loss is 0.28394359764855065 theta= [[-9.23922395]\n",
      " [ 0.07948071]\n",
      " [ 0.07316167]]\n",
      "the iterations of  299000 the loss is 0.28391936256038386 theta= [[-9.2407807 ]\n",
      " [ 0.07949293]\n",
      " [ 0.07317403]]\n",
      "the iterations of  299100 the loss is 0.28389514038399316 theta= [[-9.24233703]\n",
      " [ 0.07950515]\n",
      " [ 0.0731864 ]]\n",
      "the iterations of  299200 the loss is 0.2838709311089458 theta= [[-9.24389295]\n",
      " [ 0.07951737]\n",
      " [ 0.07319876]]\n",
      "the iterations of  299300 the loss is 0.2838467347248202 theta= [[-9.24544845]\n",
      " [ 0.07952959]\n",
      " [ 0.07321112]]\n",
      "the iterations of  299400 the loss is 0.2838225512212064 theta= [[-9.24700354]\n",
      " [ 0.0795418 ]\n",
      " [ 0.07322347]]\n",
      "the iterations of  299500 the loss is 0.28379838058770557 theta= [[-9.24855822]\n",
      " [ 0.07955401]\n",
      " [ 0.07323583]]\n",
      "the iterations of  299600 the loss is 0.2837742228139302 theta= [[-9.25011248]\n",
      " [ 0.07956621]\n",
      " [ 0.07324817]]\n",
      "the iterations of  299700 the loss is 0.2837500778895042 theta= [[-9.25166633]\n",
      " [ 0.07957841]\n",
      " [ 0.07326052]]\n",
      "the iterations of  299800 the loss is 0.28372594580406246 theta= [[-9.25321976]\n",
      " [ 0.07959061]\n",
      " [ 0.07327286]]\n",
      "the iterations of  299900 the loss is 0.2837018265472514 theta= [[-9.25477279]\n",
      " [ 0.07960281]\n",
      " [ 0.0732852 ]]\n",
      "the iterations of  300000 the loss is 0.2836777201087284 theta= [[-9.2563254 ]\n",
      " [ 0.079615  ]\n",
      " [ 0.07329754]]\n",
      "the iterations of  300100 the loss is 0.28365362647816234 theta= [[-9.25787759]\n",
      " [ 0.07962719]\n",
      " [ 0.07330987]]\n",
      "the iterations of  300200 the loss is 0.283629545645233 theta= [[-9.25942938]\n",
      " [ 0.07963938]\n",
      " [ 0.0733222 ]]\n",
      "the iterations of  300300 the loss is 0.28360547759963156 theta= [[-9.26098075]\n",
      " [ 0.07965156]\n",
      " [ 0.07333452]]\n",
      "the iterations of  300400 the loss is 0.2835814223310602 theta= [[-9.26253171]\n",
      " [ 0.07966374]\n",
      " [ 0.07334685]]\n",
      "the iterations of  300500 the loss is 0.28355737982923246 theta= [[-9.26408226]\n",
      " [ 0.07967592]\n",
      " [ 0.07335917]]\n",
      "the iterations of  300600 the loss is 0.28353335008387287 theta= [[-9.2656324 ]\n",
      " [ 0.07968809]\n",
      " [ 0.07337148]]\n",
      "the iterations of  300700 the loss is 0.2835093330847171 theta= [[-9.26718213]\n",
      " [ 0.07970026]\n",
      " [ 0.0733838 ]]\n",
      "the iterations of  300800 the loss is 0.28348532882151206 theta= [[-9.26873144]\n",
      " [ 0.07971243]\n",
      " [ 0.07339611]]\n",
      "the iterations of  300900 the loss is 0.28346133728401546 theta= [[-9.27028035]\n",
      " [ 0.07972459]\n",
      " [ 0.07340841]]\n",
      "the iterations of  301000 the loss is 0.2834373584619963 theta= [[-9.27182884]\n",
      " [ 0.07973676]\n",
      " [ 0.07342072]]\n",
      "the iterations of  301100 the loss is 0.28341339234523455 theta= [[-9.27337692]\n",
      " [ 0.07974891]\n",
      " [ 0.07343302]]\n",
      "the iterations of  301200 the loss is 0.2833894389235214 theta= [[-9.2749246 ]\n",
      " [ 0.07976107]\n",
      " [ 0.07344531]]\n",
      "the iterations of  301300 the loss is 0.2833654981866589 theta= [[-9.27647186]\n",
      " [ 0.07977322]\n",
      " [ 0.07345761]]\n",
      "the iterations of  301400 the loss is 0.28334157012446015 theta= [[-9.27801872]\n",
      " [ 0.07978537]\n",
      " [ 0.0734699 ]]\n",
      "the iterations of  301500 the loss is 0.2833176547267491 theta= [[-9.27956516]\n",
      " [ 0.07979751]\n",
      " [ 0.07348218]]\n",
      "the iterations of  301600 the loss is 0.2832937519833611 theta= [[-9.2811112 ]\n",
      " [ 0.07980966]\n",
      " [ 0.07349447]]\n",
      "the iterations of  301700 the loss is 0.28326986188414227 theta= [[-9.28265683]\n",
      " [ 0.0798218 ]\n",
      " [ 0.07350675]]\n",
      "the iterations of  301800 the loss is 0.2832459844189495 theta= [[-9.28420204]\n",
      " [ 0.07983393]\n",
      " [ 0.07351903]]\n",
      "the iterations of  301900 the loss is 0.28322211957765075 theta= [[-9.28574685]\n",
      " [ 0.07984607]\n",
      " [ 0.0735313 ]]\n",
      "the iterations of  302000 the loss is 0.28319826735012504 theta= [[-9.28729125]\n",
      " [ 0.0798582 ]\n",
      " [ 0.07354357]]\n",
      "the iterations of  302100 the loss is 0.28317442772626206 theta= [[-9.28883525]\n",
      " [ 0.07987032]\n",
      " [ 0.07355584]]\n",
      "the iterations of  302200 the loss is 0.2831506006959625 theta= [[-9.29037883]\n",
      " [ 0.07988245]\n",
      " [ 0.07356811]]\n",
      "the iterations of  302300 the loss is 0.28312678624913823 theta= [[-9.29192201]\n",
      " [ 0.07989457]\n",
      " [ 0.07358037]]\n",
      "the iterations of  302400 the loss is 0.2831029843757113 theta= [[-9.29346478]\n",
      " [ 0.07990668]\n",
      " [ 0.07359263]]\n",
      "the iterations of  302500 the loss is 0.2830791950656155 theta= [[-9.29500714]\n",
      " [ 0.0799188 ]\n",
      " [ 0.07360488]]\n",
      "the iterations of  302600 the loss is 0.2830554183087947 theta= [[-9.2965491 ]\n",
      " [ 0.07993091]\n",
      " [ 0.07361714]]\n",
      "the iterations of  302700 the loss is 0.2830316540952038 theta= [[-9.29809065]\n",
      " [ 0.07994302]\n",
      " [ 0.07362939]]\n",
      "the iterations of  302800 the loss is 0.28300790241480855 theta= [[-9.29963179]\n",
      " [ 0.07995512]\n",
      " [ 0.07364163]]\n",
      "the iterations of  302900 the loss is 0.2829841632575861 theta= [[-9.30117253]\n",
      " [ 0.07996723]\n",
      " [ 0.07365388]]\n",
      "the iterations of  303000 the loss is 0.28296043661352344 theta= [[-9.30271286]\n",
      " [ 0.07997932]\n",
      " [ 0.07366612]]\n",
      "the iterations of  303100 the loss is 0.28293672247261864 theta= [[-9.30425278]\n",
      " [ 0.07999142]\n",
      " [ 0.07367835]]\n",
      "the iterations of  303200 the loss is 0.28291302082488085 theta= [[-9.3057923 ]\n",
      " [ 0.08000351]\n",
      " [ 0.07369059]]\n",
      "the iterations of  303300 the loss is 0.28288933166032937 theta= [[-9.30733141]\n",
      " [ 0.0800156 ]\n",
      " [ 0.07370282]]\n",
      "the iterations of  303400 the loss is 0.2828656549689948 theta= [[-9.30887012]\n",
      " [ 0.08002769]\n",
      " [ 0.07371504]]\n",
      "the iterations of  303500 the loss is 0.28284199074091804 theta= [[-9.31040842]\n",
      " [ 0.08003977]\n",
      " [ 0.07372727]]\n",
      "the iterations of  303600 the loss is 0.2828183389661511 theta= [[-9.31194632]\n",
      " [ 0.08005185]\n",
      " [ 0.07373949]]\n",
      "the iterations of  303700 the loss is 0.2827946996347563 theta= [[-9.31348382]\n",
      " [ 0.08006393]\n",
      " [ 0.07375171]]\n",
      "the iterations of  303800 the loss is 0.28277107273680674 theta= [[-9.3150209 ]\n",
      " [ 0.080076  ]\n",
      " [ 0.07376392]]\n",
      "the iterations of  303900 the loss is 0.2827474582623861 theta= [[-9.31655759]\n",
      " [ 0.08008808]\n",
      " [ 0.07377613]]\n",
      "the iterations of  304000 the loss is 0.282723856201589 theta= [[-9.31809387]\n",
      " [ 0.08010014]\n",
      " [ 0.07378834]]\n",
      "the iterations of  304100 the loss is 0.28270026654452024 theta= [[-9.31962975]\n",
      " [ 0.08011221]\n",
      " [ 0.07380054]]\n",
      "the iterations of  304200 the loss is 0.28267668928129575 theta= [[-9.32116522]\n",
      " [ 0.08012427]\n",
      " [ 0.07381275]]\n",
      "the iterations of  304300 the loss is 0.2826531244020413 theta= [[-9.32270029]\n",
      " [ 0.08013633]\n",
      " [ 0.07382494]]\n",
      "the iterations of  304400 the loss is 0.2826295718968942 theta= [[-9.32423496]\n",
      " [ 0.08014839]\n",
      " [ 0.07383714]]\n",
      "the iterations of  304500 the loss is 0.2826060317560015 theta= [[-9.32576922]\n",
      " [ 0.08016044]\n",
      " [ 0.07384933]]\n",
      "the iterations of  304600 the loss is 0.2825825039695211 theta= [[-9.32730308]\n",
      " [ 0.08017249]\n",
      " [ 0.07386152]]\n",
      "the iterations of  304700 the loss is 0.2825589885276218 theta= [[-9.32883654]\n",
      " [ 0.08018453]\n",
      " [ 0.07387371]]\n",
      "the iterations of  304800 the loss is 0.2825354854204824 theta= [[-9.3303696 ]\n",
      " [ 0.08019658]\n",
      " [ 0.07388589]]\n",
      "the iterations of  304900 the loss is 0.2825119946382923 theta= [[-9.33190225]\n",
      " [ 0.08020862]\n",
      " [ 0.07389807]]\n",
      "the iterations of  305000 the loss is 0.28248851617125154 theta= [[-9.33343451]\n",
      " [ 0.08022065]\n",
      " [ 0.07391025]]\n",
      "the iterations of  305100 the loss is 0.28246505000957073 theta= [[-9.33496636]\n",
      " [ 0.08023269]\n",
      " [ 0.07392242]]\n",
      "the iterations of  305200 the loss is 0.2824415961434708 theta= [[-9.33649781]\n",
      " [ 0.08024472]\n",
      " [ 0.07393459]]\n",
      "the iterations of  305300 the loss is 0.28241815456318314 theta= [[-9.33802886]\n",
      " [ 0.08025675]\n",
      " [ 0.07394676]]\n",
      "the iterations of  305400 the loss is 0.2823947252589495 theta= [[-9.33955951]\n",
      " [ 0.08026877]\n",
      " [ 0.07395892]]\n",
      "the iterations of  305500 the loss is 0.28237130822102224 theta= [[-9.34108975]\n",
      " [ 0.08028079]\n",
      " [ 0.07397109]]\n",
      "the iterations of  305600 the loss is 0.282347903439664 theta= [[-9.3426196 ]\n",
      " [ 0.08029281]\n",
      " [ 0.07398324]]\n",
      "the iterations of  305700 the loss is 0.2823245109051479 theta= [[-9.34414905]\n",
      " [ 0.08030483]\n",
      " [ 0.0739954 ]]\n",
      "the iterations of  305800 the loss is 0.2823011306077573 theta= [[-9.34567809]\n",
      " [ 0.08031684]\n",
      " [ 0.07400755]]\n",
      "the iterations of  305900 the loss is 0.2822777625377861 theta= [[-9.34720674]\n",
      " [ 0.08032885]\n",
      " [ 0.0740197 ]]\n",
      "the iterations of  306000 the loss is 0.28225440668553864 theta= [[-9.34873499]\n",
      " [ 0.08034086]\n",
      " [ 0.07403184]]\n",
      "the iterations of  306100 the loss is 0.2822310630413293 theta= [[-9.35026283]\n",
      " [ 0.08035286]\n",
      " [ 0.07404399]]\n",
      "the iterations of  306200 the loss is 0.28220773159548285 theta= [[-9.35179028]\n",
      " [ 0.08036486]\n",
      " [ 0.07405613]]\n",
      "the iterations of  306300 the loss is 0.2821844123383346 theta= [[-9.35331733]\n",
      " [ 0.08037686]\n",
      " [ 0.07406826]]\n",
      "the iterations of  306400 the loss is 0.28216110526023 theta= [[-9.35484398]\n",
      " [ 0.08038885]\n",
      " [ 0.07408039]]\n",
      "the iterations of  306500 the loss is 0.28213781035152496 theta= [[-9.35637023]\n",
      " [ 0.08040085]\n",
      " [ 0.07409252]]\n",
      "the iterations of  306600 the loss is 0.28211452760258554 theta= [[-9.35789608]\n",
      " [ 0.08041283]\n",
      " [ 0.07410465]]\n",
      "the iterations of  306700 the loss is 0.2820912570037877 theta= [[-9.35942154]\n",
      " [ 0.08042482]\n",
      " [ 0.07411678]]\n",
      "the iterations of  306800 the loss is 0.2820679985455183 theta= [[-9.3609466]\n",
      " [ 0.0804368]\n",
      " [ 0.0741289]]\n",
      "the iterations of  306900 the loss is 0.28204475221817416 theta= [[-9.36247125]\n",
      " [ 0.08044878]\n",
      " [ 0.07414101]]\n",
      "the iterations of  307000 the loss is 0.282021518012162 theta= [[-9.36399552]\n",
      " [ 0.08046076]\n",
      " [ 0.07415313]]\n",
      "the iterations of  307100 the loss is 0.2819982959178995 theta= [[-9.36551938]\n",
      " [ 0.08047273]\n",
      " [ 0.07416524]]\n",
      "the iterations of  307200 the loss is 0.28197508592581344 theta= [[-9.36704285]\n",
      " [ 0.0804847 ]\n",
      " [ 0.07417735]]\n",
      "the iterations of  307300 the loss is 0.281951888026342 theta= [[-9.36856592]\n",
      " [ 0.08049667]\n",
      " [ 0.07418945]]\n",
      "the iterations of  307400 the loss is 0.28192870220993277 theta= [[-9.37008859]\n",
      " [ 0.08050863]\n",
      " [ 0.07420155]]\n",
      "the iterations of  307500 the loss is 0.2819055284670435 theta= [[-9.37161087]\n",
      " [ 0.08052059]\n",
      " [ 0.07421365]]\n",
      "the iterations of  307600 the loss is 0.28188236678814227 theta= [[-9.37313275]\n",
      " [ 0.08053255]\n",
      " [ 0.07422575]]\n",
      "the iterations of  307700 the loss is 0.28185921716370715 theta= [[-9.37465423]\n",
      " [ 0.08054451]\n",
      " [ 0.07423784]]\n",
      "the iterations of  307800 the loss is 0.2818360795842268 theta= [[-9.37617532]\n",
      " [ 0.08055646]\n",
      " [ 0.07424993]]\n",
      "the iterations of  307900 the loss is 0.281812954040199 theta= [[-9.37769601]\n",
      " [ 0.08056841]\n",
      " [ 0.07426202]]\n",
      "the iterations of  308000 the loss is 0.2817898405221327 theta= [[-9.37921631]\n",
      " [ 0.08058035]\n",
      " [ 0.0742741 ]]\n",
      "the iterations of  308100 the loss is 0.2817667390205461 theta= [[-9.38073621]\n",
      " [ 0.0805923 ]\n",
      " [ 0.07428618]]\n",
      "the iterations of  308200 the loss is 0.281743649525968 theta= [[-9.38225572]\n",
      " [ 0.08060424]\n",
      " [ 0.07429826]]\n",
      "the iterations of  308300 the loss is 0.28172057202893697 theta= [[-9.38377483]\n",
      " [ 0.08061617]\n",
      " [ 0.07431033]]\n",
      "the iterations of  308400 the loss is 0.2816975065200017 theta= [[-9.38529355]\n",
      " [ 0.08062811]\n",
      " [ 0.0743224 ]]\n",
      "the iterations of  308500 the loss is 0.2816744529897208 theta= [[-9.38681187]\n",
      " [ 0.08064004]\n",
      " [ 0.07433447]]\n",
      "the iterations of  308600 the loss is 0.2816514114286632 theta= [[-9.3883298 ]\n",
      " [ 0.08065196]\n",
      " [ 0.07434654]]\n",
      "the iterations of  308700 the loss is 0.28162838182740746 theta= [[-9.38984734]\n",
      " [ 0.08066389]\n",
      " [ 0.0743586 ]]\n",
      "the iterations of  308800 the loss is 0.281605364176542 theta= [[-9.39136448]\n",
      " [ 0.08067581]\n",
      " [ 0.07437066]]\n",
      "the iterations of  308900 the loss is 0.2815823584666657 theta= [[-9.39288123]\n",
      " [ 0.08068773]\n",
      " [ 0.07438271]]\n",
      "the iterations of  309000 the loss is 0.281559364688387 theta= [[-9.39439758]\n",
      " [ 0.08069965]\n",
      " [ 0.07439477]]\n",
      "the iterations of  309100 the loss is 0.2815363828323246 theta= [[-9.39591354]\n",
      " [ 0.08071156]\n",
      " [ 0.07440682]]\n",
      "the iterations of  309200 the loss is 0.28151341288910675 theta= [[-9.39742911]\n",
      " [ 0.08072347]\n",
      " [ 0.07441886]]\n",
      "the iterations of  309300 the loss is 0.2814904548493718 theta= [[-9.39894429]\n",
      " [ 0.08073538]\n",
      " [ 0.07443091]]\n",
      "the iterations of  309400 the loss is 0.2814675087037682 theta= [[-9.40045907]\n",
      " [ 0.08074728]\n",
      " [ 0.07444295]]\n",
      "the iterations of  309500 the loss is 0.281444574442954 theta= [[-9.40197346]\n",
      " [ 0.08075918]\n",
      " [ 0.07445498]]\n",
      "the iterations of  309600 the loss is 0.281421652057597 theta= [[-9.40348746]\n",
      " [ 0.08077108]\n",
      " [ 0.07446702]]\n",
      "the iterations of  309700 the loss is 0.2813987415383753 theta= [[-9.40500106]\n",
      " [ 0.08078297]\n",
      " [ 0.07447905]]\n",
      "the iterations of  309800 the loss is 0.28137584287597667 theta= [[-9.40651428]\n",
      " [ 0.08079486]\n",
      " [ 0.07449108]]\n",
      "the iterations of  309900 the loss is 0.2813529560610984 theta= [[-9.4080271 ]\n",
      " [ 0.08080675]\n",
      " [ 0.0745031 ]]\n",
      "the iterations of  310000 the loss is 0.28133008108444796 theta= [[-9.40953953]\n",
      " [ 0.08081864]\n",
      " [ 0.07451513]]\n",
      "the iterations of  310100 the loss is 0.2813072179367427 theta= [[-9.41105157]\n",
      " [ 0.08083052]\n",
      " [ 0.07452714]]\n",
      "the iterations of  310200 the loss is 0.2812843666087093 theta= [[-9.41256322]\n",
      " [ 0.0808424 ]\n",
      " [ 0.07453916]]\n",
      "the iterations of  310300 the loss is 0.28126152709108465 theta= [[-9.41407448]\n",
      " [ 0.08085428]\n",
      " [ 0.07455117]]\n",
      "the iterations of  310400 the loss is 0.28123869937461565 theta= [[-9.41558535]\n",
      " [ 0.08086615]\n",
      " [ 0.07456318]]\n",
      "the iterations of  310500 the loss is 0.2812158834500581 theta= [[-9.41709583]\n",
      " [ 0.08087802]\n",
      " [ 0.07457519]]\n",
      "the iterations of  310600 the loss is 0.281193079308178 theta= [[-9.41860592]\n",
      " [ 0.08088989]\n",
      " [ 0.07458719]]\n",
      "the iterations of  310700 the loss is 0.2811702869397515 theta= [[-9.42011562]\n",
      " [ 0.08090176]\n",
      " [ 0.07459919]]\n",
      "the iterations of  310800 the loss is 0.28114750633556373 theta= [[-9.42162493]\n",
      " [ 0.08091362]\n",
      " [ 0.07461119]]\n",
      "the iterations of  310900 the loss is 0.2811247374864102 theta= [[-9.42313385]\n",
      " [ 0.08092548]\n",
      " [ 0.07462319]]\n",
      "the iterations of  311000 the loss is 0.2811019803830956 theta= [[-9.42464238]\n",
      " [ 0.08093733]\n",
      " [ 0.07463518]]\n",
      "the iterations of  311100 the loss is 0.2810792350164342 theta= [[-9.42615052]\n",
      " [ 0.08094919]\n",
      " [ 0.07464717]]\n",
      "the iterations of  311200 the loss is 0.2810565013772507 theta= [[-9.42765827]\n",
      " [ 0.08096104]\n",
      " [ 0.07465915]]\n",
      "the iterations of  311300 the loss is 0.2810337794563788 theta= [[-9.42916563]\n",
      " [ 0.08097288]\n",
      " [ 0.07467114]]\n",
      "the iterations of  311400 the loss is 0.2810110692446622 theta= [[-9.43067261]\n",
      " [ 0.08098473]\n",
      " [ 0.07468311]]\n",
      "the iterations of  311500 the loss is 0.2809883707329535 theta= [[-9.43217919]\n",
      " [ 0.08099657]\n",
      " [ 0.07469509]]\n",
      "the iterations of  311600 the loss is 0.2809656839121159 theta= [[-9.43368539]\n",
      " [ 0.08100841]\n",
      " [ 0.07470706]]\n",
      "the iterations of  311700 the loss is 0.2809430087730216 theta= [[-9.4351912 ]\n",
      " [ 0.08102024]\n",
      " [ 0.07471903]]\n",
      "the iterations of  311800 the loss is 0.28092034530655274 theta= [[-9.43669663]\n",
      " [ 0.08103207]\n",
      " [ 0.074731  ]]\n",
      "the iterations of  311900 the loss is 0.2808976935036008 theta= [[-9.43820166]\n",
      " [ 0.0810439 ]\n",
      " [ 0.07474297]]\n",
      "the iterations of  312000 the loss is 0.28087505335506663 theta= [[-9.43970631]\n",
      " [ 0.08105573]\n",
      " [ 0.07475493]]\n",
      "the iterations of  312100 the loss is 0.2808524248518611 theta= [[-9.44121057]\n",
      " [ 0.08106755]\n",
      " [ 0.07476689]]\n",
      "the iterations of  312200 the loss is 0.28082980798490437 theta= [[-9.44271445]\n",
      " [ 0.08107937]\n",
      " [ 0.07477884]]\n",
      "the iterations of  312300 the loss is 0.28080720274512616 theta= [[-9.44421794]\n",
      " [ 0.08109119]\n",
      " [ 0.07479079]]\n",
      "the iterations of  312400 the loss is 0.2807846091234656 theta= [[-9.44572104]\n",
      " [ 0.081103  ]\n",
      " [ 0.07480274]]\n",
      "the iterations of  312500 the loss is 0.28076202711087145 theta= [[-9.44722375]\n",
      " [ 0.08111482]\n",
      " [ 0.07481469]]\n",
      "the iterations of  312600 the loss is 0.280739456698302 theta= [[-9.44872608]\n",
      " [ 0.08112662]\n",
      " [ 0.07482663]]\n",
      "the iterations of  312700 the loss is 0.2807168978767251 theta= [[-9.45022803]\n",
      " [ 0.08113843]\n",
      " [ 0.07483857]]\n",
      "the iterations of  312800 the loss is 0.28069435063711784 theta= [[-9.45172958]\n",
      " [ 0.08115023]\n",
      " [ 0.07485051]]\n",
      "the iterations of  312900 the loss is 0.28067181497046667 theta= [[-9.45323076]\n",
      " [ 0.08116203]\n",
      " [ 0.07486244]]\n",
      "the iterations of  313000 the loss is 0.28064929086776774 theta= [[-9.45473154]\n",
      " [ 0.08117383]\n",
      " [ 0.07487438]]\n",
      "the iterations of  313100 the loss is 0.2806267783200265 theta= [[-9.45623195]\n",
      " [ 0.08118562]\n",
      " [ 0.0748863 ]]\n",
      "the iterations of  313200 the loss is 0.2806042773182582 theta= [[-9.45773196]\n",
      " [ 0.08119741]\n",
      " [ 0.07489823]]\n",
      "the iterations of  313300 the loss is 0.28058178785348686 theta= [[-9.4592316 ]\n",
      " [ 0.0812092 ]\n",
      " [ 0.07491015]]\n",
      "the iterations of  313400 the loss is 0.28055930991674616 theta= [[-9.46073084]\n",
      " [ 0.08122099]\n",
      " [ 0.07492207]]\n",
      "the iterations of  313500 the loss is 0.2805368434990793 theta= [[-9.46222971]\n",
      " [ 0.08123277]\n",
      " [ 0.07493399]]\n",
      "the iterations of  313600 the loss is 0.2805143885915385 theta= [[-9.46372819]\n",
      " [ 0.08124455]\n",
      " [ 0.0749459 ]]\n",
      "the iterations of  313700 the loss is 0.2804919451851859 theta= [[-9.46522629]\n",
      " [ 0.08125632]\n",
      " [ 0.07495781]]\n",
      "the iterations of  313800 the loss is 0.2804695132710923 theta= [[-9.466724  ]\n",
      " [ 0.0812681 ]\n",
      " [ 0.07496972]]\n",
      "the iterations of  313900 the loss is 0.2804470928403385 theta= [[-9.46822133]\n",
      " [ 0.08127987]\n",
      " [ 0.07498162]]\n",
      "the iterations of  314000 the loss is 0.28042468388401426 theta= [[-9.46971827]\n",
      " [ 0.08129163]\n",
      " [ 0.07499352]]\n",
      "the iterations of  314100 the loss is 0.28040228639321857 theta= [[-9.47121484]\n",
      " [ 0.0813034 ]\n",
      " [ 0.07500542]]\n",
      "the iterations of  314200 the loss is 0.28037990035905985 theta= [[-9.47271102]\n",
      " [ 0.08131516]\n",
      " [ 0.07501732]]\n",
      "the iterations of  314300 the loss is 0.28035752577265594 theta= [[-9.47420682]\n",
      " [ 0.08132692]\n",
      " [ 0.07502921]]\n",
      "the iterations of  314400 the loss is 0.2803351626251334 theta= [[-9.47570223]\n",
      " [ 0.08133867]\n",
      " [ 0.0750411 ]]\n",
      "the iterations of  314500 the loss is 0.2803128109076288 theta= [[-9.47719726]\n",
      " [ 0.08135043]\n",
      " [ 0.07505298]]\n",
      "the iterations of  314600 the loss is 0.2802904706112874 theta= [[-9.47869192]\n",
      " [ 0.08136218]\n",
      " [ 0.07506487]]\n",
      "the iterations of  314700 the loss is 0.28026814172726394 theta= [[-9.48018618]\n",
      " [ 0.08137392]\n",
      " [ 0.07507675]]\n",
      "the iterations of  314800 the loss is 0.2802458242467225 theta= [[-9.48168007]\n",
      " [ 0.08138567]\n",
      " [ 0.07508862]]\n",
      "the iterations of  314900 the loss is 0.28022351816083596 theta= [[-9.48317358]\n",
      " [ 0.08139741]\n",
      " [ 0.0751005 ]]\n",
      "the iterations of  315000 the loss is 0.28020122346078663 theta= [[-9.4846667 ]\n",
      " [ 0.08140915]\n",
      " [ 0.07511237]]\n",
      "the iterations of  315100 the loss is 0.2801789401377661 theta= [[-9.48615945]\n",
      " [ 0.08142088]\n",
      " [ 0.07512424]]\n",
      "the iterations of  315200 the loss is 0.2801566681829749 theta= [[-9.48765181]\n",
      " [ 0.08143262]\n",
      " [ 0.0751361 ]]\n",
      "the iterations of  315300 the loss is 0.28013440758762315 theta= [[-9.48914379]\n",
      " [ 0.08144434]\n",
      " [ 0.07514797]]\n",
      "the iterations of  315400 the loss is 0.28011215834292963 theta= [[-9.4906354 ]\n",
      " [ 0.08145607]\n",
      " [ 0.07515983]]\n",
      "the iterations of  315500 the loss is 0.2800899204401226 theta= [[-9.49212662]\n",
      " [ 0.08146779]\n",
      " [ 0.07517168]]\n",
      "the iterations of  315600 the loss is 0.28006769387043906 theta= [[-9.49361746]\n",
      " [ 0.08147952]\n",
      " [ 0.07518354]]\n",
      "the iterations of  315700 the loss is 0.28004547862512547 theta= [[-9.49510792]\n",
      " [ 0.08149123]\n",
      " [ 0.07519539]]\n",
      "the iterations of  315800 the loss is 0.2800232746954374 theta= [[-9.496598  ]\n",
      " [ 0.08150295]\n",
      " [ 0.07520723]]\n",
      "the iterations of  315900 the loss is 0.2800010820726393 theta= [[-9.49808771]\n",
      " [ 0.08151466]\n",
      " [ 0.07521908]]\n",
      "the iterations of  316000 the loss is 0.27997890074800486 theta= [[-9.49957703]\n",
      " [ 0.08152637]\n",
      " [ 0.07523092]]\n",
      "the iterations of  316100 the loss is 0.2799567307128166 theta= [[-9.50106598]\n",
      " [ 0.08153807]\n",
      " [ 0.07524276]]\n",
      "the iterations of  316200 the loss is 0.27993457195836624 theta= [[-9.50255454]\n",
      " [ 0.08154978]\n",
      " [ 0.0752546 ]]\n",
      "the iterations of  316300 the loss is 0.27991242447595477 theta= [[-9.50404273]\n",
      " [ 0.08156148]\n",
      " [ 0.07526643]]\n",
      "the iterations of  316400 the loss is 0.27989028825689233 theta= [[-9.50553054]\n",
      " [ 0.08157318]\n",
      " [ 0.07527826]]\n",
      "the iterations of  316500 the loss is 0.279868163292497 theta= [[-9.50701797]\n",
      " [ 0.08158487]\n",
      " [ 0.07529008]]\n",
      "the iterations of  316600 the loss is 0.2798460495740972 theta= [[-9.50850502]\n",
      " [ 0.08159656]\n",
      " [ 0.07530191]]\n",
      "the iterations of  316700 the loss is 0.2798239470930295 theta= [[-9.50999169]\n",
      " [ 0.08160825]\n",
      " [ 0.07531373]]\n",
      "the iterations of  316800 the loss is 0.27980185584063977 theta= [[-9.51147799]\n",
      " [ 0.08161994]\n",
      " [ 0.07532555]]\n",
      "the iterations of  316900 the loss is 0.2797797758082829 theta= [[-9.51296391]\n",
      " [ 0.08163162]\n",
      " [ 0.07533736]]\n",
      "the iterations of  317000 the loss is 0.2797577069873227 theta= [[-9.51444945]\n",
      " [ 0.0816433 ]\n",
      " [ 0.07534918]]\n",
      "the iterations of  317100 the loss is 0.2797356493691318 theta= [[-9.51593462]\n",
      " [ 0.08165498]\n",
      " [ 0.07536098]]\n",
      "the iterations of  317200 the loss is 0.27971360294509184 theta= [[-9.5174194 ]\n",
      " [ 0.08166665]\n",
      " [ 0.07537279]]\n",
      "the iterations of  317300 the loss is 0.2796915677065934 theta= [[-9.51890381]\n",
      " [ 0.08167832]\n",
      " [ 0.07538459]]\n",
      "the iterations of  317400 the loss is 0.279669543645036 theta= [[-9.52038785]\n",
      " [ 0.08168999]\n",
      " [ 0.07539639]]\n",
      "the iterations of  317500 the loss is 0.279647530751828 theta= [[-9.52187151]\n",
      " [ 0.08170166]\n",
      " [ 0.07540819]]\n",
      "the iterations of  317600 the loss is 0.2796255290183867 theta= [[-9.52335479]\n",
      " [ 0.08171332]\n",
      " [ 0.07541999]]\n",
      "the iterations of  317700 the loss is 0.2796035384361383 theta= [[-9.5248377 ]\n",
      " [ 0.08172498]\n",
      " [ 0.07543178]]\n",
      "the iterations of  317800 the loss is 0.2795815589965176 theta= [[-9.52632023]\n",
      " [ 0.08173664]\n",
      " [ 0.07544357]]\n",
      "the iterations of  317900 the loss is 0.27955959069096875 theta= [[-9.52780238]\n",
      " [ 0.08174829]\n",
      " [ 0.07545535]]\n",
      "the iterations of  318000 the loss is 0.27953763351094457 theta= [[-9.52928416]\n",
      " [ 0.08175994]\n",
      " [ 0.07546714]]\n",
      "the iterations of  318100 the loss is 0.2795156874479062 theta= [[-9.53076556]\n",
      " [ 0.08177159]\n",
      " [ 0.07547892]]\n",
      "the iterations of  318200 the loss is 0.2794937524933246 theta= [[-9.53224659]\n",
      " [ 0.08178324]\n",
      " [ 0.07549069]]\n",
      "the iterations of  318300 the loss is 0.2794718286386785 theta= [[-9.53372725]\n",
      " [ 0.08179488]\n",
      " [ 0.07550247]]\n",
      "the iterations of  318400 the loss is 0.27944991587545603 theta= [[-9.53520753]\n",
      " [ 0.08180652]\n",
      " [ 0.07551424]]\n",
      "the iterations of  318500 the loss is 0.27942801419515395 theta= [[-9.53668743]\n",
      " [ 0.08181816]\n",
      " [ 0.07552601]]\n",
      "the iterations of  318600 the loss is 0.279406123589278 theta= [[-9.53816697]\n",
      " [ 0.08182979]\n",
      " [ 0.07553777]]\n",
      "the iterations of  318700 the loss is 0.2793842440493424 theta= [[-9.53964612]\n",
      " [ 0.08184142]\n",
      " [ 0.07554953]]\n",
      "the iterations of  318800 the loss is 0.27936237556687016 theta= [[-9.54112491]\n",
      " [ 0.08185305]\n",
      " [ 0.07556129]]\n",
      "the iterations of  318900 the loss is 0.2793405181333933 theta= [[-9.54260332]\n",
      " [ 0.08186467]\n",
      " [ 0.07557305]]\n",
      "the iterations of  319000 the loss is 0.2793186717404521 theta= [[-9.54408135]\n",
      " [ 0.0818763 ]\n",
      " [ 0.0755848 ]]\n",
      "the iterations of  319100 the loss is 0.27929683637959607 theta= [[-9.54555902]\n",
      " [ 0.08188792]\n",
      " [ 0.07559655]]\n",
      "the iterations of  319200 the loss is 0.2792750120423834 theta= [[-9.54703631]\n",
      " [ 0.08189953]\n",
      " [ 0.0756083 ]]\n",
      "the iterations of  319300 the loss is 0.2792531987203803 theta= [[-9.54851322]\n",
      " [ 0.08191115]\n",
      " [ 0.07562005]]\n",
      "the iterations of  319400 the loss is 0.2792313964051624 theta= [[-9.54998977]\n",
      " [ 0.08192276]\n",
      " [ 0.07563179]]\n",
      "the iterations of  319500 the loss is 0.27920960508831366 theta= [[-9.55146594]\n",
      " [ 0.08193437]\n",
      " [ 0.07564353]]\n",
      "the iterations of  319600 the loss is 0.279187824761427 theta= [[-9.55294174]\n",
      " [ 0.08194597]\n",
      " [ 0.07565526]]\n",
      "the iterations of  319700 the loss is 0.2791660554161038 theta= [[-9.55441717]\n",
      " [ 0.08195758]\n",
      " [ 0.075667  ]]\n",
      "the iterations of  319800 the loss is 0.27914429704395366 theta= [[-9.55589222]\n",
      " [ 0.08196918]\n",
      " [ 0.07567873]]\n",
      "the iterations of  319900 the loss is 0.27912254963659555 theta= [[-9.55736691]\n",
      " [ 0.08198077]\n",
      " [ 0.07569045]]\n",
      "the iterations of  320000 the loss is 0.27910081318565644 theta= [[-9.55884122]\n",
      " [ 0.08199237]\n",
      " [ 0.07570218]]\n",
      "the iterations of  320100 the loss is 0.2790790876827727 theta= [[-9.56031516]\n",
      " [ 0.08200396]\n",
      " [ 0.0757139 ]]\n",
      "the iterations of  320200 the loss is 0.2790573731195884 theta= [[-9.56178873]\n",
      " [ 0.08201555]\n",
      " [ 0.07572562]]\n",
      "the iterations of  320300 the loss is 0.27903566948775693 theta= [[-9.56326193]\n",
      " [ 0.08202713]\n",
      " [ 0.07573733]]\n",
      "the iterations of  320400 the loss is 0.2790139767789397 theta= [[-9.56473476]\n",
      " [ 0.08203871]\n",
      " [ 0.07574905]]\n",
      "the iterations of  320500 the loss is 0.27899229498480677 theta= [[-9.56620722]\n",
      " [ 0.08205029]\n",
      " [ 0.07576076]]\n",
      "the iterations of  320600 the loss is 0.27897062409703716 theta= [[-9.5676793 ]\n",
      " [ 0.08206187]\n",
      " [ 0.07577246]]\n",
      "the iterations of  320700 the loss is 0.27894896410731806 theta= [[-9.56915102]\n",
      " [ 0.08207345]\n",
      " [ 0.07578417]]\n",
      "the iterations of  320800 the loss is 0.2789273150073452 theta= [[-9.57062237]\n",
      " [ 0.08208502]\n",
      " [ 0.07579587]]\n",
      "the iterations of  320900 the loss is 0.2789056767888232 theta= [[-9.57209335]\n",
      " [ 0.08209659]\n",
      " [ 0.07580757]]\n",
      "the iterations of  321000 the loss is 0.27888404944346457 theta= [[-9.57356395]\n",
      " [ 0.08210815]\n",
      " [ 0.07581926]]\n",
      "the iterations of  321100 the loss is 0.2788624329629909 theta= [[-9.57503419]\n",
      " [ 0.08211971]\n",
      " [ 0.07583096]]\n",
      "the iterations of  321200 the loss is 0.278840827339132 theta= [[-9.57650406]\n",
      " [ 0.08213127]\n",
      " [ 0.07584265]]\n",
      "the iterations of  321300 the loss is 0.27881923256362606 theta= [[-9.57797356]\n",
      " [ 0.08214283]\n",
      " [ 0.07585433]]\n",
      "the iterations of  321400 the loss is 0.27879764862822004 theta= [[-9.57944269]\n",
      " [ 0.08215439]\n",
      " [ 0.07586602]]\n",
      "the iterations of  321500 the loss is 0.27877607552466904 theta= [[-9.58091145]\n",
      " [ 0.08216594]\n",
      " [ 0.0758777 ]]\n",
      "the iterations of  321600 the loss is 0.2787545132447369 theta= [[-9.58237985]\n",
      " [ 0.08217749]\n",
      " [ 0.07588938]]\n",
      "the iterations of  321700 the loss is 0.2787329617801955 theta= [[-9.58384787]\n",
      " [ 0.08218903]\n",
      " [ 0.07590105]]\n",
      "the iterations of  321800 the loss is 0.27871142112282554 theta= [[-9.58531553]\n",
      " [ 0.08220058]\n",
      " [ 0.07591272]]\n",
      "the iterations of  321900 the loss is 0.2786898912644162 theta= [[-9.58678282]\n",
      " [ 0.08221212]\n",
      " [ 0.07592439]]\n",
      "the iterations of  322000 the loss is 0.27866837219676427 theta= [[-9.58824974]\n",
      " [ 0.08222365]\n",
      " [ 0.07593606]]\n",
      "the iterations of  322100 the loss is 0.2786468639116758 theta= [[-9.5897163 ]\n",
      " [ 0.08223519]\n",
      " [ 0.07594772]]\n",
      "the iterations of  322200 the loss is 0.2786253664009649 theta= [[-9.59118248]\n",
      " [ 0.08224672]\n",
      " [ 0.07595939]]\n",
      "the iterations of  322300 the loss is 0.27860387965645433 theta= [[-9.5926483 ]\n",
      " [ 0.08225825]\n",
      " [ 0.07597104]]\n",
      "the iterations of  322400 the loss is 0.2785824036699748 theta= [[-9.59411375]\n",
      " [ 0.08226978]\n",
      " [ 0.0759827 ]]\n",
      "the iterations of  322500 the loss is 0.2785609384333653 theta= [[-9.59557884]\n",
      " [ 0.0822813 ]\n",
      " [ 0.07599435]]\n",
      "the iterations of  322600 the loss is 0.27853948393847344 theta= [[-9.59704356]\n",
      " [ 0.08229282]\n",
      " [ 0.076006  ]]\n",
      "the iterations of  322700 the loss is 0.27851804017715526 theta= [[-9.59850791]\n",
      " [ 0.08230434]\n",
      " [ 0.07601765]]\n",
      "the iterations of  322800 the loss is 0.27849660714127483 theta= [[-9.5999719 ]\n",
      " [ 0.08231585]\n",
      " [ 0.07602929]]\n",
      "the iterations of  322900 the loss is 0.2784751848227045 theta= [[-9.60143552]\n",
      " [ 0.08232736]\n",
      " [ 0.07604093]]\n",
      "the iterations of  323000 the loss is 0.27845377321332526 theta= [[-9.60289877]\n",
      " [ 0.08233887]\n",
      " [ 0.07605257]]\n",
      "the iterations of  323100 the loss is 0.2784323723050262 theta= [[-9.60436166]\n",
      " [ 0.08235038]\n",
      " [ 0.07606421]]\n",
      "the iterations of  323200 the loss is 0.2784109820897045 theta= [[-9.60582418]\n",
      " [ 0.08236188]\n",
      " [ 0.07607584]]\n",
      "the iterations of  323300 the loss is 0.2783896025592656 theta= [[-9.60728634]\n",
      " [ 0.08237339]\n",
      " [ 0.07608747]]\n",
      "the iterations of  323400 the loss is 0.2783682337056236 theta= [[-9.60874813]\n",
      " [ 0.08238488]\n",
      " [ 0.0760991 ]]\n",
      "the iterations of  323500 the loss is 0.27834687552070053 theta= [[-9.61020956]\n",
      " [ 0.08239638]\n",
      " [ 0.07611072]]\n",
      "the iterations of  323600 the loss is 0.2783255279964268 theta= [[-9.61167063]\n",
      " [ 0.08240787]\n",
      " [ 0.07612234]]\n",
      "the iterations of  323700 the loss is 0.27830419112474075 theta= [[-9.61313132]\n",
      " [ 0.08241936]\n",
      " [ 0.07613396]]\n",
      "the iterations of  323800 the loss is 0.27828286489758924 theta= [[-9.61459166]\n",
      " [ 0.08243085]\n",
      " [ 0.07614557]]\n",
      "the iterations of  323900 the loss is 0.27826154930692726 theta= [[-9.61605163]\n",
      " [ 0.08244233]\n",
      " [ 0.07615719]]\n",
      "the iterations of  324000 the loss is 0.27824024434471784 theta= [[-9.61751123]\n",
      " [ 0.08245381]\n",
      " [ 0.0761688 ]]\n",
      "the iterations of  324100 the loss is 0.2782189500029326 theta= [[-9.61897048]\n",
      " [ 0.08246529]\n",
      " [ 0.0761804 ]]\n",
      "the iterations of  324200 the loss is 0.2781976662735508 theta= [[-9.62042935]\n",
      " [ 0.08247677]\n",
      " [ 0.07619201]]\n",
      "the iterations of  324300 the loss is 0.2781763931485601 theta= [[-9.62188787]\n",
      " [ 0.08248824]\n",
      " [ 0.07620361]]\n",
      "the iterations of  324400 the loss is 0.2781551306199563 theta= [[-9.62334602]\n",
      " [ 0.08249971]\n",
      " [ 0.07621521]]\n",
      "the iterations of  324500 the loss is 0.2781338786797434 theta= [[-9.62480381]\n",
      " [ 0.08251118]\n",
      " [ 0.0762268 ]]\n",
      "the iterations of  324600 the loss is 0.2781126373199334 theta= [[-9.62626123]\n",
      " [ 0.08252264]\n",
      " [ 0.07623839]]\n",
      "the iterations of  324700 the loss is 0.27809140653254644 theta= [[-9.6277183 ]\n",
      " [ 0.08253411]\n",
      " [ 0.07624998]]\n",
      "the iterations of  324800 the loss is 0.2780701863096107 theta= [[-9.629175  ]\n",
      " [ 0.08254557]\n",
      " [ 0.07626157]]\n",
      "the iterations of  324900 the loss is 0.27804897664316297 theta= [[-9.63063133]\n",
      " [ 0.08255702]\n",
      " [ 0.07627315]]\n",
      "the iterations of  325000 the loss is 0.27802777752524727 theta= [[-9.63208731]\n",
      " [ 0.08256848]\n",
      " [ 0.07628474]]\n",
      "the iterations of  325100 the loss is 0.27800658894791624 theta= [[-9.63354292]\n",
      " [ 0.08257993]\n",
      " [ 0.07629631]]\n",
      "the iterations of  325200 the loss is 0.2779854109032308 theta= [[-9.63499817]\n",
      " [ 0.08259137]\n",
      " [ 0.07630789]]\n",
      "the iterations of  325300 the loss is 0.2779642433832592 theta= [[-9.63645306]\n",
      " [ 0.08260282]\n",
      " [ 0.07631946]]\n",
      "the iterations of  325400 the loss is 0.2779430863800783 theta= [[-9.63790759]\n",
      " [ 0.08261426]\n",
      " [ 0.07633103]]\n",
      "the iterations of  325500 the loss is 0.2779219398857729 theta= [[-9.63936176]\n",
      " [ 0.0826257 ]\n",
      " [ 0.0763426 ]]\n",
      "the iterations of  325600 the loss is 0.2779008038924358 theta= [[-9.64081556]\n",
      " [ 0.08263714]\n",
      " [ 0.07635416]]\n",
      "the iterations of  325700 the loss is 0.27787967839216754 theta= [[-9.64226901]\n",
      " [ 0.08264857]\n",
      " [ 0.07636573]]\n",
      "the iterations of  325800 the loss is 0.2778585633770771 theta= [[-9.64372209]\n",
      " [ 0.08266   ]\n",
      " [ 0.07637728]]\n",
      "the iterations of  325900 the loss is 0.2778374588392812 theta= [[-9.64517482]\n",
      " [ 0.08267143]\n",
      " [ 0.07638884]]\n",
      "the iterations of  326000 the loss is 0.2778163647709045 theta= [[-9.64662718]\n",
      " [ 0.08268286]\n",
      " [ 0.07640039]]\n",
      "the iterations of  326100 the loss is 0.2777952811640799 theta= [[-9.64807918]\n",
      " [ 0.08269428]\n",
      " [ 0.07641194]]\n",
      "the iterations of  326200 the loss is 0.27777420801094804 theta= [[-9.64953083]\n",
      " [ 0.0827057 ]\n",
      " [ 0.07642349]]\n",
      "the iterations of  326300 the loss is 0.2777531453036573 theta= [[-9.65098211]\n",
      " [ 0.08271712]\n",
      " [ 0.07643504]]\n",
      "the iterations of  326400 the loss is 0.27773209303436464 theta= [[-9.65243303]\n",
      " [ 0.08272854]\n",
      " [ 0.07644658]]\n",
      "the iterations of  326500 the loss is 0.2777110511952346 theta= [[-9.6538836 ]\n",
      " [ 0.08273995]\n",
      " [ 0.07645812]]\n",
      "the iterations of  326600 the loss is 0.27769001977843905 theta= [[-9.6553338 ]\n",
      " [ 0.08275136]\n",
      " [ 0.07646965]]\n",
      "the iterations of  326700 the loss is 0.27766899877615914 theta= [[-9.65678365]\n",
      " [ 0.08276276]\n",
      " [ 0.07648119]]\n",
      "the iterations of  326800 the loss is 0.2776479881805828 theta= [[-9.65823314]\n",
      " [ 0.08277417]\n",
      " [ 0.07649272]]\n",
      "the iterations of  326900 the loss is 0.27762698798390584 theta= [[-9.65968226]\n",
      " [ 0.08278557]\n",
      " [ 0.07650424]]\n",
      "the iterations of  327000 the loss is 0.2776059981783327 theta= [[-9.66113103]\n",
      " [ 0.08279697]\n",
      " [ 0.07651577]]\n",
      "the iterations of  327100 the loss is 0.27758501875607516 theta= [[-9.66257944]\n",
      " [ 0.08280836]\n",
      " [ 0.07652729]]\n",
      "the iterations of  327200 the loss is 0.2775640497093532 theta= [[-9.6640275 ]\n",
      " [ 0.08281976]\n",
      " [ 0.07653881]]\n",
      "the iterations of  327300 the loss is 0.277543091030394 theta= [[-9.66547519]\n",
      " [ 0.08283115]\n",
      " [ 0.07655033]]\n",
      "the iterations of  327400 the loss is 0.2775221427114334 theta= [[-9.66692253]\n",
      " [ 0.08284253]\n",
      " [ 0.07656184]]\n",
      "the iterations of  327500 the loss is 0.2775012047447144 theta= [[-9.66836951]\n",
      " [ 0.08285392]\n",
      " [ 0.07657335]]\n",
      "the iterations of  327600 the loss is 0.27748027712248857 theta= [[-9.66981613]\n",
      " [ 0.0828653 ]\n",
      " [ 0.07658486]]\n",
      "the iterations of  327700 the loss is 0.27745935983701464 theta= [[-9.67126239]\n",
      " [ 0.08287668]\n",
      " [ 0.07659637]]\n",
      "the iterations of  327800 the loss is 0.27743845288055896 theta= [[-9.6727083 ]\n",
      " [ 0.08288805]\n",
      " [ 0.07660787]]\n",
      "the iterations of  327900 the loss is 0.2774175562453967 theta= [[-9.67415385]\n",
      " [ 0.08289943]\n",
      " [ 0.07661937]]\n",
      "the iterations of  328000 the loss is 0.27739666992381 theta= [[-9.67559905]\n",
      " [ 0.0829108 ]\n",
      " [ 0.07663087]]\n",
      "the iterations of  328100 the loss is 0.27737579390808875 theta= [[-9.67704388]\n",
      " [ 0.08292217]\n",
      " [ 0.07664236]]\n",
      "the iterations of  328200 the loss is 0.2773549281905308 theta= [[-9.67848836]\n",
      " [ 0.08293353]\n",
      " [ 0.07665385]]\n",
      "the iterations of  328300 the loss is 0.2773340727634419 theta= [[-9.67993249]\n",
      " [ 0.0829449 ]\n",
      " [ 0.07666534]]\n",
      "the iterations of  328400 the loss is 0.2773132276191355 theta= [[-9.68137626]\n",
      " [ 0.08295626]\n",
      " [ 0.07667683]]\n",
      "the iterations of  328500 the loss is 0.27729239274993245 theta= [[-9.68281967]\n",
      " [ 0.08296761]\n",
      " [ 0.07668831]]\n",
      "the iterations of  328600 the loss is 0.27727156814816156 theta= [[-9.68426273]\n",
      " [ 0.08297897]\n",
      " [ 0.07669979]]\n",
      "the iterations of  328700 the loss is 0.2772507538061596 theta= [[-9.68570543]\n",
      " [ 0.08299032]\n",
      " [ 0.07671127]]\n",
      "the iterations of  328800 the loss is 0.2772299497162707 theta= [[-9.68714777]\n",
      " [ 0.08300167]\n",
      " [ 0.07672274]]\n",
      "the iterations of  328900 the loss is 0.2772091558708468 theta= [[-9.68858976]\n",
      " [ 0.08301302]\n",
      " [ 0.07673421]]\n",
      "the iterations of  329000 the loss is 0.2771883722622476 theta= [[-9.6900314 ]\n",
      " [ 0.08302436]\n",
      " [ 0.07674568]]\n",
      "the iterations of  329100 the loss is 0.27716759888284026 theta= [[-9.69147268]\n",
      " [ 0.0830357 ]\n",
      " [ 0.07675715]]\n",
      "the iterations of  329200 the loss is 0.2771468357249998 theta= [[-9.69291361]\n",
      " [ 0.08304704]\n",
      " [ 0.07676861]]\n",
      "the iterations of  329300 the loss is 0.27712608278110884 theta= [[-9.69435418]\n",
      " [ 0.08305837]\n",
      " [ 0.07678008]]\n",
      "the iterations of  329400 the loss is 0.27710534004355764 theta= [[-9.69579439]\n",
      " [ 0.08306971]\n",
      " [ 0.07679153]]\n",
      "the iterations of  329500 the loss is 0.27708460750474406 theta= [[-9.69723426]\n",
      " [ 0.08308104]\n",
      " [ 0.07680299]]\n",
      "the iterations of  329600 the loss is 0.27706388515707386 theta= [[-9.69867377]\n",
      " [ 0.08309236]\n",
      " [ 0.07681444]]\n",
      "the iterations of  329700 the loss is 0.2770431729929598 theta= [[-9.70011292]\n",
      " [ 0.08310369]\n",
      " [ 0.07682589]]\n",
      "the iterations of  329800 the loss is 0.2770224710048228 theta= [[-9.70155173]\n",
      " [ 0.08311501]\n",
      " [ 0.07683734]]\n",
      "the iterations of  329900 the loss is 0.27700177918509133 theta= [[-9.70299017]\n",
      " [ 0.08312633]\n",
      " [ 0.07684878]]\n",
      "the iterations of  330000 the loss is 0.2769810975262014 theta= [[-9.70442827]\n",
      " [ 0.08313765]\n",
      " [ 0.07686022]]\n",
      "the iterations of  330100 the loss is 0.2769604260205964 theta= [[-9.70586601]\n",
      " [ 0.08314896]\n",
      " [ 0.07687166]]\n",
      "the iterations of  330200 the loss is 0.27693976466072745 theta= [[-9.7073034 ]\n",
      " [ 0.08316027]\n",
      " [ 0.0768831 ]]\n",
      "the iterations of  330300 the loss is 0.27691911343905334 theta= [[-9.70874044]\n",
      " [ 0.08317158]\n",
      " [ 0.07689453]]\n",
      "the iterations of  330400 the loss is 0.2768984723480403 theta= [[-9.71017712]\n",
      " [ 0.08318288]\n",
      " [ 0.07690596]]\n",
      "the iterations of  330500 the loss is 0.276877841380162 theta= [[-9.71161345]\n",
      " [ 0.08319419]\n",
      " [ 0.07691739]]\n",
      "the iterations of  330600 the loss is 0.27685722052789974 theta= [[-9.71304943]\n",
      " [ 0.08320549]\n",
      " [ 0.07692882]]\n",
      "the iterations of  330700 the loss is 0.27683660978374225 theta= [[-9.71448506]\n",
      " [ 0.08321679]\n",
      " [ 0.07694024]]\n",
      "the iterations of  330800 the loss is 0.2768160091401859 theta= [[-9.71592034]\n",
      " [ 0.08322808]\n",
      " [ 0.07695166]]\n",
      "the iterations of  330900 the loss is 0.27679541858973467 theta= [[-9.71735526]\n",
      " [ 0.08323937]\n",
      " [ 0.07696308]]\n",
      "the iterations of  331000 the loss is 0.27677483812489984 theta= [[-9.71878983]\n",
      " [ 0.08325066]\n",
      " [ 0.07697449]]\n",
      "the iterations of  331100 the loss is 0.27675426773820017 theta= [[-9.72022406]\n",
      " [ 0.08326195]\n",
      " [ 0.0769859 ]]\n",
      "the iterations of  331200 the loss is 0.27673370742216186 theta= [[-9.72165793]\n",
      " [ 0.08327323]\n",
      " [ 0.07699731]]\n",
      "the iterations of  331300 the loss is 0.27671315716931894 theta= [[-9.72309145]\n",
      " [ 0.08328451]\n",
      " [ 0.07700872]]\n",
      "the iterations of  331400 the loss is 0.27669261697221265 theta= [[-9.72452461]\n",
      " [ 0.08329579]\n",
      " [ 0.07702012]]\n",
      "the iterations of  331500 the loss is 0.27667208682339145 theta= [[-9.72595743]\n",
      " [ 0.08330707]\n",
      " [ 0.07703152]]\n",
      "the iterations of  331600 the loss is 0.27665156671541147 theta= [[-9.7273899 ]\n",
      " [ 0.08331834]\n",
      " [ 0.07704292]]\n",
      "the iterations of  331700 the loss is 0.2766310566408364 theta= [[-9.72882202]\n",
      " [ 0.08332961]\n",
      " [ 0.07705431]]\n",
      "the iterations of  331800 the loss is 0.27661055659223704 theta= [[-9.73025378]\n",
      " [ 0.08334088]\n",
      " [ 0.0770657 ]]\n",
      "the iterations of  331900 the loss is 0.27659006656219187 theta= [[-9.7316852 ]\n",
      " [ 0.08335215]\n",
      " [ 0.07707709]]\n",
      "the iterations of  332000 the loss is 0.27656958654328656 theta= [[-9.73311627]\n",
      " [ 0.08336341]\n",
      " [ 0.07708848]]\n",
      "the iterations of  332100 the loss is 0.27654911652811426 theta= [[-9.73454699]\n",
      " [ 0.08337467]\n",
      " [ 0.07709986]]\n",
      "the iterations of  332200 the loss is 0.2765286565092757 theta= [[-9.73597736]\n",
      " [ 0.08338593]\n",
      " [ 0.07711124]]\n",
      "the iterations of  332300 the loss is 0.27650820647937874 theta= [[-9.73740738]\n",
      " [ 0.08339718]\n",
      " [ 0.07712262]]\n",
      "the iterations of  332400 the loss is 0.2764877664310387 theta= [[-9.73883705]\n",
      " [ 0.08340843]\n",
      " [ 0.077134  ]]\n",
      "the iterations of  332500 the loss is 0.27646733635687815 theta= [[-9.74026637]\n",
      " [ 0.08341968]\n",
      " [ 0.07714537]]\n",
      "the iterations of  332600 the loss is 0.27644691624952705 theta= [[-9.74169534]\n",
      " [ 0.08343093]\n",
      " [ 0.07715674]]\n",
      "the iterations of  332700 the loss is 0.276426506101623 theta= [[-9.74312396]\n",
      " [ 0.08344217]\n",
      " [ 0.07716811]]\n",
      "the iterations of  332800 the loss is 0.27640610590581044 theta= [[-9.74455224]\n",
      " [ 0.08345341]\n",
      " [ 0.07717948]]\n",
      "the iterations of  332900 the loss is 0.27638571565474124 theta= [[-9.74598017]\n",
      " [ 0.08346465]\n",
      " [ 0.07719084]]\n",
      "the iterations of  333000 the loss is 0.27636533534107516 theta= [[-9.74740775]\n",
      " [ 0.08347589]\n",
      " [ 0.0772022 ]]\n",
      "the iterations of  333100 the loss is 0.2763449649574783 theta= [[-9.74883498]\n",
      " [ 0.08348712]\n",
      " [ 0.07721355]]\n",
      "the iterations of  333200 the loss is 0.2763246044966247 theta= [[-9.75026186]\n",
      " [ 0.08349835]\n",
      " [ 0.07722491]]\n",
      "the iterations of  333300 the loss is 0.2763042539511956 theta= [[-9.7516884 ]\n",
      " [ 0.08350958]\n",
      " [ 0.07723626]]\n",
      "the iterations of  333400 the loss is 0.27628391331387964 theta= [[-9.75311459]\n",
      " [ 0.0835208 ]\n",
      " [ 0.07724761]]\n",
      "the iterations of  333500 the loss is 0.276263582577372 theta= [[-9.75454043]\n",
      " [ 0.08353203]\n",
      " [ 0.07725895]]\n",
      "the iterations of  333600 the loss is 0.2762432617343761 theta= [[-9.75596593]\n",
      " [ 0.08354325]\n",
      " [ 0.0772703 ]]\n",
      "the iterations of  333700 the loss is 0.27622295077760184 theta= [[-9.75739108]\n",
      " [ 0.08355446]\n",
      " [ 0.07728164]]\n",
      "the iterations of  333800 the loss is 0.2762026496997668 theta= [[-9.75881588]\n",
      " [ 0.08356568]\n",
      " [ 0.07729297]]\n",
      "the iterations of  333900 the loss is 0.276182358493596 theta= [[-9.76024033]\n",
      " [ 0.08357689]\n",
      " [ 0.07730431]]\n",
      "the iterations of  334000 the loss is 0.276162077151821 theta= [[-9.76166444]\n",
      " [ 0.0835881 ]\n",
      " [ 0.07731564]]\n",
      "the iterations of  334100 the loss is 0.2761418056671808 theta= [[-9.76308821]\n",
      " [ 0.08359931]\n",
      " [ 0.07732697]]\n",
      "the iterations of  334200 the loss is 0.27612154403242195 theta= [[-9.76451162]\n",
      " [ 0.08361051]\n",
      " [ 0.0773383 ]]\n",
      "the iterations of  334300 the loss is 0.276101292240298 theta= [[-9.76593469]\n",
      " [ 0.08362171]\n",
      " [ 0.07734962]]\n",
      "the iterations of  334400 the loss is 0.2760810502835695 theta= [[-9.76735742]\n",
      " [ 0.08363291]\n",
      " [ 0.07736094]]\n",
      "the iterations of  334500 the loss is 0.2760608181550043 theta= [[-9.7687798 ]\n",
      " [ 0.08364411]\n",
      " [ 0.07737226]]\n",
      "the iterations of  334600 the loss is 0.27604059584737745 theta= [[-9.77020183]\n",
      " [ 0.0836553 ]\n",
      " [ 0.07738358]]\n",
      "the iterations of  334700 the loss is 0.2760203833534711 theta= [[-9.77162352]\n",
      " [ 0.08366649]\n",
      " [ 0.07739489]]\n",
      "the iterations of  334800 the loss is 0.27600018066607485 theta= [[-9.77304487]\n",
      " [ 0.08367768]\n",
      " [ 0.0774062 ]]\n",
      "the iterations of  334900 the loss is 0.27597998777798466 theta= [[-9.77446587]\n",
      " [ 0.08368886]\n",
      " [ 0.07741751]]\n",
      "the iterations of  335000 the loss is 0.27595980468200476 theta= [[-9.77588653]\n",
      " [ 0.08370004]\n",
      " [ 0.07742882]]\n",
      "the iterations of  335100 the loss is 0.2759396313709454 theta= [[-9.77730684]\n",
      " [ 0.08371122]\n",
      " [ 0.07744012]]\n",
      "the iterations of  335200 the loss is 0.2759194678376246 theta= [[-9.7787268 ]\n",
      " [ 0.0837224 ]\n",
      " [ 0.07745142]]\n",
      "the iterations of  335300 the loss is 0.27589931407486756 theta= [[-9.78014643]\n",
      " [ 0.08373358]\n",
      " [ 0.07746272]]\n",
      "the iterations of  335400 the loss is 0.2758791700755059 theta= [[-9.78156571]\n",
      " [ 0.08374475]\n",
      " [ 0.07747401]]\n",
      "the iterations of  335500 the loss is 0.27585903583237903 theta= [[-9.78298464]\n",
      " [ 0.08375592]\n",
      " [ 0.0774853 ]]\n",
      "the iterations of  335600 the loss is 0.275838911338333 theta= [[-9.78440323]\n",
      " [ 0.08376709]\n",
      " [ 0.07749659]]\n",
      "the iterations of  335700 the loss is 0.27581879658622127 theta= [[-9.78582148]\n",
      " [ 0.08377825]\n",
      " [ 0.07750788]]\n",
      "the iterations of  335800 the loss is 0.2757986915689041 theta= [[-9.78723938]\n",
      " [ 0.08378941]\n",
      " [ 0.07751916]]\n",
      "the iterations of  335900 the loss is 0.27577859627924867 theta= [[-9.78865695]\n",
      " [ 0.08380057]\n",
      " [ 0.07753044]]\n",
      "the iterations of  336000 the loss is 0.27575851071012975 theta= [[-9.79007417]\n",
      " [ 0.08381173]\n",
      " [ 0.07754172]]\n",
      "the iterations of  336100 the loss is 0.2757384348544286 theta= [[-9.79149104]\n",
      " [ 0.08382288]\n",
      " [ 0.077553  ]]\n",
      "the iterations of  336200 the loss is 0.27571836870503363 theta= [[-9.79290758]\n",
      " [ 0.08383403]\n",
      " [ 0.07756427]]\n",
      "the iterations of  336300 the loss is 0.27569831225484054 theta= [[-9.79432377]\n",
      " [ 0.08384518]\n",
      " [ 0.07757554]]\n",
      "the iterations of  336400 the loss is 0.275678265496752 theta= [[-9.79573962]\n",
      " [ 0.08385632]\n",
      " [ 0.07758681]]\n",
      "the iterations of  336500 the loss is 0.2756582284236771 theta= [[-9.79715512]\n",
      " [ 0.08386747]\n",
      " [ 0.07759808]]\n",
      "the iterations of  336600 the loss is 0.27563820102853276 theta= [[-9.79857029]\n",
      " [ 0.08387861]\n",
      " [ 0.07760934]]\n",
      "the iterations of  336700 the loss is 0.27561818330424226 theta= [[-9.79998511]\n",
      " [ 0.08388975]\n",
      " [ 0.0776206 ]]\n",
      "the iterations of  336800 the loss is 0.27559817524373614 theta= [[-9.80139959]\n",
      " [ 0.08390088]\n",
      " [ 0.07763185]]\n",
      "the iterations of  336900 the loss is 0.2755781768399516 theta= [[-9.80281373]\n",
      " [ 0.08391201]\n",
      " [ 0.07764311]]\n",
      "the iterations of  337000 the loss is 0.2755581880858334 theta= [[-9.80422753]\n",
      " [ 0.08392314]\n",
      " [ 0.07765436]]\n",
      "the iterations of  337100 the loss is 0.27553820897433273 theta= [[-9.80564099]\n",
      " [ 0.08393427]\n",
      " [ 0.07766561]]\n",
      "the iterations of  337200 the loss is 0.27551823949840776 theta= [[-9.80705411]\n",
      " [ 0.0839454 ]\n",
      " [ 0.07767686]]\n",
      "the iterations of  337300 the loss is 0.27549827965102364 theta= [[-9.80846688]\n",
      " [ 0.08395652]\n",
      " [ 0.0776881 ]]\n",
      "the iterations of  337400 the loss is 0.27547832942515266 theta= [[-9.80987932]\n",
      " [ 0.08396764]\n",
      " [ 0.07769934]]\n",
      "the iterations of  337500 the loss is 0.275458388813774 theta= [[-9.81129142]\n",
      " [ 0.08397876]\n",
      " [ 0.07771058]]\n",
      "the iterations of  337600 the loss is 0.27543845780987314 theta= [[-9.81270317]\n",
      " [ 0.08398987]\n",
      " [ 0.07772182]]\n",
      "the iterations of  337700 the loss is 0.27541853640644337 theta= [[-9.81411459]\n",
      " [ 0.08400098]\n",
      " [ 0.07773305]]\n",
      "the iterations of  337800 the loss is 0.2753986245964844 theta= [[-9.81552566]\n",
      " [ 0.08401209]\n",
      " [ 0.07774428]]\n",
      "the iterations of  337900 the loss is 0.2753787223730026 theta= [[-9.8169364 ]\n",
      " [ 0.0840232 ]\n",
      " [ 0.07775551]]\n",
      "the iterations of  338000 the loss is 0.2753588297290115 theta= [[-9.81834679]\n",
      " [ 0.0840343 ]\n",
      " [ 0.07776673]]\n",
      "the iterations of  338100 the loss is 0.2753389466575314 theta= [[-9.81975685]\n",
      " [ 0.0840454 ]\n",
      " [ 0.07777795]]\n",
      "the iterations of  338200 the loss is 0.27531907315158965 theta= [[-9.82116657]\n",
      " [ 0.0840565 ]\n",
      " [ 0.07778917]]\n",
      "the iterations of  338300 the loss is 0.27529920920422035 theta= [[-9.82257595]\n",
      " [ 0.0840676 ]\n",
      " [ 0.07780039]]\n",
      "the iterations of  338400 the loss is 0.2752793548084643 theta= [[-9.82398499]\n",
      " [ 0.08407869]\n",
      " [ 0.07781161]]\n",
      "the iterations of  338500 the loss is 0.2752595099573691 theta= [[-9.82539369]\n",
      " [ 0.08408978]\n",
      " [ 0.07782282]]\n",
      "the iterations of  338600 the loss is 0.2752396746439895 theta= [[-9.82680205]\n",
      " [ 0.08410087]\n",
      " [ 0.07783403]]\n",
      "the iterations of  338700 the loss is 0.2752198488613865 theta= [[-9.82821007]\n",
      " [ 0.08411195]\n",
      " [ 0.07784523]]\n",
      "the iterations of  338800 the loss is 0.2752000326026285 theta= [[-9.82961776]\n",
      " [ 0.08412304]\n",
      " [ 0.07785644]]\n",
      "the iterations of  338900 the loss is 0.2751802258607905 theta= [[-9.83102511]\n",
      " [ 0.08413412]\n",
      " [ 0.07786764]]\n",
      "the iterations of  339000 the loss is 0.2751604286289541 theta= [[-9.83243211]\n",
      " [ 0.0841452 ]\n",
      " [ 0.07787884]]\n",
      "the iterations of  339100 the loss is 0.2751406409002079 theta= [[-9.83383879]\n",
      " [ 0.08415627]\n",
      " [ 0.07789003]]\n",
      "the iterations of  339200 the loss is 0.27512086266764707 theta= [[-9.83524512]\n",
      " [ 0.08416734]\n",
      " [ 0.07790123]]\n",
      "the iterations of  339300 the loss is 0.27510109392437365 theta= [[-9.83665112]\n",
      " [ 0.08417841]\n",
      " [ 0.07791242]]\n",
      "the iterations of  339400 the loss is 0.2750813346634965 theta= [[-9.83805678]\n",
      " [ 0.08418948]\n",
      " [ 0.07792361]]\n",
      "the iterations of  339500 the loss is 0.27506158487813115 theta= [[-9.8394621 ]\n",
      " [ 0.08420055]\n",
      " [ 0.07793479]]\n",
      "the iterations of  339600 the loss is 0.27504184456139996 theta= [[-9.84086709]\n",
      " [ 0.08421161]\n",
      " [ 0.07794598]]\n",
      "the iterations of  339700 the loss is 0.27502211370643187 theta= [[-9.84227173]\n",
      " [ 0.08422267]\n",
      " [ 0.07795716]]\n",
      "the iterations of  339800 the loss is 0.2750023923063626 theta= [[-9.84367605]\n",
      " [ 0.08423373]\n",
      " [ 0.07796833]]\n",
      "the iterations of  339900 the loss is 0.2749826803543342 theta= [[-9.84508002]\n",
      " [ 0.08424478]\n",
      " [ 0.07797951]]\n",
      "the iterations of  340000 the loss is 0.27496297784349655 theta= [[-9.84648366]\n",
      " [ 0.08425583]\n",
      " [ 0.07799068]]\n",
      "the iterations of  340100 the loss is 0.2749432847670051 theta= [[-9.84788697]\n",
      " [ 0.08426688]\n",
      " [ 0.07800185]]\n",
      "the iterations of  340200 the loss is 0.2749236011180225 theta= [[-9.84928993]\n",
      " [ 0.08427793]\n",
      " [ 0.07801302]]\n",
      "the iterations of  340300 the loss is 0.274903926889718 theta= [[-9.85069256]\n",
      " [ 0.08428897]\n",
      " [ 0.07802418]]\n",
      "the iterations of  340400 the loss is 0.2748842620752671 theta= [[-9.85209486]\n",
      " [ 0.08430001]\n",
      " [ 0.07803534]]\n",
      "the iterations of  340500 the loss is 0.27486460666785273 theta= [[-9.85349682]\n",
      " [ 0.08431105]\n",
      " [ 0.0780465 ]]\n",
      "the iterations of  340600 the loss is 0.274844960660664 theta= [[-9.85489845]\n",
      " [ 0.08432209]\n",
      " [ 0.07805766]]\n",
      "the iterations of  340700 the loss is 0.27482532404689647 theta= [[-9.85629974]\n",
      " [ 0.08433312]\n",
      " [ 0.07806881]]\n",
      "the iterations of  340800 the loss is 0.274805696819753 theta= [[-9.85770069]\n",
      " [ 0.08434416]\n",
      " [ 0.07807997]]\n",
      "the iterations of  340900 the loss is 0.2747860789724427 theta= [[-9.85910131]\n",
      " [ 0.08435518]\n",
      " [ 0.07809111]]\n",
      "the iterations of  341000 the loss is 0.27476647049818104 theta= [[-9.8605016 ]\n",
      " [ 0.08436621]\n",
      " [ 0.07810226]]\n",
      "the iterations of  341100 the loss is 0.27474687139019077 theta= [[-9.86190155]\n",
      " [ 0.08437723]\n",
      " [ 0.0781134 ]]\n",
      "the iterations of  341200 the loss is 0.27472728164170046 theta= [[-9.86330117]\n",
      " [ 0.08438825]\n",
      " [ 0.07812454]]\n",
      "the iterations of  341300 the loss is 0.27470770124594585 theta= [[-9.86470045]\n",
      " [ 0.08439927]\n",
      " [ 0.07813568]]\n",
      "the iterations of  341400 the loss is 0.27468813019616894 theta= [[-9.8660994 ]\n",
      " [ 0.08441029]\n",
      " [ 0.07814682]]\n",
      "the iterations of  341500 the loss is 0.27466856848561866 theta= [[-9.86749801]\n",
      " [ 0.0844213 ]\n",
      " [ 0.07815795]]\n",
      "the iterations of  341600 the loss is 0.2746490161075502 theta= [[-9.86889629]\n",
      " [ 0.08443231]\n",
      " [ 0.07816908]]\n",
      "the iterations of  341700 the loss is 0.27462947305522545 theta= [[-9.87029424]\n",
      " [ 0.08444332]\n",
      " [ 0.07818021]]\n",
      "the iterations of  341800 the loss is 0.2746099393219129 theta= [[-9.87169186]\n",
      " [ 0.08445433]\n",
      " [ 0.07819134]]\n",
      "the iterations of  341900 the loss is 0.27459041490088754 theta= [[-9.87308914]\n",
      " [ 0.08446533]\n",
      " [ 0.07820246]]\n",
      "the iterations of  342000 the loss is 0.27457089978543064 theta= [[-9.87448609]\n",
      " [ 0.08447633]\n",
      " [ 0.07821358]]\n",
      "the iterations of  342100 the loss is 0.2745513939688306 theta= [[-9.8758827 ]\n",
      " [ 0.08448733]\n",
      " [ 0.0782247 ]]\n",
      "the iterations of  342200 the loss is 0.2745318974443819 theta= [[-9.87727898]\n",
      " [ 0.08449832]\n",
      " [ 0.07823581]]\n",
      "the iterations of  342300 the loss is 0.27451241020538525 theta= [[-9.87867493]\n",
      " [ 0.08450932]\n",
      " [ 0.07824692]]\n",
      "the iterations of  342400 the loss is 0.27449293224514887 theta= [[-9.88007055]\n",
      " [ 0.08452031]\n",
      " [ 0.07825803]]\n",
      "the iterations of  342500 the loss is 0.2744734635569866 theta= [[-9.88146584]\n",
      " [ 0.0845313 ]\n",
      " [ 0.07826914]]\n",
      "the iterations of  342600 the loss is 0.2744540041342189 theta= [[-9.88286079]\n",
      " [ 0.08454228]\n",
      " [ 0.07828024]]\n",
      "the iterations of  342700 the loss is 0.274434553970173 theta= [[-9.88425541]\n",
      " [ 0.08455326]\n",
      " [ 0.07829135]]\n",
      "the iterations of  342800 the loss is 0.27441511305818267 theta= [[-9.8856497 ]\n",
      " [ 0.08456424]\n",
      " [ 0.07830245]]\n",
      "the iterations of  342900 the loss is 0.2743956813915879 theta= [[-9.88704366]\n",
      " [ 0.08457522]\n",
      " [ 0.07831354]]\n",
      "the iterations of  343000 the loss is 0.27437625896373496 theta= [[-9.88843729]\n",
      " [ 0.0845862 ]\n",
      " [ 0.07832464]]\n",
      "the iterations of  343100 the loss is 0.2743568457679771 theta= [[-9.88983058]\n",
      " [ 0.08459717]\n",
      " [ 0.07833573]]\n",
      "the iterations of  343200 the loss is 0.2743374417976732 theta= [[-9.89122355]\n",
      " [ 0.08460814]\n",
      " [ 0.07834682]]\n",
      "the iterations of  343300 the loss is 0.2743180470461898 theta= [[-9.89261618]\n",
      " [ 0.08461911]\n",
      " [ 0.0783579 ]]\n",
      "the iterations of  343400 the loss is 0.27429866150689874 theta= [[-9.89400848]\n",
      " [ 0.08463007]\n",
      " [ 0.07836899]]\n",
      "the iterations of  343500 the loss is 0.2742792851731789 theta= [[-9.89540045]\n",
      " [ 0.08464103]\n",
      " [ 0.07838007]]\n",
      "the iterations of  343600 the loss is 0.2742599180384152 theta= [[-9.8967921 ]\n",
      " [ 0.08465199]\n",
      " [ 0.07839115]]\n",
      "the iterations of  343700 the loss is 0.27424056009599945 theta= [[-9.89818341]\n",
      " [ 0.08466295]\n",
      " [ 0.07840222]]\n",
      "the iterations of  343800 the loss is 0.2742212113393296 theta= [[-9.89957439]\n",
      " [ 0.0846739 ]\n",
      " [ 0.0784133 ]]\n",
      "the iterations of  343900 the loss is 0.27420187176180955 theta= [[-9.90096504]\n",
      " [ 0.08468486]\n",
      " [ 0.07842437]]\n",
      "the iterations of  344000 the loss is 0.2741825413568505 theta= [[-9.90235536]\n",
      " [ 0.08469581]\n",
      " [ 0.07843543]]\n",
      "the iterations of  344100 the loss is 0.274163220117869 theta= [[-9.90374535]\n",
      " [ 0.08470675]\n",
      " [ 0.0784465 ]]\n",
      "the iterations of  344200 the loss is 0.27414390803828903 theta= [[-9.90513501]\n",
      " [ 0.0847177 ]\n",
      " [ 0.07845756]]\n",
      "the iterations of  344300 the loss is 0.2741246051115403 theta= [[-9.90652435]\n",
      " [ 0.08472864]\n",
      " [ 0.07846862]]\n",
      "the iterations of  344400 the loss is 0.27410531133105887 theta= [[-9.90791335]\n",
      " [ 0.08473958]\n",
      " [ 0.07847968]]\n",
      "the iterations of  344500 the loss is 0.2740860266902871 theta= [[-9.90930203]\n",
      " [ 0.08475052]\n",
      " [ 0.07849074]]\n",
      "the iterations of  344600 the loss is 0.2740667511826741 theta= [[-9.91069037]\n",
      " [ 0.08476145]\n",
      " [ 0.07850179]]\n",
      "the iterations of  344700 the loss is 0.27404748480167496 theta= [[-9.91207839]\n",
      " [ 0.08477238]\n",
      " [ 0.07851284]]\n",
      "the iterations of  344800 the loss is 0.2740282275407513 theta= [[-9.91346608]\n",
      " [ 0.08478331]\n",
      " [ 0.07852389]]\n",
      "the iterations of  344900 the loss is 0.2740089793933707 theta= [[-9.91485344]\n",
      " [ 0.08479424]\n",
      " [ 0.07853493]]\n",
      "the iterations of  345000 the loss is 0.2739897403530075 theta= [[-9.91624047]\n",
      " [ 0.08480516]\n",
      " [ 0.07854598]]\n",
      "the iterations of  345100 the loss is 0.2739705104131421 theta= [[-9.91762717]\n",
      " [ 0.08481608]\n",
      " [ 0.07855702]]\n",
      "the iterations of  345200 the loss is 0.2739512895672613 theta= [[-9.91901355]\n",
      " [ 0.084827  ]\n",
      " [ 0.07856805]]\n",
      "the iterations of  345300 the loss is 0.2739320778088579 theta= [[-9.9203996 ]\n",
      " [ 0.08483792]\n",
      " [ 0.07857909]]\n",
      "the iterations of  345400 the loss is 0.2739128751314315 theta= [[-9.92178532]\n",
      " [ 0.08484883]\n",
      " [ 0.07859012]]\n",
      "the iterations of  345500 the loss is 0.27389368152848725 theta= [[-9.92317071]\n",
      " [ 0.08485975]\n",
      " [ 0.07860115]]\n",
      "the iterations of  345600 the loss is 0.2738744969935371 theta= [[-9.92455577]\n",
      " [ 0.08487066]\n",
      " [ 0.07861218]]\n",
      "the iterations of  345700 the loss is 0.27385532152009945 theta= [[-9.92594051]\n",
      " [ 0.08488156]\n",
      " [ 0.0786232 ]]\n",
      "the iterations of  345800 the loss is 0.27383615510169823 theta= [[-9.92732492]\n",
      " [ 0.08489247]\n",
      " [ 0.07863422]]\n",
      "the iterations of  345900 the loss is 0.27381699773186413 theta= [[-9.92870901]\n",
      " [ 0.08490337]\n",
      " [ 0.07864524]]\n",
      "the iterations of  346000 the loss is 0.2737978494041341 theta= [[-9.93009277]\n",
      " [ 0.08491427]\n",
      " [ 0.07865626]]\n",
      "the iterations of  346100 the loss is 0.2737787101120508 theta= [[-9.9314762 ]\n",
      " [ 0.08492516]\n",
      " [ 0.07866727]]\n",
      "the iterations of  346200 the loss is 0.2737595798491638 theta= [[-9.9328593 ]\n",
      " [ 0.08493606]\n",
      " [ 0.07867829]]\n",
      "the iterations of  346300 the loss is 0.2737404586090286 theta= [[-9.93424208]\n",
      " [ 0.08494695]\n",
      " [ 0.0786893 ]]\n",
      "the iterations of  346400 the loss is 0.2737213463852066 theta= [[-9.93562453]\n",
      " [ 0.08495784]\n",
      " [ 0.0787003 ]]\n",
      "the iterations of  346500 the loss is 0.2737022431712658 theta= [[-9.93700666]\n",
      " [ 0.08496873]\n",
      " [ 0.07871131]]\n",
      "the iterations of  346600 the loss is 0.27368314896078016 theta= [[-9.93838846]\n",
      " [ 0.08497961]\n",
      " [ 0.07872231]]\n",
      "the iterations of  346700 the loss is 0.2736640637473299 theta= [[-9.93976994]\n",
      " [ 0.08499049]\n",
      " [ 0.07873331]]\n",
      "the iterations of  346800 the loss is 0.2736449875245013 theta= [[-9.94115109]\n",
      " [ 0.08500137]\n",
      " [ 0.0787443 ]]\n",
      "the iterations of  346900 the loss is 0.273625920285887 theta= [[-9.94253191]\n",
      " [ 0.08501225]\n",
      " [ 0.0787553 ]]\n",
      "the iterations of  347000 the loss is 0.27360686202508594 theta= [[-9.94391241]\n",
      " [ 0.08502312]\n",
      " [ 0.07876629]]\n",
      "the iterations of  347100 the loss is 0.2735878127357027 theta= [[-9.94529259]\n",
      " [ 0.08503399]\n",
      " [ 0.07877728]]\n",
      "the iterations of  347200 the loss is 0.2735687724113483 theta= [[-9.94667244]\n",
      " [ 0.08504486]\n",
      " [ 0.07878827]]\n",
      "the iterations of  347300 the loss is 0.27354974104563995 theta= [[-9.94805196]\n",
      " [ 0.08505573]\n",
      " [ 0.07879925]]\n",
      "the iterations of  347400 the loss is 0.27353071863220063 theta= [[-9.94943116]\n",
      " [ 0.08506659]\n",
      " [ 0.07881023]]\n",
      "the iterations of  347500 the loss is 0.2735117051646601 theta= [[-9.95081004]\n",
      " [ 0.08507745]\n",
      " [ 0.07882121]]\n",
      "the iterations of  347600 the loss is 0.27349270063665354 theta= [[-9.95218859]\n",
      " [ 0.08508831]\n",
      " [ 0.07883219]]\n",
      "the iterations of  347700 the loss is 0.27347370504182267 theta= [[-9.95356682]\n",
      " [ 0.08509917]\n",
      " [ 0.07884316]]\n",
      "the iterations of  347800 the loss is 0.2734547183738154 theta= [[-9.95494472]\n",
      " [ 0.08511002]\n",
      " [ 0.07885413]]\n",
      "the iterations of  347900 the loss is 0.27343574062628523 theta= [[-9.9563223 ]\n",
      " [ 0.08512088]\n",
      " [ 0.0788651 ]]\n",
      "the iterations of  348000 the loss is 0.2734167717928922 theta= [[-9.95769956]\n",
      " [ 0.08513172]\n",
      " [ 0.07887607]]\n",
      "the iterations of  348100 the loss is 0.2733978118673022 theta= [[-9.95907649]\n",
      " [ 0.08514257]\n",
      " [ 0.07888703]]\n",
      "the iterations of  348200 the loss is 0.27337886084318724 theta= [[-9.9604531 ]\n",
      " [ 0.08515341]\n",
      " [ 0.07889799]]\n",
      "the iterations of  348300 the loss is 0.2733599187142254 theta= [[-9.96182939]\n",
      " [ 0.08516426]\n",
      " [ 0.07890895]]\n",
      "the iterations of  348400 the loss is 0.27334098547410096 theta= [[-9.96320535]\n",
      " [ 0.0851751 ]\n",
      " [ 0.0789199 ]]\n",
      "the iterations of  348500 the loss is 0.2733220611165038 theta= [[-9.96458099]\n",
      " [ 0.08518593]\n",
      " [ 0.07893086]]\n",
      "the iterations of  348600 the loss is 0.27330314563513025 theta= [[-9.96595631]\n",
      " [ 0.08519677]\n",
      " [ 0.07894181]]\n",
      "the iterations of  348700 the loss is 0.27328423902368265 theta= [[-9.96733131]\n",
      " [ 0.0852076 ]\n",
      " [ 0.07895276]]\n",
      "the iterations of  348800 the loss is 0.27326534127586916 theta= [[-9.96870598]\n",
      " [ 0.08521843]\n",
      " [ 0.0789637 ]]\n",
      "the iterations of  348900 the loss is 0.27324645238540424 theta= [[-9.97008033]\n",
      " [ 0.08522925]\n",
      " [ 0.07897465]]\n",
      "the iterations of  349000 the loss is 0.27322757234600803 theta= [[-9.97145436]\n",
      " [ 0.08524008]\n",
      " [ 0.07898559]]\n",
      "the iterations of  349100 the loss is 0.27320870115140705 theta= [[-9.97282807]\n",
      " [ 0.0852509 ]\n",
      " [ 0.07899653]]\n",
      "the iterations of  349200 the loss is 0.2731898387953332 theta= [[-9.97420146]\n",
      " [ 0.08526172]\n",
      " [ 0.07900746]]\n",
      "the iterations of  349300 the loss is 0.2731709852715252 theta= [[-9.97557452]\n",
      " [ 0.08527254]\n",
      " [ 0.07901839]]\n",
      "the iterations of  349400 the loss is 0.2731521405737272 theta= [[-9.97694726]\n",
      " [ 0.08528335]\n",
      " [ 0.07902933]]\n",
      "the iterations of  349500 the loss is 0.27313330469568964 theta= [[-9.97831969]\n",
      " [ 0.08529416]\n",
      " [ 0.07904025]]\n",
      "the iterations of  349600 the loss is 0.27311447763116836 theta= [[-9.97969179]\n",
      " [ 0.08530497]\n",
      " [ 0.07905118]]\n",
      "the iterations of  349700 the loss is 0.2730956593739257 theta= [[-9.98106357]\n",
      " [ 0.08531578]\n",
      " [ 0.0790621 ]]\n",
      "the iterations of  349800 the loss is 0.27307684991772985 theta= [[-9.98243502]\n",
      " [ 0.08532658]\n",
      " [ 0.07907302]]\n",
      "the iterations of  349900 the loss is 0.2730580492563549 theta= [[-9.98380616]\n",
      " [ 0.08533739]\n",
      " [ 0.07908394]]\n",
      "the iterations of  350000 the loss is 0.2730392573835807 theta= [[-9.98517698]\n",
      " [ 0.08534819]\n",
      " [ 0.07909486]]\n",
      "the iterations of  350100 the loss is 0.27302047429319337 theta= [[-9.98654748]\n",
      " [ 0.08535898]\n",
      " [ 0.07910577]]\n",
      "the iterations of  350200 the loss is 0.27300169997898477 theta= [[-9.98791765]\n",
      " [ 0.08536978]\n",
      " [ 0.07911668]]\n",
      "the iterations of  350300 the loss is 0.27298293443475274 theta= [[-9.98928751]\n",
      " [ 0.08538057]\n",
      " [ 0.07912759]]\n",
      "the iterations of  350400 the loss is 0.27296417765430087 theta= [[-9.99065705]\n",
      " [ 0.08539136]\n",
      " [ 0.0791385 ]]\n",
      "the iterations of  350500 the loss is 0.2729454296314389 theta= [[-9.99202626]\n",
      " [ 0.08540215]\n",
      " [ 0.0791494 ]]\n",
      "the iterations of  350600 the loss is 0.27292669035998207 theta= [[-9.99339516]\n",
      " [ 0.08541293]\n",
      " [ 0.0791603 ]]\n",
      "the iterations of  350700 the loss is 0.2729079598337521 theta= [[-9.99476374]\n",
      " [ 0.08542371]\n",
      " [ 0.0791712 ]]\n",
      "the iterations of  350800 the loss is 0.27288923804657617 theta= [[-9.996132  ]\n",
      " [ 0.08543449]\n",
      " [ 0.07918209]]\n",
      "the iterations of  350900 the loss is 0.2728705249922873 theta= [[-9.99749994]\n",
      " [ 0.08544527]\n",
      " [ 0.07919299]]\n",
      "the iterations of  351000 the loss is 0.27285182066472463 theta= [[-9.99886756]\n",
      " [ 0.08545605]\n",
      " [ 0.07920388]]\n",
      "the iterations of  351100 the loss is 0.2728331250577332 theta= [[-10.00023486]\n",
      " [  0.08546682]\n",
      " [  0.07921477]]\n",
      "the iterations of  351200 the loss is 0.2728144381651636 theta= [[-10.00160184]\n",
      " [  0.08547759]\n",
      " [  0.07922565]]\n",
      "the iterations of  351300 the loss is 0.2727957599808723 theta= [[-10.00296851]\n",
      " [  0.08548836]\n",
      " [  0.07923654]]\n",
      "the iterations of  351400 the loss is 0.2727770904987219 theta= [[-10.00433485]\n",
      " [  0.08549912]\n",
      " [  0.07924742]]\n",
      "the iterations of  351500 the loss is 0.2727584297125805 theta= [[-10.00570088]\n",
      " [  0.08550988]\n",
      " [  0.07925829]]\n",
      "the iterations of  351600 the loss is 0.27273977761632245 theta= [[-10.00706659]\n",
      " [  0.08552064]\n",
      " [  0.07926917]]\n",
      "the iterations of  351700 the loss is 0.27272113420382743 theta= [[-10.00843198]\n",
      " [  0.0855314 ]\n",
      " [  0.07928004]]\n",
      "the iterations of  351800 the loss is 0.27270249946898134 theta= [[-10.00979706]\n",
      " [  0.08554216]\n",
      " [  0.07929091]]\n",
      "the iterations of  351900 the loss is 0.2726838734056755 theta= [[-10.01116181]\n",
      " [  0.08555291]\n",
      " [  0.07930178]]\n",
      "the iterations of  352000 the loss is 0.2726652560078072 theta= [[-10.01252625]\n",
      " [  0.08556366]\n",
      " [  0.07931265]]\n",
      "the iterations of  352100 the loss is 0.27264664726928 theta= [[-10.01389037]\n",
      " [  0.08557441]\n",
      " [  0.07932351]]\n",
      "the iterations of  352200 the loss is 0.27262804718400235 theta= [[-10.01525418]\n",
      " [  0.08558515]\n",
      " [  0.07933437]]\n",
      "the iterations of  352300 the loss is 0.2726094557458893 theta= [[-10.01661766]\n",
      " [  0.0855959 ]\n",
      " [  0.07934523]]\n",
      "the iterations of  352400 the loss is 0.27259087294886114 theta= [[-10.01798083]\n",
      " [  0.08560664]\n",
      " [  0.07935609]]\n",
      "the iterations of  352500 the loss is 0.2725722987868439 theta= [[-10.01934369]\n",
      " [  0.08561738]\n",
      " [  0.07936694]]\n",
      "the iterations of  352600 the loss is 0.27255373325376997 theta= [[-10.02070622]\n",
      " [  0.08562811]\n",
      " [  0.07937779]]\n",
      "the iterations of  352700 the loss is 0.2725351763435767 theta= [[-10.02206844]\n",
      " [  0.08563885]\n",
      " [  0.07938864]]\n",
      "the iterations of  352800 the loss is 0.2725166280502078 theta= [[-10.02343035]\n",
      " [  0.08564958]\n",
      " [  0.07939949]]\n",
      "the iterations of  352900 the loss is 0.2724980883676124 theta= [[-10.02479194]\n",
      " [  0.08566031]\n",
      " [  0.07941033]]\n",
      "the iterations of  353000 the loss is 0.2724795572897455 theta= [[-10.02615321]\n",
      " [  0.08567103]\n",
      " [  0.07942117]]\n",
      "the iterations of  353100 the loss is 0.27246103481056755 theta= [[-10.02751416]\n",
      " [  0.08568176]\n",
      " [  0.07943201]]\n",
      "the iterations of  353200 the loss is 0.27244252092404525 theta= [[-10.0288748 ]\n",
      " [  0.08569248]\n",
      " [  0.07944285]]\n",
      "the iterations of  353300 the loss is 0.2724240156241505 theta= [[-10.03023513]\n",
      " [  0.0857032 ]\n",
      " [  0.07945368]]\n",
      "the iterations of  353400 the loss is 0.272405518904861 theta= [[-10.03159514]\n",
      " [  0.08571391]\n",
      " [  0.07946451]]\n",
      "the iterations of  353500 the loss is 0.27238703076016046 theta= [[-10.03295483]\n",
      " [  0.08572463]\n",
      " [  0.07947534]]\n",
      "the iterations of  353600 the loss is 0.27236855118403797 theta= [[-10.03431421]\n",
      " [  0.08573534]\n",
      " [  0.07948617]]\n",
      "the iterations of  353700 the loss is 0.2723500801704885 theta= [[-10.03567327]\n",
      " [  0.08574605]\n",
      " [  0.07949699]]\n",
      "the iterations of  353800 the loss is 0.27233161771351244 theta= [[-10.03703202]\n",
      " [  0.08575675]\n",
      " [  0.07950781]]\n",
      "the iterations of  353900 the loss is 0.27231316380711607 theta= [[-10.03839046]\n",
      " [  0.08576746]\n",
      " [  0.07951863]]\n",
      "the iterations of  354000 the loss is 0.2722947184453113 theta= [[-10.03974858]\n",
      " [  0.08577816]\n",
      " [  0.07952945]]\n",
      "the iterations of  354100 the loss is 0.2722762816221154 theta= [[-10.04110638]\n",
      " [  0.08578886]\n",
      " [  0.07954026]]\n",
      "the iterations of  354200 the loss is 0.2722578533315519 theta= [[-10.04246387]\n",
      " [  0.08579956]\n",
      " [  0.07955107]]\n",
      "the iterations of  354300 the loss is 0.2722394335676497 theta= [[-10.04382105]\n",
      " [  0.08581025]\n",
      " [  0.07956188]]\n",
      "the iterations of  354400 the loss is 0.27222102232444284 theta= [[-10.04517791]\n",
      " [  0.08582094]\n",
      " [  0.07957269]]\n",
      "the iterations of  354500 the loss is 0.2722026195959717 theta= [[-10.04653446]\n",
      " [  0.08583163]\n",
      " [  0.07958349]]\n",
      "the iterations of  354600 the loss is 0.27218422537628206 theta= [[-10.0478907 ]\n",
      " [  0.08584232]\n",
      " [  0.07959429]]\n",
      "the iterations of  354700 the loss is 0.2721658396594251 theta= [[-10.04924662]\n",
      " [  0.085853  ]\n",
      " [  0.07960509]]\n",
      "the iterations of  354800 the loss is 0.2721474624394577 theta= [[-10.05060223]\n",
      " [  0.08586369]\n",
      " [  0.07961589]]\n",
      "the iterations of  354900 the loss is 0.2721290937104424 theta= [[-10.05195753]\n",
      " [  0.08587437]\n",
      " [  0.07962668]]\n",
      "the iterations of  355000 the loss is 0.27211073346644754 theta= [[-10.05331251]\n",
      " [  0.08588504]\n",
      " [  0.07963748]]\n",
      "the iterations of  355100 the loss is 0.2720923817015467 theta= [[-10.05466718]\n",
      " [  0.08589572]\n",
      " [  0.07964827]]\n",
      "the iterations of  355200 the loss is 0.2720740384098191 theta= [[-10.05602154]\n",
      " [  0.08590639]\n",
      " [  0.07965905]]\n",
      "the iterations of  355300 the loss is 0.27205570358534986 theta= [[-10.05737558]\n",
      " [  0.08591706]\n",
      " [  0.07966984]]\n",
      "the iterations of  355400 the loss is 0.27203737722222937 theta= [[-10.05872931]\n",
      " [  0.08592773]\n",
      " [  0.07968062]]\n",
      "the iterations of  355500 the loss is 0.27201905931455345 theta= [[-10.06008273]\n",
      " [  0.0859384 ]\n",
      " [  0.0796914 ]]\n",
      "the iterations of  355600 the loss is 0.272000749856424 theta= [[-10.06143584]\n",
      " [  0.08594906]\n",
      " [  0.07970218]]\n",
      "the iterations of  355700 the loss is 0.2719824488419478 theta= [[-10.06278863]\n",
      " [  0.08595972]\n",
      " [  0.07971295]]\n",
      "the iterations of  355800 the loss is 0.2719641562652375 theta= [[-10.06414112]\n",
      " [  0.08597038]\n",
      " [  0.07972372]]\n",
      "the iterations of  355900 the loss is 0.2719458721204117 theta= [[-10.06549329]\n",
      " [  0.08598104]\n",
      " [  0.07973449]]\n",
      "the iterations of  356000 the loss is 0.2719275964015937 theta= [[-10.06684515]\n",
      " [  0.08599169]\n",
      " [  0.07974526]]\n",
      "the iterations of  356100 the loss is 0.2719093291029132 theta= [[-10.0681967 ]\n",
      " [  0.08600234]\n",
      " [  0.07975603]]\n",
      "the iterations of  356200 the loss is 0.27189107021850484 theta= [[-10.06954793]\n",
      " [  0.08601299]\n",
      " [  0.07976679]]\n",
      "the iterations of  356300 the loss is 0.2718728197425088 theta= [[-10.07089886]\n",
      " [  0.08602364]\n",
      " [  0.07977755]]\n",
      "the iterations of  356400 the loss is 0.271854577669071 theta= [[-10.07224947]\n",
      " [  0.08603428]\n",
      " [  0.07978831]]\n",
      "the iterations of  356500 the loss is 0.2718363439923427 theta= [[-10.07359978]\n",
      " [  0.08604492]\n",
      " [  0.07979906]]\n",
      "the iterations of  356600 the loss is 0.2718181187064805 theta= [[-10.07494977]\n",
      " [  0.08605556]\n",
      " [  0.07980982]]\n",
      "the iterations of  356700 the loss is 0.2717999018056468 theta= [[-10.07629945]\n",
      " [  0.0860662 ]\n",
      " [  0.07982057]]\n",
      "the iterations of  356800 the loss is 0.2717816932840093 theta= [[-10.07764883]\n",
      " [  0.08607683]\n",
      " [  0.07983131]]\n",
      "the iterations of  356900 the loss is 0.2717634931357412 theta= [[-10.07899789]\n",
      " [  0.08608746]\n",
      " [  0.07984206]]\n",
      "the iterations of  357000 the loss is 0.2717453013550215 theta= [[-10.08034664]\n",
      " [  0.08609809]\n",
      " [  0.0798528 ]]\n",
      "the iterations of  357100 the loss is 0.2717271179360338 theta= [[-10.08169508]\n",
      " [  0.08610872]\n",
      " [  0.07986354]]\n",
      "the iterations of  357200 the loss is 0.27170894287296826 theta= [[-10.08304321]\n",
      " [  0.08611935]\n",
      " [  0.07987428]]\n",
      "the iterations of  357300 the loss is 0.27169077616001963 theta= [[-10.08439104]\n",
      " [  0.08612997]\n",
      " [  0.07988502]]\n",
      "the iterations of  357400 the loss is 0.2716726177913883 theta= [[-10.08573855]\n",
      " [  0.08614059]\n",
      " [  0.07989575]]\n",
      "the iterations of  357500 the loss is 0.27165446776128044 theta= [[-10.08708575]\n",
      " [  0.08615121]\n",
      " [  0.07990648]]\n",
      "the iterations of  357600 the loss is 0.27163632606390714 theta= [[-10.08843265]\n",
      " [  0.08616182]\n",
      " [  0.07991721]]\n",
      "the iterations of  357700 the loss is 0.27161819269348536 theta= [[-10.08977923]\n",
      " [  0.08617243]\n",
      " [  0.07992794]]\n",
      "the iterations of  357800 the loss is 0.27160006764423733 theta= [[-10.09112551]\n",
      " [  0.08618305]\n",
      " [  0.07993866]]\n",
      "the iterations of  357900 the loss is 0.2715819509103903 theta= [[-10.09247147]\n",
      " [  0.08619365]\n",
      " [  0.07994938]]\n",
      "the iterations of  358000 the loss is 0.2715638424861775 theta= [[-10.09381713]\n",
      " [  0.08620426]\n",
      " [  0.0799601 ]]\n",
      "the iterations of  358100 the loss is 0.27154574236583745 theta= [[-10.09516248]\n",
      " [  0.08621486]\n",
      " [  0.07997082]]\n",
      "the iterations of  358200 the loss is 0.27152765054361366 theta= [[-10.09650752]\n",
      " [  0.08622546]\n",
      " [  0.07998153]]\n",
      "the iterations of  358300 the loss is 0.27150956701375545 theta= [[-10.09785225]\n",
      " [  0.08623606]\n",
      " [  0.07999224]]\n",
      "the iterations of  358400 the loss is 0.27149149177051735 theta= [[-10.09919668]\n",
      " [  0.08624666]\n",
      " [  0.08000295]]\n",
      "the iterations of  358500 the loss is 0.2714734248081591 theta= [[-10.1005408 ]\n",
      " [  0.08625725]\n",
      " [  0.08001366]]\n",
      "the iterations of  358600 the loss is 0.2714553661209463 theta= [[-10.1018846 ]\n",
      " [  0.08626784]\n",
      " [  0.08002436]]\n",
      "the iterations of  358700 the loss is 0.27143731570314916 theta= [[-10.10322811]\n",
      " [  0.08627843]\n",
      " [  0.08003507]]\n",
      "the iterations of  358800 the loss is 0.271419273549044 theta= [[-10.1045713 ]\n",
      " [  0.08628902]\n",
      " [  0.08004577]]\n",
      "the iterations of  358900 the loss is 0.2714012396529118 theta= [[-10.10591419]\n",
      " [  0.08629961]\n",
      " [  0.08005646]]\n",
      "the iterations of  359000 the loss is 0.27138321400903936 theta= [[-10.10725676]\n",
      " [  0.08631019]\n",
      " [  0.08006716]]\n",
      "the iterations of  359100 the loss is 0.27136519661171876 theta= [[-10.10859904]\n",
      " [  0.08632077]\n",
      " [  0.08007785]]\n",
      "the iterations of  359200 the loss is 0.2713471874552475 theta= [[-10.109941  ]\n",
      " [  0.08633134]\n",
      " [  0.08008854]]\n",
      "the iterations of  359300 the loss is 0.27132918653392785 theta= [[-10.11128266]\n",
      " [  0.08634192]\n",
      " [  0.08009923]]\n",
      "the iterations of  359400 the loss is 0.2713111938420679 theta= [[-10.11262401]\n",
      " [  0.08635249]\n",
      " [  0.08010991]]\n",
      "the iterations of  359500 the loss is 0.27129320937398105 theta= [[-10.11396505]\n",
      " [  0.08636306]\n",
      " [  0.0801206 ]]\n",
      "the iterations of  359600 the loss is 0.2712752331239856 theta= [[-10.11530579]\n",
      " [  0.08637363]\n",
      " [  0.08013128]]\n",
      "the iterations of  359700 the loss is 0.2712572650864056 theta= [[-10.11664622]\n",
      " [  0.0863842 ]\n",
      " [  0.08014196]]\n",
      "the iterations of  359800 the loss is 0.27123930525556994 theta= [[-10.11798635]\n",
      " [  0.08639476]\n",
      " [  0.08015263]]\n",
      "the iterations of  359900 the loss is 0.27122135362581345 theta= [[-10.11932617]\n",
      " [  0.08640532]\n",
      " [  0.0801633 ]]\n",
      "the iterations of  360000 the loss is 0.27120341019147565 theta= [[-10.12066568]\n",
      " [  0.08641588]\n",
      " [  0.08017398]]\n",
      "the iterations of  360100 the loss is 0.2711854749469014 theta= [[-10.12200489]\n",
      " [  0.08642644]\n",
      " [  0.08018464]]\n",
      "the iterations of  360200 the loss is 0.2711675478864411 theta= [[-10.12334379]\n",
      " [  0.08643699]\n",
      " [  0.08019531]]\n",
      "the iterations of  360300 the loss is 0.27114962900445005 theta= [[-10.12468239]\n",
      " [  0.08644754]\n",
      " [  0.08020597]]\n",
      "the iterations of  360400 the loss is 0.27113171829528937 theta= [[-10.12602068]\n",
      " [  0.08645809]\n",
      " [  0.08021663]]\n",
      "the iterations of  360500 the loss is 0.2711138157533248 theta= [[-10.12735867]\n",
      " [  0.08646864]\n",
      " [  0.08022729]]\n",
      "the iterations of  360600 the loss is 0.2710959213729275 theta= [[-10.12869635]\n",
      " [  0.08647918]\n",
      " [  0.08023795]]\n",
      "the iterations of  360700 the loss is 0.271078035148474 theta= [[-10.13003373]\n",
      " [  0.08648972]\n",
      " [  0.0802486 ]]\n",
      "the iterations of  360800 the loss is 0.271060157074346 theta= [[-10.1313708 ]\n",
      " [  0.08650026]\n",
      " [  0.08025926]]\n",
      "the iterations of  360900 the loss is 0.2710422871449305 theta= [[-10.13270756]\n",
      " [  0.0865108 ]\n",
      " [  0.08026991]]\n",
      "the iterations of  361000 the loss is 0.2710244253546195 theta= [[-10.13404403]\n",
      " [  0.08652133]\n",
      " [  0.08028055]]\n",
      "the iterations of  361100 the loss is 0.27100657169781056 theta= [[-10.13538019]\n",
      " [  0.08653187]\n",
      " [  0.0802912 ]]\n",
      "the iterations of  361200 the loss is 0.270988726168906 theta= [[-10.13671604]\n",
      " [  0.0865424 ]\n",
      " [  0.08030184]]\n",
      "the iterations of  361300 the loss is 0.2709708887623136 theta= [[-10.13805159]\n",
      " [  0.08655293]\n",
      " [  0.08031248]]\n",
      "the iterations of  361400 the loss is 0.2709530594724466 theta= [[-10.13938684]\n",
      " [  0.08656345]\n",
      " [  0.08032312]]\n",
      "the iterations of  361500 the loss is 0.2709352382937228 theta= [[-10.14072178]\n",
      " [  0.08657397]\n",
      " [  0.08033375]]\n",
      "the iterations of  361600 the loss is 0.2709174252205653 theta= [[-10.14205642]\n",
      " [  0.0865845 ]\n",
      " [  0.08034439]]\n",
      "the iterations of  361700 the loss is 0.2708996202474031 theta= [[-10.14339075]\n",
      " [  0.08659501]\n",
      " [  0.08035502]]\n",
      "the iterations of  361800 the loss is 0.27088182336866934 theta= [[-10.14472478]\n",
      " [  0.08660553]\n",
      " [  0.08036564]]\n",
      "the iterations of  361900 the loss is 0.2708640345788032 theta= [[-10.14605851]\n",
      " [  0.08661604]\n",
      " [  0.08037627]]\n",
      "the iterations of  362000 the loss is 0.2708462538722486 theta= [[-10.14739194]\n",
      " [  0.08662656]\n",
      " [  0.08038689]]\n",
      "the iterations of  362100 the loss is 0.27082848124345427 theta= [[-10.14872506]\n",
      " [  0.08663707]\n",
      " [  0.08039751]]\n",
      "the iterations of  362200 the loss is 0.2708107166868748 theta= [[-10.15005788]\n",
      " [  0.08664757]\n",
      " [  0.08040813]]\n",
      "the iterations of  362300 the loss is 0.27079296019696947 theta= [[-10.1513904 ]\n",
      " [  0.08665808]\n",
      " [  0.08041875]]\n",
      "the iterations of  362400 the loss is 0.2707752117682027 theta= [[-10.15272261]\n",
      " [  0.08666858]\n",
      " [  0.08042936]]\n",
      "the iterations of  362500 the loss is 0.2707574713950443 theta= [[-10.15405453]\n",
      " [  0.08667908]\n",
      " [  0.08043997]]\n",
      "the iterations of  362600 the loss is 0.27073973907196897 theta= [[-10.15538614]\n",
      " [  0.08668958]\n",
      " [  0.08045058]]\n",
      "the iterations of  362700 the loss is 0.2707220147934564 theta= [[-10.15671745]\n",
      " [  0.08670007]\n",
      " [  0.08046119]]\n",
      "the iterations of  362800 the loss is 0.270704298553992 theta= [[-10.15804845]\n",
      " [  0.08671057]\n",
      " [  0.08047179]]\n",
      "the iterations of  362900 the loss is 0.27068659034806547 theta= [[-10.15937916]\n",
      " [  0.08672106]\n",
      " [  0.0804824 ]]\n",
      "the iterations of  363000 the loss is 0.27066889017017215 theta= [[-10.16070956]\n",
      " [  0.08673154]\n",
      " [  0.08049299]]\n",
      "the iterations of  363100 the loss is 0.27065119801481224 theta= [[-10.16203966]\n",
      " [  0.08674203]\n",
      " [  0.08050359]]\n",
      "the iterations of  363200 the loss is 0.270633513876491 theta= [[-10.16336946]\n",
      " [  0.08675251]\n",
      " [  0.08051419]]\n",
      "the iterations of  363300 the loss is 0.2706158377497189 theta= [[-10.16469896]\n",
      " [  0.08676299]\n",
      " [  0.08052478]]\n",
      "the iterations of  363400 the loss is 0.2705981696290115 theta= [[-10.16602816]\n",
      " [  0.08677347]\n",
      " [  0.08053537]]\n",
      "the iterations of  363500 the loss is 0.27058050950888923 theta= [[-10.16735705]\n",
      " [  0.08678395]\n",
      " [  0.08054596]]\n",
      "the iterations of  363600 the loss is 0.2705628573838778 theta= [[-10.16868565]\n",
      " [  0.08679443]\n",
      " [  0.08055654]]\n",
      "the iterations of  363700 the loss is 0.2705452132485082 theta= [[-10.17001394]\n",
      " [  0.0868049 ]\n",
      " [  0.08056713]]\n",
      "the iterations of  363800 the loss is 0.2705275770973158 theta= [[-10.17134194]\n",
      " [  0.08681537]\n",
      " [  0.08057771]]\n",
      "the iterations of  363900 the loss is 0.27050994892484126 theta= [[-10.17266963]\n",
      " [  0.08682583]\n",
      " [  0.08058829]]\n",
      "the iterations of  364000 the loss is 0.2704923287256308 theta= [[-10.17399702]\n",
      " [  0.0868363 ]\n",
      " [  0.08059886]]\n",
      "the iterations of  364100 the loss is 0.270474716494235 theta= [[-10.17532412]\n",
      " [  0.08684676]\n",
      " [  0.08060944]]\n",
      "the iterations of  364200 the loss is 0.27045711222520963 theta= [[-10.17665091]\n",
      " [  0.08685722]\n",
      " [  0.08062001]]\n",
      "the iterations of  364300 the loss is 0.27043951591311566 theta= [[-10.1779774 ]\n",
      " [  0.08686768]\n",
      " [  0.08063058]]\n",
      "the iterations of  364400 the loss is 0.27042192755251926 theta= [[-10.1793036 ]\n",
      " [  0.08687814]\n",
      " [  0.08064114]]\n",
      "the iterations of  364500 the loss is 0.2704043471379908 theta= [[-10.18062949]\n",
      " [  0.08688859]\n",
      " [  0.08065171]]\n",
      "the iterations of  364600 the loss is 0.2703867746641064 theta= [[-10.18195509]\n",
      " [  0.08689904]\n",
      " [  0.08066227]]\n",
      "the iterations of  364700 the loss is 0.270369210125447 theta= [[-10.18328038]\n",
      " [  0.08690949]\n",
      " [  0.08067283]]\n",
      "the iterations of  364800 the loss is 0.2703516535165986 theta= [[-10.18460538]\n",
      " [  0.08691994]\n",
      " [  0.08068339]]\n",
      "the iterations of  364900 the loss is 0.27033410483215176 theta= [[-10.18593007]\n",
      " [  0.08693038]\n",
      " [  0.08069394]]\n",
      "the iterations of  365000 the loss is 0.27031656406670274 theta= [[-10.18725447]\n",
      " [  0.08694082]\n",
      " [  0.08070449]]\n",
      "the iterations of  365100 the loss is 0.2702990312148518 theta= [[-10.18857857]\n",
      " [  0.08695126]\n",
      " [  0.08071504]]\n",
      "the iterations of  365200 the loss is 0.27028150627120523 theta= [[-10.18990237]\n",
      " [  0.0869617 ]\n",
      " [  0.08072559]]\n",
      "the iterations of  365300 the loss is 0.27026398923037376 theta= [[-10.19122587]\n",
      " [  0.08697214]\n",
      " [  0.08073614]]\n",
      "the iterations of  365400 the loss is 0.27024648008697283 theta= [[-10.19254908]\n",
      " [  0.08698257]\n",
      " [  0.08074668]]\n",
      "the iterations of  365500 the loss is 0.2702289788356234 theta= [[-10.19387198]\n",
      " [  0.086993  ]\n",
      " [  0.08075722]]\n",
      "the iterations of  365600 the loss is 0.2702114854709507 theta= [[-10.19519459]\n",
      " [  0.08700343]\n",
      " [  0.08076776]]\n",
      "the iterations of  365700 the loss is 0.27019399998758553 theta= [[-10.1965169 ]\n",
      " [  0.08701385]\n",
      " [  0.0807783 ]]\n",
      "the iterations of  365800 the loss is 0.27017652238016315 theta= [[-10.19783891]\n",
      " [  0.08702428]\n",
      " [  0.08078883]]\n",
      "the iterations of  365900 the loss is 0.2701590526433244 theta= [[-10.19916063]\n",
      " [  0.0870347 ]\n",
      " [  0.08079936]]\n",
      "the iterations of  366000 the loss is 0.2701415907717142 theta= [[-10.20048204]\n",
      " [  0.08704512]\n",
      " [  0.08080989]]\n",
      "the iterations of  366100 the loss is 0.27012413675998276 theta= [[-10.20180316]\n",
      " [  0.08705553]\n",
      " [  0.08082042]]\n",
      "the iterations of  366200 the loss is 0.27010669060278547 theta= [[-10.20312398]\n",
      " [  0.08706595]\n",
      " [  0.08083094]]\n",
      "the iterations of  366300 the loss is 0.27008925229478215 theta= [[-10.20444451]\n",
      " [  0.08707636]\n",
      " [  0.08084147]]\n",
      "the iterations of  366400 the loss is 0.2700718218306379 theta= [[-10.20576473]\n",
      " [  0.08708677]\n",
      " [  0.08085199]]\n",
      "the iterations of  366500 the loss is 0.2700543992050224 theta= [[-10.20708466]\n",
      " [  0.08709718]\n",
      " [  0.0808625 ]]\n",
      "the iterations of  366600 the loss is 0.27003698441261076 theta= [[-10.2084043 ]\n",
      " [  0.08710758]\n",
      " [  0.08087302]]\n",
      "the iterations of  366700 the loss is 0.27001957744808214 theta= [[-10.20972364]\n",
      " [  0.08711799]\n",
      " [  0.08088353]]\n",
      "the iterations of  366800 the loss is 0.2700021783061213 theta= [[-10.21104268]\n",
      " [  0.08712839]\n",
      " [  0.08089404]]\n",
      "the iterations of  366900 the loss is 0.2699847869814175 theta= [[-10.21236142]\n",
      " [  0.08713878]\n",
      " [  0.08090455]]\n",
      "the iterations of  367000 the loss is 0.26996740346866505 theta= [[-10.21367987]\n",
      " [  0.08714918]\n",
      " [  0.08091506]]\n",
      "the iterations of  367100 the loss is 0.26995002776256277 theta= [[-10.21499802]\n",
      " [  0.08715957]\n",
      " [  0.08092556]]\n",
      "the iterations of  367200 the loss is 0.2699326598578151 theta= [[-10.21631588]\n",
      " [  0.08716997]\n",
      " [  0.08093606]]\n",
      "the iterations of  367300 the loss is 0.2699152997491303 theta= [[-10.21763344]\n",
      " [  0.08718036]\n",
      " [  0.08094656]]\n",
      "the iterations of  367400 the loss is 0.2698979474312221 theta= [[-10.2189507 ]\n",
      " [  0.08719074]\n",
      " [  0.08095706]]\n",
      "the iterations of  367500 the loss is 0.2698806028988093 theta= [[-10.22026767]\n",
      " [  0.08720113]\n",
      " [  0.08096755]]\n",
      "the iterations of  367600 the loss is 0.2698632661466149 theta= [[-10.22158434]\n",
      " [  0.08721151]\n",
      " [  0.08097804]]\n",
      "the iterations of  367700 the loss is 0.26984593716936695 theta= [[-10.22290072]\n",
      " [  0.08722189]\n",
      " [  0.08098853]]\n",
      "the iterations of  367800 the loss is 0.2698286159617987 theta= [[-10.2242168 ]\n",
      " [  0.08723227]\n",
      " [  0.08099902]]\n",
      "the iterations of  367900 the loss is 0.2698113025186479 theta= [[-10.22553259]\n",
      " [  0.08724264]\n",
      " [  0.08100951]]\n",
      "the iterations of  368000 the loss is 0.26979399683465666 theta= [[-10.22684809]\n",
      " [  0.08725302]\n",
      " [  0.08101999]]\n",
      "the iterations of  368100 the loss is 0.26977669890457284 theta= [[-10.22816328]\n",
      " [  0.08726339]\n",
      " [  0.08103047]]\n",
      "the iterations of  368200 the loss is 0.2697594087231485 theta= [[-10.22947819]\n",
      " [  0.08727376]\n",
      " [  0.08104095]]\n",
      "the iterations of  368300 the loss is 0.26974212628514055 theta= [[-10.2307928 ]\n",
      " [  0.08728412]\n",
      " [  0.08105142]]\n",
      "the iterations of  368400 the loss is 0.26972485158531073 theta= [[-10.23210711]\n",
      " [  0.08729449]\n",
      " [  0.0810619 ]]\n",
      "the iterations of  368500 the loss is 0.2697075846184254 theta= [[-10.23342113]\n",
      " [  0.08730485]\n",
      " [  0.08107237]]\n",
      "the iterations of  368600 the loss is 0.2696903253792562 theta= [[-10.23473486]\n",
      " [  0.08731521]\n",
      " [  0.08108284]]\n",
      "the iterations of  368700 the loss is 0.2696730738625791 theta= [[-10.23604829]\n",
      " [  0.08732557]\n",
      " [  0.08109331]]\n",
      "the iterations of  368800 the loss is 0.2696558300631749 theta= [[-10.23736143]\n",
      " [  0.08733592]\n",
      " [  0.08110377]]\n",
      "the iterations of  368900 the loss is 0.26963859397582896 theta= [[-10.23867428]\n",
      " [  0.08734627]\n",
      " [  0.08111423]]\n",
      "the iterations of  369000 the loss is 0.269621365595332 theta= [[-10.23998683]\n",
      " [  0.08735662]\n",
      " [  0.08112469]]\n",
      "the iterations of  369100 the loss is 0.26960414491647894 theta= [[-10.24129909]\n",
      " [  0.08736697]\n",
      " [  0.08113515]]\n",
      "the iterations of  369200 the loss is 0.26958693193406985 theta= [[-10.24261105]\n",
      " [  0.08737732]\n",
      " [  0.08114561]]\n",
      "the iterations of  369300 the loss is 0.2695697266429091 theta= [[-10.24392272]\n",
      " [  0.08738766]\n",
      " [  0.08115606]]\n",
      "the iterations of  369400 the loss is 0.2695525290378062 theta= [[-10.2452341 ]\n",
      " [  0.087398  ]\n",
      " [  0.08116651]]\n",
      "the iterations of  369500 the loss is 0.2695353391135751 theta= [[-10.24654519]\n",
      " [  0.08740834]\n",
      " [  0.08117696]]\n",
      "the iterations of  369600 the loss is 0.2695181568650343 theta= [[-10.24785598]\n",
      " [  0.08741868]\n",
      " [  0.0811874 ]]\n",
      "the iterations of  369700 the loss is 0.26950098228700786 theta= [[-10.24916648]\n",
      " [  0.08742902]\n",
      " [  0.08119785]]\n",
      "the iterations of  369800 the loss is 0.2694838153743235 theta= [[-10.25047669]\n",
      " [  0.08743935]\n",
      " [  0.08120829]]\n",
      "the iterations of  369900 the loss is 0.2694666561218143 theta= [[-10.25178661]\n",
      " [  0.08744968]\n",
      " [  0.08121873]]\n",
      "the iterations of  370000 the loss is 0.26944950452431793 theta= [[-10.25309623]\n",
      " [  0.08746001]\n",
      " [  0.08122917]]\n",
      "the iterations of  370100 the loss is 0.26943236057667663 theta= [[-10.25440556]\n",
      " [  0.08747033]\n",
      " [  0.0812396 ]]\n",
      "the iterations of  370200 the loss is 0.26941522427373754 theta= [[-10.2557146 ]\n",
      " [  0.08748066]\n",
      " [  0.08125003]]\n",
      "the iterations of  370300 the loss is 0.2693980956103521 theta= [[-10.25702335]\n",
      " [  0.08749098]\n",
      " [  0.08126046]]\n",
      "the iterations of  370400 the loss is 0.26938097458137666 theta= [[-10.2583318 ]\n",
      " [  0.0875013 ]\n",
      " [  0.08127089]]\n",
      "the iterations of  370500 the loss is 0.2693638611816727 theta= [[-10.25963997]\n",
      " [  0.08751161]\n",
      " [  0.08128132]]\n",
      "the iterations of  370600 the loss is 0.26934675540610525 theta= [[-10.26094784]\n",
      " [  0.08752193]\n",
      " [  0.08129174]]\n",
      "the iterations of  370700 the loss is 0.26932965724954505 theta= [[-10.26225542]\n",
      " [  0.08753224]\n",
      " [  0.08130216]]\n",
      "the iterations of  370800 the loss is 0.2693125667068673 theta= [[-10.26356271]\n",
      " [  0.08754255]\n",
      " [  0.08131258]]\n",
      "the iterations of  370900 the loss is 0.26929548377295126 theta= [[-10.26486971]\n",
      " [  0.08755286]\n",
      " [  0.081323  ]]\n",
      "the iterations of  371000 the loss is 0.2692784084426814 theta= [[-10.26617642]\n",
      " [  0.08756316]\n",
      " [  0.08133341]]\n",
      "the iterations of  371100 the loss is 0.2692613407109466 theta= [[-10.26748284]\n",
      " [  0.08757347]\n",
      " [  0.08134382]]\n",
      "the iterations of  371200 the loss is 0.26924428057264077 theta= [[-10.26878896]\n",
      " [  0.08758377]\n",
      " [  0.08135423]]\n",
      "the iterations of  371300 the loss is 0.2692272280226619 theta= [[-10.2700948 ]\n",
      " [  0.08759407]\n",
      " [  0.08136464]]\n",
      "the iterations of  371400 the loss is 0.269210183055913 theta= [[-10.27140035]\n",
      " [  0.08760436]\n",
      " [  0.08137504]]\n",
      "the iterations of  371500 the loss is 0.26919314566730124 theta= [[-10.2727056 ]\n",
      " [  0.08761466]\n",
      " [  0.08138545]]\n",
      "the iterations of  371600 the loss is 0.2691761158517391 theta= [[-10.27401057]\n",
      " [  0.08762495]\n",
      " [  0.08139585]]\n",
      "the iterations of  371700 the loss is 0.26915909360414303 theta= [[-10.27531524]\n",
      " [  0.08763524]\n",
      " [  0.08140624]]\n",
      "the iterations of  371800 the loss is 0.26914207891943426 theta= [[-10.27661963]\n",
      " [  0.08764553]\n",
      " [  0.08141664]]\n",
      "the iterations of  371900 the loss is 0.2691250717925389 theta= [[-10.27792373]\n",
      " [  0.08765581]\n",
      " [  0.08142703]]\n",
      "the iterations of  372000 the loss is 0.26910807221838745 theta= [[-10.27922753]\n",
      " [  0.0876661 ]\n",
      " [  0.08143743]]\n",
      "the iterations of  372100 the loss is 0.26909108019191497 theta= [[-10.28053105]\n",
      " [  0.08767638]\n",
      " [  0.08144782]]\n",
      "the iterations of  372200 the loss is 0.269074095708061 theta= [[-10.28183428]\n",
      " [  0.08768665]\n",
      " [  0.0814582 ]]\n",
      "the iterations of  372300 the loss is 0.26905711876176996 theta= [[-10.28313722]\n",
      " [  0.08769693]\n",
      " [  0.08146859]]\n",
      "the iterations of  372400 the loss is 0.2690401493479907 theta= [[-10.28443987]\n",
      " [  0.08770721]\n",
      " [  0.08147897]]\n",
      "the iterations of  372500 the loss is 0.26902318746167636 theta= [[-10.28574223]\n",
      " [  0.08771748]\n",
      " [  0.08148935]]\n",
      "the iterations of  372600 the loss is 0.26900623309778526 theta= [[-10.2870443 ]\n",
      " [  0.08772775]\n",
      " [  0.08149973]]\n",
      "the iterations of  372700 the loss is 0.26898928625127966 theta= [[-10.28834608]\n",
      " [  0.08773801]\n",
      " [  0.0815101 ]]\n",
      "the iterations of  372800 the loss is 0.2689723469171266 theta= [[-10.28964758]\n",
      " [  0.08774828]\n",
      " [  0.08152048]]\n",
      "the iterations of  372900 the loss is 0.2689554150902981 theta= [[-10.29094878]\n",
      " [  0.08775854]\n",
      " [  0.08153085]]\n",
      "the iterations of  373000 the loss is 0.26893849076576964 theta= [[-10.2922497 ]\n",
      " [  0.0877688 ]\n",
      " [  0.08154122]]\n",
      "the iterations of  373100 the loss is 0.2689215739385225 theta= [[-10.29355033]\n",
      " [  0.08777906]\n",
      " [  0.08155158]]\n",
      "the iterations of  373200 the loss is 0.26890466460354184 theta= [[-10.29485067]\n",
      " [  0.08778932]\n",
      " [  0.08156195]]\n",
      "the iterations of  373300 the loss is 0.26888776275581705 theta= [[-10.29615072]\n",
      " [  0.08779957]\n",
      " [  0.08157231]]\n",
      "the iterations of  373400 the loss is 0.26887086839034274 theta= [[-10.29745049]\n",
      " [  0.08780983]\n",
      " [  0.08158267]]\n",
      "the iterations of  373500 the loss is 0.26885398150211764 theta= [[-10.29874997]\n",
      " [  0.08782007]\n",
      " [  0.08159303]]\n",
      "the iterations of  373600 the loss is 0.2688371020861451 theta= [[-10.30004916]\n",
      " [  0.08783032]\n",
      " [  0.08160338]]\n",
      "the iterations of  373700 the loss is 0.26882023013743284 theta= [[-10.30134806]\n",
      " [  0.08784057]\n",
      " [  0.08161373]]\n",
      "the iterations of  373800 the loss is 0.26880336565099316 theta= [[-10.30264668]\n",
      " [  0.08785081]\n",
      " [  0.08162409]]\n",
      "the iterations of  373900 the loss is 0.2687865086218428 theta= [[-10.303945  ]\n",
      " [  0.08786105]\n",
      " [  0.08163443]]\n",
      "the iterations of  374000 the loss is 0.2687696590450034 theta= [[-10.30524305]\n",
      " [  0.08787129]\n",
      " [  0.08164478]]\n",
      "the iterations of  374100 the loss is 0.26875281691550046 theta= [[-10.3065408 ]\n",
      " [  0.08788153]\n",
      " [  0.08165512]]\n",
      "the iterations of  374200 the loss is 0.26873598222836437 theta= [[-10.30783827]\n",
      " [  0.08789176]\n",
      " [  0.08166547]]\n",
      "the iterations of  374300 the loss is 0.26871915497862964 theta= [[-10.30913545]\n",
      " [  0.08790199]\n",
      " [  0.08167581]]\n",
      "the iterations of  374400 the loss is 0.26870233516133596 theta= [[-10.31043234]\n",
      " [  0.08791222]\n",
      " [  0.08168614]]\n",
      "the iterations of  374500 the loss is 0.2686855227715267 theta= [[-10.31172895]\n",
      " [  0.08792245]\n",
      " [  0.08169648]]\n",
      "the iterations of  374600 the loss is 0.2686687178042498 theta= [[-10.31302527]\n",
      " [  0.08793268]\n",
      " [  0.08170681]]\n",
      "the iterations of  374700 the loss is 0.26865192025455853 theta= [[-10.31432131]\n",
      " [  0.0879429 ]\n",
      " [  0.08171714]]\n",
      "the iterations of  374800 the loss is 0.2686351301175092 theta= [[-10.31561706]\n",
      " [  0.08795312]\n",
      " [  0.08172747]]\n",
      "the iterations of  374900 the loss is 0.2686183473881639 theta= [[-10.31691252]\n",
      " [  0.08796334]\n",
      " [  0.0817378 ]]\n",
      "the iterations of  375000 the loss is 0.2686015720615882 theta= [[-10.3182077 ]\n",
      " [  0.08797355]\n",
      " [  0.08174812]]\n",
      "the iterations of  375100 the loss is 0.2685848041328525 theta= [[-10.3195026 ]\n",
      " [  0.08798377]\n",
      " [  0.08175844]]\n",
      "the iterations of  375200 the loss is 0.26856804359703185 theta= [[-10.3207972 ]\n",
      " [  0.08799398]\n",
      " [  0.08176876]]\n",
      "the iterations of  375300 the loss is 0.2685512904492053 theta= [[-10.32209152]\n",
      " [  0.08800419]\n",
      " [  0.08177908]]\n",
      "the iterations of  375400 the loss is 0.2685345446844565 theta= [[-10.32338556]\n",
      " [  0.0880144 ]\n",
      " [  0.08178939]]\n",
      "the iterations of  375500 the loss is 0.26851780629787353 theta= [[-10.32467931]\n",
      " [  0.0880246 ]\n",
      " [  0.08179971]]\n",
      "the iterations of  375600 the loss is 0.26850107528454903 theta= [[-10.32597278]\n",
      " [  0.08803481]\n",
      " [  0.08181002]]\n",
      "the iterations of  375700 the loss is 0.2684843516395798 theta= [[-10.32726596]\n",
      " [  0.08804501]\n",
      " [  0.08182033]]\n",
      "the iterations of  375800 the loss is 0.268467635358067 theta= [[-10.32855886]\n",
      " [  0.08805521]\n",
      " [  0.08183063]]\n",
      "the iterations of  375900 the loss is 0.26845092643511614 theta= [[-10.32985147]\n",
      " [  0.0880654 ]\n",
      " [  0.08184093]]\n",
      "the iterations of  376000 the loss is 0.26843422486583784 theta= [[-10.3311438 ]\n",
      " [  0.0880756 ]\n",
      " [  0.08185124]]\n",
      "the iterations of  376100 the loss is 0.2684175306453463 theta= [[-10.33243584]\n",
      " [  0.08808579]\n",
      " [  0.08186154]]\n",
      "the iterations of  376200 the loss is 0.26840084376876033 theta= [[-10.3337276 ]\n",
      " [  0.08809598]\n",
      " [  0.08187183]]\n",
      "the iterations of  376300 the loss is 0.268384164231203 theta= [[-10.33501908]\n",
      " [  0.08810617]\n",
      " [  0.08188213]]\n",
      "the iterations of  376400 the loss is 0.26836749202780225 theta= [[-10.33631027]\n",
      " [  0.08811636]\n",
      " [  0.08189242]]\n",
      "the iterations of  376500 the loss is 0.26835082715368985 theta= [[-10.33760117]\n",
      " [  0.08812654]\n",
      " [  0.08190271]]\n",
      "the iterations of  376600 the loss is 0.26833416960400214 theta= [[-10.3388918 ]\n",
      " [  0.08813672]\n",
      " [  0.081913  ]]\n",
      "the iterations of  376700 the loss is 0.26831751937387976 theta= [[-10.34018214]\n",
      " [  0.0881469 ]\n",
      " [  0.08192329]]\n",
      "the iterations of  376800 the loss is 0.2683008764584676 theta= [[-10.34147219]\n",
      " [  0.08815708]\n",
      " [  0.08193357]]\n",
      "the iterations of  376900 the loss is 0.26828424085291536 theta= [[-10.34276197]\n",
      " [  0.08816725]\n",
      " [  0.08194385]]\n",
      "the iterations of  377000 the loss is 0.2682676125523765 theta= [[-10.34405146]\n",
      " [  0.08817742]\n",
      " [  0.08195413]]\n",
      "the iterations of  377100 the loss is 0.26825099155200927 theta= [[-10.34534066]\n",
      " [  0.08818759]\n",
      " [  0.08196441]]\n",
      "the iterations of  377200 the loss is 0.26823437784697596 theta= [[-10.34662959]\n",
      " [  0.08819776]\n",
      " [  0.08197468]]\n",
      "the iterations of  377300 the loss is 0.26821777143244324 theta= [[-10.34791823]\n",
      " [  0.08820793]\n",
      " [  0.08198495]]\n",
      "the iterations of  377400 the loss is 0.268201172303582 theta= [[-10.34920659]\n",
      " [  0.08821809]\n",
      " [  0.08199523]]\n",
      "the iterations of  377500 the loss is 0.2681845804555679 theta= [[-10.35049466]\n",
      " [  0.08822825]\n",
      " [  0.08200549]]\n",
      "the iterations of  377600 the loss is 0.26816799588358026 theta= [[-10.35178246]\n",
      " [  0.08823841]\n",
      " [  0.08201576]]\n",
      "the iterations of  377700 the loss is 0.26815141858280334 theta= [[-10.35306997]\n",
      " [  0.08824857]\n",
      " [  0.08202602]]\n",
      "the iterations of  377800 the loss is 0.2681348485484252 theta= [[-10.3543572 ]\n",
      " [  0.08825873]\n",
      " [  0.08203629]]\n",
      "the iterations of  377900 the loss is 0.26811828577563873 theta= [[-10.35564414]\n",
      " [  0.08826888]\n",
      " [  0.08204654]]\n",
      "the iterations of  378000 the loss is 0.2681017302596405 theta= [[-10.35693081]\n",
      " [  0.08827903]\n",
      " [  0.0820568 ]]\n",
      "the iterations of  378100 the loss is 0.26808518199563186 theta= [[-10.35821719]\n",
      " [  0.08828918]\n",
      " [  0.08206706]]\n",
      "the iterations of  378200 the loss is 0.26806864097881833 theta= [[-10.35950329]\n",
      " [  0.08829932]\n",
      " [  0.08207731]]\n",
      "the iterations of  378300 the loss is 0.2680521072044093 theta= [[-10.36078911]\n",
      " [  0.08830947]\n",
      " [  0.08208756]]\n",
      "the iterations of  378400 the loss is 0.26803558066761907 theta= [[-10.36207465]\n",
      " [  0.08831961]\n",
      " [  0.08209781]]\n",
      "the iterations of  378500 the loss is 0.2680190613636656 theta= [[-10.36335991]\n",
      " [  0.08832975]\n",
      " [  0.08210805]]\n",
      "the iterations of  378600 the loss is 0.2680025492877715 theta= [[-10.36464488]\n",
      " [  0.08833989]\n",
      " [  0.0821183 ]]\n",
      "the iterations of  378700 the loss is 0.2679860444351639 theta= [[-10.36592958]\n",
      " [  0.08835002]\n",
      " [  0.08212854]]\n",
      "the iterations of  378800 the loss is 0.2679695468010733 theta= [[-10.36721399]\n",
      " [  0.08836016]\n",
      " [  0.08213878]]\n",
      "the iterations of  378900 the loss is 0.2679530563807354 theta= [[-10.36849812]\n",
      " [  0.08837029]\n",
      " [  0.08214902]]\n",
      "the iterations of  379000 the loss is 0.2679365731693895 theta= [[-10.36978197]\n",
      " [  0.08838042]\n",
      " [  0.08215925]]\n",
      "the iterations of  379100 the loss is 0.2679200971622797 theta= [[-10.37106554]\n",
      " [  0.08839054]\n",
      " [  0.08216949]]\n",
      "the iterations of  379200 the loss is 0.26790362835465364 theta= [[-10.37234884]\n",
      " [  0.08840067]\n",
      " [  0.08217972]]\n",
      "the iterations of  379300 the loss is 0.2678871667417637 theta= [[-10.37363185]\n",
      " [  0.08841079]\n",
      " [  0.08218995]]\n",
      "the iterations of  379400 the loss is 0.26787071231886644 theta= [[-10.37491458]\n",
      " [  0.08842091]\n",
      " [  0.08220017]]\n",
      "the iterations of  379500 the loss is 0.2678542650812224 theta= [[-10.37619703]\n",
      " [  0.08843103]\n",
      " [  0.0822104 ]]\n",
      "the iterations of  379600 the loss is 0.26783782502409664 theta= [[-10.3774792 ]\n",
      " [  0.08844115]\n",
      " [  0.08222062]]\n",
      "the iterations of  379700 the loss is 0.26782139214275813 theta= [[-10.37876109]\n",
      " [  0.08845126]\n",
      " [  0.08223084]]\n",
      "the iterations of  379800 the loss is 0.26780496643248025 theta= [[-10.3800427 ]\n",
      " [  0.08846137]\n",
      " [  0.08224106]]\n",
      "the iterations of  379900 the loss is 0.26778854788854073 theta= [[-10.38132403]\n",
      " [  0.08847148]\n",
      " [  0.08225127]]\n",
      "the iterations of  380000 the loss is 0.26777213650622106 theta= [[-10.38260508]\n",
      " [  0.08848159]\n",
      " [  0.08226148]]\n",
      "the iterations of  380100 the loss is 0.2677557322808073 theta= [[-10.38388585]\n",
      " [  0.08849169]\n",
      " [  0.0822717 ]]\n",
      "the iterations of  380200 the loss is 0.26773933520758947 theta= [[-10.38516634]\n",
      " [  0.0885018 ]\n",
      " [  0.0822819 ]]\n",
      "the iterations of  380300 the loss is 0.26772294528186175 theta= [[-10.38644656]\n",
      " [  0.0885119 ]\n",
      " [  0.08229211]]\n",
      "the iterations of  380400 the loss is 0.26770656249892283 theta= [[-10.38772649]\n",
      " [  0.088522  ]\n",
      " [  0.08230232]]\n",
      "the iterations of  380500 the loss is 0.2676901868540754 theta= [[-10.38900615]\n",
      " [  0.08853209]\n",
      " [  0.08231252]]\n",
      "the iterations of  380600 the loss is 0.2676738183426261 theta= [[-10.39028553]\n",
      " [  0.08854219]\n",
      " [  0.08232272]]\n",
      "the iterations of  380700 the loss is 0.2676574569598863 theta= [[-10.39156462]\n",
      " [  0.08855228]\n",
      " [  0.08233292]]\n",
      "the iterations of  380800 the loss is 0.26764110270117053 theta= [[-10.39284344]\n",
      " [  0.08856237]\n",
      " [  0.08234311]]\n",
      "the iterations of  380900 the loss is 0.2676247555617986 theta= [[-10.39412199]\n",
      " [  0.08857246]\n",
      " [  0.08235331]]\n",
      "the iterations of  381000 the loss is 0.26760841553709364 theta= [[-10.39540025]\n",
      " [  0.08858255]\n",
      " [  0.0823635 ]]\n",
      "the iterations of  381100 the loss is 0.26759208262238354 theta= [[-10.39667823]\n",
      " [  0.08859263]\n",
      " [  0.08237369]]\n",
      "the iterations of  381200 the loss is 0.26757575681299983 theta= [[-10.39795594]\n",
      " [  0.08860271]\n",
      " [  0.08238387]]\n",
      "the iterations of  381300 the loss is 0.26755943810427846 theta= [[-10.39923337]\n",
      " [  0.08861279]\n",
      " [  0.08239406]]\n",
      "the iterations of  381400 the loss is 0.26754312649155965 theta= [[-10.40051052]\n",
      " [  0.08862287]\n",
      " [  0.08240424]]\n",
      "the iterations of  381500 the loss is 0.26752682197018735 theta= [[-10.4017874 ]\n",
      " [  0.08863294]\n",
      " [  0.08241442]]\n",
      "the iterations of  381600 the loss is 0.26751052453550966 theta= [[-10.40306399]\n",
      " [  0.08864302]\n",
      " [  0.0824246 ]]\n",
      "the iterations of  381700 the loss is 0.2674942341828792 theta= [[-10.40434031]\n",
      " [  0.08865309]\n",
      " [  0.08243478]]\n",
      "the iterations of  381800 the loss is 0.2674779509076525 theta= [[-10.40561635]\n",
      " [  0.08866316]\n",
      " [  0.08244495]]\n",
      "the iterations of  381900 the loss is 0.26746167470519 theta= [[-10.40689212]\n",
      " [  0.08867322]\n",
      " [  0.08245512]]\n",
      "the iterations of  382000 the loss is 0.26744540557085666 theta= [[-10.4081676 ]\n",
      " [  0.08868329]\n",
      " [  0.08246529]]\n",
      "the iterations of  382100 the loss is 0.267429143500021 theta= [[-10.40944281]\n",
      " [  0.08869335]\n",
      " [  0.08247546]]\n",
      "the iterations of  382200 the loss is 0.2674128884880563 theta= [[-10.41071775]\n",
      " [  0.08870341]\n",
      " [  0.08248562]]\n",
      "the iterations of  382300 the loss is 0.2673966405303392 theta= [[-10.4119924 ]\n",
      " [  0.08871347]\n",
      " [  0.08249579]]\n",
      "the iterations of  382400 the loss is 0.2673803996222511 theta= [[-10.41326679]\n",
      " [  0.08872352]\n",
      " [  0.08250595]]\n",
      "the iterations of  382500 the loss is 0.267364165759177 theta= [[-10.41454089]\n",
      " [  0.08873358]\n",
      " [  0.08251611]]\n",
      "the iterations of  382600 the loss is 0.2673479389365061 theta= [[-10.41581472]\n",
      " [  0.08874363]\n",
      " [  0.08252626]]\n",
      "the iterations of  382700 the loss is 0.267331719149632 theta= [[-10.41708827]\n",
      " [  0.08875368]\n",
      " [  0.08253642]]\n",
      "the iterations of  382800 the loss is 0.267315506393952 theta= [[-10.41836154]\n",
      " [  0.08876372]\n",
      " [  0.08254657]]\n",
      "the iterations of  382900 the loss is 0.26729930066486757 theta= [[-10.41963454]\n",
      " [  0.08877377]\n",
      " [  0.08255672]]\n",
      "the iterations of  383000 the loss is 0.26728310195778415 theta= [[-10.42090727]\n",
      " [  0.08878381]\n",
      " [  0.08256687]]\n",
      "the iterations of  383100 the loss is 0.2672669102681111 theta= [[-10.42217971]\n",
      " [  0.08879385]\n",
      " [  0.08257701]]\n",
      "the iterations of  383200 the loss is 0.26725072559126256 theta= [[-10.42345189]\n",
      " [  0.08880389]\n",
      " [  0.08258716]]\n",
      "the iterations of  383300 the loss is 0.267234547922656 theta= [[-10.42472378]\n",
      " [  0.08881393]\n",
      " [  0.0825973 ]]\n",
      "the iterations of  383400 the loss is 0.267218377257713 theta= [[-10.4259954 ]\n",
      " [  0.08882396]\n",
      " [  0.08260744]]\n",
      "the iterations of  383500 the loss is 0.2672022135918597 theta= [[-10.42726675]\n",
      " [  0.088834  ]\n",
      " [  0.08261758]]\n",
      "the iterations of  383600 the loss is 0.26718605692052555 theta= [[-10.42853782]\n",
      " [  0.08884403]\n",
      " [  0.08262771]]\n",
      "the iterations of  383700 the loss is 0.2671699072391444 theta= [[-10.42980862]\n",
      " [  0.08885405]\n",
      " [  0.08263784]]\n",
      "the iterations of  383800 the loss is 0.2671537645431541 theta= [[-10.43107914]\n",
      " [  0.08886408]\n",
      " [  0.08264798]]\n",
      "the iterations of  383900 the loss is 0.2671376288279965 theta= [[-10.43234939]\n",
      " [  0.0888741 ]\n",
      " [  0.0826581 ]]\n",
      "the iterations of  384000 the loss is 0.26712150008911767 theta= [[-10.43361936]\n",
      " [  0.08888413]\n",
      " [  0.08266823]]\n",
      "the iterations of  384100 the loss is 0.26710537832196746 theta= [[-10.43488906]\n",
      " [  0.08889415]\n",
      " [  0.08267836]]\n",
      "the iterations of  384200 the loss is 0.2670892635219996 theta= [[-10.43615848]\n",
      " [  0.08890416]\n",
      " [  0.08268848]]\n",
      "the iterations of  384300 the loss is 0.2670731556846721 theta= [[-10.43742763]\n",
      " [  0.08891418]\n",
      " [  0.0826986 ]]\n",
      "the iterations of  384400 the loss is 0.26705705480544695 theta= [[-10.4386965 ]\n",
      " [  0.08892419]\n",
      " [  0.08270872]]\n",
      "the iterations of  384500 the loss is 0.2670409608797899 theta= [[-10.4399651 ]\n",
      " [  0.0889342 ]\n",
      " [  0.08271883]]\n",
      "the iterations of  384600 the loss is 0.267024873903171 theta= [[-10.44123343]\n",
      " [  0.08894421]\n",
      " [  0.08272894]]\n",
      "the iterations of  384700 the loss is 0.2670087938710641 theta= [[-10.44250148]\n",
      " [  0.08895422]\n",
      " [  0.08273906]]\n",
      "the iterations of  384800 the loss is 0.2669927207789468 theta= [[-10.44376926]\n",
      " [  0.08896422]\n",
      " [  0.08274917]]\n",
      "the iterations of  384900 the loss is 0.2669766546223012 theta= [[-10.44503677]\n",
      " [  0.08897423]\n",
      " [  0.08275927]]\n",
      "the iterations of  385000 the loss is 0.2669605953966129 theta= [[-10.446304  ]\n",
      " [  0.08898423]\n",
      " [  0.08276938]]\n",
      "the iterations of  385100 the loss is 0.266944543097372 theta= [[-10.44757096]\n",
      " [  0.08899423]\n",
      " [  0.08277948]]\n",
      "the iterations of  385200 the loss is 0.2669284977200717 theta= [[-10.44883764]\n",
      " [  0.08900422]\n",
      " [  0.08278958]]\n",
      "the iterations of  385300 the loss is 0.2669124592602101 theta= [[-10.45010406]\n",
      " [  0.08901422]\n",
      " [  0.08279968]]\n",
      "the iterations of  385400 the loss is 0.26689642771328853 theta= [[-10.4513702 ]\n",
      " [  0.08902421]\n",
      " [  0.08280978]]\n",
      "the iterations of  385500 the loss is 0.26688040307481287 theta= [[-10.45263606]\n",
      " [  0.0890342 ]\n",
      " [  0.08281987]]\n",
      "the iterations of  385600 the loss is 0.2668643853402926 theta= [[-10.45390166]\n",
      " [  0.08904419]\n",
      " [  0.08282996]]\n",
      "the iterations of  385700 the loss is 0.26684837450524124 theta= [[-10.45516698]\n",
      " [  0.08905417]\n",
      " [  0.08284005]]\n",
      "the iterations of  385800 the loss is 0.266832370565176 theta= [[-10.45643203]\n",
      " [  0.08906416]\n",
      " [  0.08285014]]\n",
      "the iterations of  385900 the loss is 0.26681637351561827 theta= [[-10.4576968 ]\n",
      " [  0.08907414]\n",
      " [  0.08286023]]\n",
      "the iterations of  386000 the loss is 0.26680038335209355 theta= [[-10.45896131]\n",
      " [  0.08908412]\n",
      " [  0.08287031]]\n",
      "the iterations of  386100 the loss is 0.26678440007013077 theta= [[-10.46022554]\n",
      " [  0.0890941 ]\n",
      " [  0.08288039]]\n",
      "the iterations of  386200 the loss is 0.2667684236652632 theta= [[-10.4614895 ]\n",
      " [  0.08910407]\n",
      " [  0.08289047]]\n",
      "the iterations of  386300 the loss is 0.26675245413302756 theta= [[-10.46275319]\n",
      " [  0.08911404]\n",
      " [  0.08290055]]\n",
      "the iterations of  386400 the loss is 0.2667364914689653 theta= [[-10.4640166 ]\n",
      " [  0.08912402]\n",
      " [  0.08291062]]\n",
      "the iterations of  386500 the loss is 0.2667205356686208 theta= [[-10.46527975]\n",
      " [  0.08913398]\n",
      " [  0.0829207 ]]\n",
      "the iterations of  386600 the loss is 0.2667045867275431 theta= [[-10.46654262]\n",
      " [  0.08914395]\n",
      " [  0.08293077]]\n",
      "the iterations of  386700 the loss is 0.2666886446412846 theta= [[-10.46780522]\n",
      " [  0.08915392]\n",
      " [  0.08294084]]\n",
      "the iterations of  386800 the loss is 0.266672709405402 theta= [[-10.46906755]\n",
      " [  0.08916388]\n",
      " [  0.0829509 ]]\n",
      "the iterations of  386900 the loss is 0.26665678101545587 theta= [[-10.47032961]\n",
      " [  0.08917384]\n",
      " [  0.08296097]]\n",
      "the iterations of  387000 the loss is 0.2666408594670104 theta= [[-10.4715914 ]\n",
      " [  0.0891838 ]\n",
      " [  0.08297103]]\n",
      "the iterations of  387100 the loss is 0.2666249447556336 theta= [[-10.47285292]\n",
      " [  0.08919375]\n",
      " [  0.08298109]]\n",
      "the iterations of  387200 the loss is 0.26660903687689785 theta= [[-10.47411417]\n",
      " [  0.08920371]\n",
      " [  0.08299115]]\n",
      "the iterations of  387300 the loss is 0.2665931358263787 theta= [[-10.47537514]\n",
      " [  0.08921366]\n",
      " [  0.0830012 ]]\n",
      "the iterations of  387400 the loss is 0.2665772415996563 theta= [[-10.47663585]\n",
      " [  0.08922361]\n",
      " [  0.08301126]]\n",
      "the iterations of  387500 the loss is 0.2665613541923142 theta= [[-10.47789628]\n",
      " [  0.08923356]\n",
      " [  0.08302131]]\n",
      "the iterations of  387600 the loss is 0.26654547359993985 theta= [[-10.47915645]\n",
      " [  0.08924351]\n",
      " [  0.08303136]]\n",
      "the iterations of  387700 the loss is 0.26652959981812485 theta= [[-10.48041634]\n",
      " [  0.08925345]\n",
      " [  0.08304141]]\n",
      "the iterations of  387800 the loss is 0.2665137328424644 theta= [[-10.48167596]\n",
      " [  0.08926339]\n",
      " [  0.08305145]]\n",
      "the iterations of  387900 the loss is 0.2664978726685574 theta= [[-10.48293532]\n",
      " [  0.08927333]\n",
      " [  0.0830615 ]]\n",
      "the iterations of  388000 the loss is 0.2664820192920069 theta= [[-10.4841944 ]\n",
      " [  0.08928327]\n",
      " [  0.08307154]]\n",
      "the iterations of  388100 the loss is 0.26646617270841955 theta= [[-10.48545322]\n",
      " [  0.0892932 ]\n",
      " [  0.08308158]]\n",
      "the iterations of  388200 the loss is 0.2664503329134059 theta= [[-10.48671176]\n",
      " [  0.08930314]\n",
      " [  0.08309161]]\n",
      "the iterations of  388300 the loss is 0.26643449990258067 theta= [[-10.48797004]\n",
      " [  0.08931307]\n",
      " [  0.08310165]]\n",
      "the iterations of  388400 the loss is 0.2664186736715618 theta= [[-10.48922804]\n",
      " [  0.089323  ]\n",
      " [  0.08311168]]\n",
      "the iterations of  388500 the loss is 0.2664028542159716 theta= [[-10.49048578]\n",
      " [  0.08933293]\n",
      " [  0.08312171]]\n",
      "the iterations of  388600 the loss is 0.26638704153143583 theta= [[-10.49174325]\n",
      " [  0.08934285]\n",
      " [  0.08313174]]\n",
      "the iterations of  388700 the loss is 0.2663712356135843 theta= [[-10.49300044]\n",
      " [  0.08935277]\n",
      " [  0.08314177]]\n",
      "the iterations of  388800 the loss is 0.2663554364580504 theta= [[-10.49425737]\n",
      " [  0.08936269]\n",
      " [  0.08315179]]\n",
      "the iterations of  388900 the loss is 0.2663396440604717 theta= [[-10.49551403]\n",
      " [  0.08937261]\n",
      " [  0.08316181]]\n",
      "the iterations of  389000 the loss is 0.2663238584164889 theta= [[-10.49677042]\n",
      " [  0.08938253]\n",
      " [  0.08317183]]\n",
      "the iterations of  389100 the loss is 0.2663080795217472 theta= [[-10.49802655]\n",
      " [  0.08939244]\n",
      " [  0.08318185]]\n",
      "the iterations of  389200 the loss is 0.26629230737189535 theta= [[-10.4992824 ]\n",
      " [  0.08940236]\n",
      " [  0.08319187]]\n",
      "the iterations of  389300 the loss is 0.2662765419625856 theta= [[-10.50053799]\n",
      " [  0.08941227]\n",
      " [  0.08320188]]\n",
      "the iterations of  389400 the loss is 0.26626078328947456 theta= [[-10.5017933 ]\n",
      " [  0.08942218]\n",
      " [  0.08321189]]\n",
      "the iterations of  389500 the loss is 0.266245031348222 theta= [[-10.50304835]\n",
      " [  0.08943208]\n",
      " [  0.0832219 ]]\n",
      "the iterations of  389600 the loss is 0.2662292861344918 theta= [[-10.50430314]\n",
      " [  0.08944199]\n",
      " [  0.08323191]]\n",
      "the iterations of  389700 the loss is 0.26621354764395166 theta= [[-10.50555765]\n",
      " [  0.08945189]\n",
      " [  0.08324191]]\n",
      "the iterations of  389800 the loss is 0.2661978158722729 theta= [[-10.50681189]\n",
      " [  0.08946179]\n",
      " [  0.08325192]]\n",
      "the iterations of  389900 the loss is 0.2661820908151308 theta= [[-10.50806587]\n",
      " [  0.08947169]\n",
      " [  0.08326192]]\n",
      "the iterations of  390000 the loss is 0.2661663724682039 theta= [[-10.50931958]\n",
      " [  0.08948158]\n",
      " [  0.08327192]]\n",
      "the iterations of  390100 the loss is 0.26615066082717503 theta= [[-10.51057303]\n",
      " [  0.08949148]\n",
      " [  0.08328191]]\n",
      "the iterations of  390200 the loss is 0.2661349558877307 theta= [[-10.5118262 ]\n",
      " [  0.08950137]\n",
      " [  0.08329191]]\n",
      "the iterations of  390300 the loss is 0.2661192576455609 theta= [[-10.51307911]\n",
      " [  0.08951126]\n",
      " [  0.0833019 ]]\n",
      "the iterations of  390400 the loss is 0.2661035660963596 theta= [[-10.51433175]\n",
      " [  0.08952115]\n",
      " [  0.08331189]]\n",
      "the iterations of  390500 the loss is 0.26608788123582444 theta= [[-10.51558412]\n",
      " [  0.08953103]\n",
      " [  0.08332188]]\n",
      "the iterations of  390600 the loss is 0.26607220305965684 theta= [[-10.51683623]\n",
      " [  0.08954092]\n",
      " [  0.08333187]]\n",
      "the iterations of  390700 the loss is 0.26605653156356185 theta= [[-10.51808807]\n",
      " [  0.0895508 ]\n",
      " [  0.08334185]]\n",
      "the iterations of  390800 the loss is 0.26604086674324817 theta= [[-10.51933965]\n",
      " [  0.08956068]\n",
      " [  0.08335183]]\n",
      "the iterations of  390900 the loss is 0.26602520859442846 theta= [[-10.52059095]\n",
      " [  0.08957056]\n",
      " [  0.08336182]]\n",
      "the iterations of  391000 the loss is 0.26600955711281893 theta= [[-10.52184199]\n",
      " [  0.08958043]\n",
      " [  0.08337179]]\n",
      "the iterations of  391100 the loss is 0.2659939122941396 theta= [[-10.52309277]\n",
      " [  0.0895903 ]\n",
      " [  0.08338177]]\n",
      "the iterations of  391200 the loss is 0.2659782741341142 theta= [[-10.52434328]\n",
      " [  0.08960018]\n",
      " [  0.08339174]]\n",
      "the iterations of  391300 the loss is 0.26596264262847 theta= [[-10.52559352]\n",
      " [  0.08961004]\n",
      " [  0.08340172]]\n",
      "the iterations of  391400 the loss is 0.2659470177729382 theta= [[-10.5268435 ]\n",
      " [  0.08961991]\n",
      " [  0.08341169]]\n",
      "the iterations of  391500 the loss is 0.2659313995632535 theta= [[-10.52809321]\n",
      " [  0.08962978]\n",
      " [  0.08342165]]\n",
      "the iterations of  391600 the loss is 0.26591578799515453 theta= [[-10.52934265]\n",
      " [  0.08963964]\n",
      " [  0.08343162]]\n",
      "the iterations of  391700 the loss is 0.26590018306438323 theta= [[-10.53059183]\n",
      " [  0.0896495 ]\n",
      " [  0.08344158]]\n",
      "the iterations of  391800 the loss is 0.2658845847666856 theta= [[-10.53184074]\n",
      " [  0.08965936]\n",
      " [  0.08345154]]\n",
      "the iterations of  391900 the loss is 0.26586899309781126 theta= [[-10.53308939]\n",
      " [  0.08966922]\n",
      " [  0.0834615 ]]\n",
      "the iterations of  392000 the loss is 0.2658534080535132 theta= [[-10.53433777]\n",
      " [  0.08967907]\n",
      " [  0.08347146]]\n",
      "the iterations of  392100 the loss is 0.26583782962954844 theta= [[-10.53558589]\n",
      " [  0.08968892]\n",
      " [  0.08348142]]\n",
      "the iterations of  392200 the loss is 0.26582225782167757 theta= [[-10.53683374]\n",
      " [  0.08969878]\n",
      " [  0.08349137]]\n",
      "the iterations of  392300 the loss is 0.26580669262566475 theta= [[-10.53808133]\n",
      " [  0.08970862]\n",
      " [  0.08350132]]\n",
      "the iterations of  392400 the loss is 0.26579113403727805 theta= [[-10.53932865]\n",
      " [  0.08971847]\n",
      " [  0.08351127]]\n",
      "the iterations of  392500 the loss is 0.26577558205228885 theta= [[-10.54057571]\n",
      " [  0.08972832]\n",
      " [  0.08352122]]\n",
      "the iterations of  392600 the loss is 0.2657600366664723 theta= [[-10.54182251]\n",
      " [  0.08973816]\n",
      " [  0.08353116]]\n",
      "the iterations of  392700 the loss is 0.26574449787560744 theta= [[-10.54306904]\n",
      " [  0.089748  ]\n",
      " [  0.0835411 ]]\n",
      "the iterations of  392800 the loss is 0.26572896567547666 theta= [[-10.5443153 ]\n",
      " [  0.08975784]\n",
      " [  0.08355104]]\n",
      "the iterations of  392900 the loss is 0.2657134400618661 theta= [[-10.5455613 ]\n",
      " [  0.08976767]\n",
      " [  0.08356098]]\n",
      "the iterations of  393000 the loss is 0.26569792103056566 theta= [[-10.54680704]\n",
      " [  0.08977751]\n",
      " [  0.08357092]]\n",
      "the iterations of  393100 the loss is 0.2656824085773685 theta= [[-10.54805251]\n",
      " [  0.08978734]\n",
      " [  0.08358085]]\n",
      "the iterations of  393200 the loss is 0.265666902698072 theta= [[-10.54929772]\n",
      " [  0.08979717]\n",
      " [  0.08359079]]\n",
      "the iterations of  393300 the loss is 0.26565140338847654 theta= [[-10.55054266]\n",
      " [  0.089807  ]\n",
      " [  0.08360072]]\n",
      "the iterations of  393400 the loss is 0.2656359106443867 theta= [[-10.55178734]\n",
      " [  0.08981682]\n",
      " [  0.08361064]]\n",
      "the iterations of  393500 the loss is 0.26562042446161005 theta= [[-10.55303176]\n",
      " [  0.08982665]\n",
      " [  0.08362057]]\n",
      "the iterations of  393600 the loss is 0.2656049448359583 theta= [[-10.55427591]\n",
      " [  0.08983647]\n",
      " [  0.0836305 ]]\n",
      "the iterations of  393700 the loss is 0.2655894717632467 theta= [[-10.5555198 ]\n",
      " [  0.08984629]\n",
      " [  0.08364042]]\n",
      "the iterations of  393800 the loss is 0.26557400523929375 theta= [[-10.55676343]\n",
      " [  0.08985611]\n",
      " [  0.08365034]]\n",
      "the iterations of  393900 the loss is 0.26555854525992195 theta= [[-10.55800679]\n",
      " [  0.08986592]\n",
      " [  0.08366026]]\n",
      "the iterations of  394000 the loss is 0.2655430918209572 theta= [[-10.55924989]\n",
      " [  0.08987574]\n",
      " [  0.08367017]]\n",
      "the iterations of  394100 the loss is 0.2655276449182291 theta= [[-10.56049273]\n",
      " [  0.08988555]\n",
      " [  0.08368009]]\n",
      "the iterations of  394200 the loss is 0.26551220454757063 theta= [[-10.56173531]\n",
      " [  0.08989536]\n",
      " [  0.08369   ]]\n",
      "the iterations of  394300 the loss is 0.2654967707048186 theta= [[-10.56297762]\n",
      " [  0.08990517]\n",
      " [  0.08369991]]\n",
      "the iterations of  394400 the loss is 0.2654813433858135 theta= [[-10.56421967]\n",
      " [  0.08991497]\n",
      " [  0.08370981]]\n",
      "the iterations of  394500 the loss is 0.26546592258639884 theta= [[-10.56546145]\n",
      " [  0.08992478]\n",
      " [  0.08371972]]\n",
      "the iterations of  394600 the loss is 0.26545050830242245 theta= [[-10.56670298]\n",
      " [  0.08993458]\n",
      " [  0.08372962]]\n",
      "the iterations of  394700 the loss is 0.265435100529735 theta= [[-10.56794424]\n",
      " [  0.08994438]\n",
      " [  0.08373953]]\n",
      "the iterations of  394800 the loss is 0.26541969926419146 theta= [[-10.56918524]\n",
      " [  0.08995418]\n",
      " [  0.08374942]]\n",
      "the iterations of  394900 the loss is 0.2654043045016497 theta= [[-10.57042598]\n",
      " [  0.08996397]\n",
      " [  0.08375932]]\n",
      "the iterations of  395000 the loss is 0.26538891623797173 theta= [[-10.57166646]\n",
      " [  0.08997377]\n",
      " [  0.08376922]]\n",
      "the iterations of  395100 the loss is 0.26537353446902245 theta= [[-10.57290667]\n",
      " [  0.08998356]\n",
      " [  0.08377911]]\n",
      "the iterations of  395200 the loss is 0.2653581591906711 theta= [[-10.57414662]\n",
      " [  0.08999335]\n",
      " [  0.083789  ]]\n",
      "the iterations of  395300 the loss is 0.2653427903987896 theta= [[-10.57538632]\n",
      " [  0.09000314]\n",
      " [  0.08379889]]\n",
      "the iterations of  395400 the loss is 0.26532742808925425 theta= [[-10.57662575]\n",
      " [  0.09001292]\n",
      " [  0.08380878]]\n",
      "the iterations of  395500 the loss is 0.26531207225794445 theta= [[-10.57786491]\n",
      " [  0.0900227 ]\n",
      " [  0.08381866]]\n",
      "the iterations of  395600 the loss is 0.265296722900743 theta= [[-10.57910382]\n",
      " [  0.09003249]\n",
      " [  0.08382855]]\n",
      "the iterations of  395700 the loss is 0.26528138001353674 theta= [[-10.58034247]\n",
      " [  0.09004227]\n",
      " [  0.08383843]]\n",
      "the iterations of  395800 the loss is 0.26526604359221556 theta= [[-10.58158085]\n",
      " [  0.09005204]\n",
      " [  0.08384831]]\n",
      "the iterations of  395900 the loss is 0.265250713632673 theta= [[-10.58281898]\n",
      " [  0.09006182]\n",
      " [  0.08385818]]\n",
      "the iterations of  396000 the loss is 0.2652353901308062 theta= [[-10.58405684]\n",
      " [  0.09007159]\n",
      " [  0.08386806]]\n",
      "the iterations of  396100 the loss is 0.26522007308251583 theta= [[-10.58529444]\n",
      " [  0.09008136]\n",
      " [  0.08387793]]\n",
      "the iterations of  396200 the loss is 0.265204762483706 theta= [[-10.58653178]\n",
      " [  0.09009113]\n",
      " [  0.0838878 ]]\n",
      "the iterations of  396300 the loss is 0.2651894583302845 theta= [[-10.58776887]\n",
      " [  0.0901009 ]\n",
      " [  0.08389767]]\n",
      "the iterations of  396400 the loss is 0.2651741606181626 theta= [[-10.58900569]\n",
      " [  0.09011067]\n",
      " [  0.08390754]]\n",
      "the iterations of  396500 the loss is 0.26515886934325456 theta= [[-10.59024225]\n",
      " [  0.09012043]\n",
      " [  0.0839174 ]]\n",
      "the iterations of  396600 the loss is 0.2651435845014788 theta= [[-10.59147855]\n",
      " [  0.09013019]\n",
      " [  0.08392726]]\n",
      "the iterations of  396700 the loss is 0.2651283060887571 theta= [[-10.59271459]\n",
      " [  0.09013995]\n",
      " [  0.08393713]]\n",
      "the iterations of  396800 the loss is 0.2651130341010143 theta= [[-10.59395037]\n",
      " [  0.09014971]\n",
      " [  0.08394698]]\n",
      "the iterations of  396900 the loss is 0.26509776853417916 theta= [[-10.59518589]\n",
      " [  0.09015946]\n",
      " [  0.08395684]]\n",
      "the iterations of  397000 the loss is 0.265082509384184 theta= [[-10.59642115]\n",
      " [  0.09016922]\n",
      " [  0.0839667 ]]\n",
      "the iterations of  397100 the loss is 0.2650672566469642 theta= [[-10.59765615]\n",
      " [  0.09017897]\n",
      " [  0.08397655]]\n",
      "the iterations of  397200 the loss is 0.2650520103184589 theta= [[-10.5988909 ]\n",
      " [  0.09018872]\n",
      " [  0.0839864 ]]\n",
      "the iterations of  397300 the loss is 0.26503677039461043 theta= [[-10.60012538]\n",
      " [  0.09019846]\n",
      " [  0.08399625]]\n",
      "the iterations of  397400 the loss is 0.26502153687136526 theta= [[-10.6013596 ]\n",
      " [  0.09020821]\n",
      " [  0.08400609]]\n",
      "the iterations of  397500 the loss is 0.26500630974467254 theta= [[-10.60259357]\n",
      " [  0.09021795]\n",
      " [  0.08401594]]\n",
      "the iterations of  397600 the loss is 0.2649910890104854 theta= [[-10.60382727]\n",
      " [  0.09022769]\n",
      " [  0.08402578]]\n",
      "the iterations of  397700 the loss is 0.26497587466476014 theta= [[-10.60506072]\n",
      " [  0.09023743]\n",
      " [  0.08403562]]\n",
      "the iterations of  397800 the loss is 0.2649606667034566 theta= [[-10.60629391]\n",
      " [  0.09024717]\n",
      " [  0.08404546]]\n",
      "the iterations of  397900 the loss is 0.26494546512253836 theta= [[-10.60752684]\n",
      " [  0.09025691]\n",
      " [  0.08405529]]\n",
      "the iterations of  398000 the loss is 0.264930269917972 theta= [[-10.60875951]\n",
      " [  0.09026664]\n",
      " [  0.08406513]]\n",
      "the iterations of  398100 the loss is 0.2649150810857277 theta= [[-10.60999192]\n",
      " [  0.09027637]\n",
      " [  0.08407496]]\n",
      "the iterations of  398200 the loss is 0.26489989862177904 theta= [[-10.61122407]\n",
      " [  0.0902861 ]\n",
      " [  0.08408479]]\n",
      "the iterations of  398300 the loss is 0.2648847225221031 theta= [[-10.61245597]\n",
      " [  0.09029583]\n",
      " [  0.08409462]]\n",
      "the iterations of  398400 the loss is 0.26486955278268065 theta= [[-10.6136876 ]\n",
      " [  0.09030555]\n",
      " [  0.08410444]]\n",
      "the iterations of  398500 the loss is 0.2648543893994954 theta= [[-10.61491898]\n",
      " [  0.09031527]\n",
      " [  0.08411427]]\n",
      "the iterations of  398600 the loss is 0.2648392323685348 theta= [[-10.6161501 ]\n",
      " [  0.090325  ]\n",
      " [  0.08412409]]\n",
      "the iterations of  398700 the loss is 0.2648240816857896 theta= [[-10.61738097]\n",
      " [  0.09033472]\n",
      " [  0.08413391]]\n",
      "the iterations of  398800 the loss is 0.264808937347254 theta= [[-10.61861157]\n",
      " [  0.09034443]\n",
      " [  0.08414373]]\n",
      "the iterations of  398900 the loss is 0.2647937993489257 theta= [[-10.61984192]\n",
      " [  0.09035415]\n",
      " [  0.08415354]]\n",
      "the iterations of  399000 the loss is 0.2647786676868057 theta= [[-10.62107201]\n",
      " [  0.09036386]\n",
      " [  0.08416336]]\n",
      "the iterations of  399100 the loss is 0.26476354235689836 theta= [[-10.62230184]\n",
      " [  0.09037357]\n",
      " [  0.08417317]]\n",
      "the iterations of  399200 the loss is 0.2647484233552116 theta= [[-10.62353142]\n",
      " [  0.09038328]\n",
      " [  0.08418298]]\n",
      "the iterations of  399300 the loss is 0.26473331067775663 theta= [[-10.62476074]\n",
      " [  0.09039299]\n",
      " [  0.08419279]]\n",
      "the iterations of  399400 the loss is 0.264718204320548 theta= [[-10.6259898 ]\n",
      " [  0.09040269]\n",
      " [  0.08420259]]\n",
      "the iterations of  399500 the loss is 0.26470310427960375 theta= [[-10.6272186]\n",
      " [  0.0904124]\n",
      " [  0.0842124]]\n",
      "the iterations of  399600 the loss is 0.26468801055094543 theta= [[-10.62844715]\n",
      " [  0.0904221 ]\n",
      " [  0.0842222 ]]\n",
      "the iterations of  399700 the loss is 0.26467292313059787 theta= [[-10.62967544]\n",
      " [  0.0904318 ]\n",
      " [  0.084232  ]]\n",
      "the iterations of  399800 the loss is 0.264657842014589 theta= [[-10.63090347]\n",
      " [  0.0904415 ]\n",
      " [  0.0842418 ]]\n",
      "the iterations of  399900 the loss is 0.26464276719895047 theta= [[-10.63213125]\n",
      " [  0.09045119]\n",
      " [  0.08425159]]\n",
      "the iterations of  400000 the loss is 0.26462769867971736 theta= [[-10.63335877]\n",
      " [  0.09046088]\n",
      " [  0.08426139]]\n",
      "the iterations of  400100 the loss is 0.2646126364529279 theta= [[-10.63458603]\n",
      " [  0.09047058]\n",
      " [  0.08427118]]\n",
      "the iterations of  400200 the loss is 0.26459758051462356 theta= [[-10.63581304]\n",
      " [  0.09048026]\n",
      " [  0.08428097]]\n",
      "the iterations of  400300 the loss is 0.26458253086084965 theta= [[-10.6370398 ]\n",
      " [  0.09048995]\n",
      " [  0.08429076]]\n",
      "the iterations of  400400 the loss is 0.2645674874876544 theta= [[-10.63826629]\n",
      " [  0.09049964]\n",
      " [  0.08430054]]\n",
      "the iterations of  400500 the loss is 0.26455245039108966 theta= [[-10.63949253]\n",
      " [  0.09050932]\n",
      " [  0.08431033]]\n",
      "the iterations of  400600 the loss is 0.26453741956721033 theta= [[-10.64071852]\n",
      " [  0.090519  ]\n",
      " [  0.08432011]]\n",
      "the iterations of  400700 the loss is 0.26452239501207514 theta= [[-10.64194425]\n",
      " [  0.09052868]\n",
      " [  0.08432989]]\n",
      "the iterations of  400800 the loss is 0.2645073767217457 theta= [[-10.64316972]\n",
      " [  0.09053836]\n",
      " [  0.08433967]]\n",
      "the iterations of  400900 the loss is 0.2644923646922873 theta= [[-10.64439494]\n",
      " [  0.09054804]\n",
      " [  0.08434944]]\n",
      "the iterations of  401000 the loss is 0.2644773589197682 theta= [[-10.6456199 ]\n",
      " [  0.09055771]\n",
      " [  0.08435921]]\n",
      "the iterations of  401100 the loss is 0.26446235940026025 theta= [[-10.64684461]\n",
      " [  0.09056738]\n",
      " [  0.08436899]]\n",
      "the iterations of  401200 the loss is 0.26444736612983877 theta= [[-10.64806906]\n",
      " [  0.09057705]\n",
      " [  0.08437876]]\n",
      "the iterations of  401300 the loss is 0.26443237910458206 theta= [[-10.64929326]\n",
      " [  0.09058672]\n",
      " [  0.08438852]]\n",
      "the iterations of  401400 the loss is 0.26441739832057193 theta= [[-10.6505172 ]\n",
      " [  0.09059638]\n",
      " [  0.08439829]]\n",
      "the iterations of  401500 the loss is 0.2644024237738934 theta= [[-10.65174088]\n",
      " [  0.09060605]\n",
      " [  0.08440805]]\n",
      "the iterations of  401600 the loss is 0.26438745546063497 theta= [[-10.65296432]\n",
      " [  0.09061571]\n",
      " [  0.08441782]]\n",
      "the iterations of  401700 the loss is 0.26437249337688856 theta= [[-10.65418749]\n",
      " [  0.09062537]\n",
      " [  0.08442758]]\n",
      "the iterations of  401800 the loss is 0.26435753751874885 theta= [[-10.65541042]\n",
      " [  0.09063503]\n",
      " [  0.08443733]]\n",
      "the iterations of  401900 the loss is 0.2643425878823145 theta= [[-10.65663309]\n",
      " [  0.09064468]\n",
      " [  0.08444709]]\n",
      "the iterations of  402000 the loss is 0.26432764446368706 theta= [[-10.6578555 ]\n",
      " [  0.09065434]\n",
      " [  0.08445684]]\n",
      "the iterations of  402100 the loss is 0.2643127072589715 theta= [[-10.65907766]\n",
      " [  0.09066399]\n",
      " [  0.08446659]]\n",
      "the iterations of  402200 the loss is 0.2642977762642758 theta= [[-10.66029957]\n",
      " [  0.09067364]\n",
      " [  0.08447634]]\n",
      "the iterations of  402300 the loss is 0.2642828514757119 theta= [[-10.66152122]\n",
      " [  0.09068329]\n",
      " [  0.08448609]]\n",
      "the iterations of  402400 the loss is 0.2642679328893946 theta= [[-10.66274262]\n",
      " [  0.09069293]\n",
      " [  0.08449584]]\n",
      "the iterations of  402500 the loss is 0.26425302050144195 theta= [[-10.66396376]\n",
      " [  0.09070258]\n",
      " [  0.08450558]]\n",
      "the iterations of  402600 the loss is 0.26423811430797517 theta= [[-10.66518466]\n",
      " [  0.09071222]\n",
      " [  0.08451532]]\n",
      "the iterations of  402700 the loss is 0.264223214305119 theta= [[-10.66640529]\n",
      " [  0.09072186]\n",
      " [  0.08452506]]\n",
      "the iterations of  402800 the loss is 0.2642083204890018 theta= [[-10.66762568]\n",
      " [  0.0907315 ]\n",
      " [  0.0845348 ]]\n",
      "the iterations of  402900 the loss is 0.26419343285575425 theta= [[-10.66884581]\n",
      " [  0.09074113]\n",
      " [  0.08454454]]\n",
      "the iterations of  403000 the loss is 0.2641785514015111 theta= [[-10.67006569]\n",
      " [  0.09075077]\n",
      " [  0.08455427]]\n",
      "the iterations of  403100 the loss is 0.26416367612240993 theta= [[-10.67128531]\n",
      " [  0.0907604 ]\n",
      " [  0.084564  ]]\n",
      "the iterations of  403200 the loss is 0.264148807014592 theta= [[-10.67250468]\n",
      " [  0.09077003]\n",
      " [  0.08457373]]\n",
      "the iterations of  403300 the loss is 0.2641339440742014 theta= [[-10.6737238 ]\n",
      " [  0.09077966]\n",
      " [  0.08458346]]\n",
      "the iterations of  403400 the loss is 0.26411908729738576 theta= [[-10.67494267]\n",
      " [  0.09078928]\n",
      " [  0.08459319]]\n",
      "the iterations of  403500 the loss is 0.26410423668029576 theta= [[-10.67616128]\n",
      " [  0.09079891]\n",
      " [  0.08460291]]\n",
      "the iterations of  403600 the loss is 0.2640893922190855 theta= [[-10.67737964]\n",
      " [  0.09080853]\n",
      " [  0.08461263]]\n",
      "the iterations of  403700 the loss is 0.2640745539099123 theta= [[-10.67859775]\n",
      " [  0.09081815]\n",
      " [  0.08462235]]\n",
      "the iterations of  403800 the loss is 0.2640597217489364 theta= [[-10.6798156 ]\n",
      " [  0.09082777]\n",
      " [  0.08463207]]\n",
      "the iterations of  403900 the loss is 0.2640448957323217 theta= [[-10.68103321]\n",
      " [  0.09083739]\n",
      " [  0.08464179]]\n",
      "the iterations of  404000 the loss is 0.2640300758562354 theta= [[-10.68225056]\n",
      " [  0.090847  ]\n",
      " [  0.0846515 ]]\n",
      "the iterations of  404100 the loss is 0.2640152621168473 theta= [[-10.68346766]\n",
      " [  0.09085661]\n",
      " [  0.08466121]]\n",
      "the iterations of  404200 the loss is 0.2640004545103311 theta= [[-10.6846845 ]\n",
      " [  0.09086622]\n",
      " [  0.08467092]]\n",
      "the iterations of  404300 the loss is 0.2639856530328636 theta= [[-10.6859011 ]\n",
      " [  0.09087583]\n",
      " [  0.08468063]]\n",
      "the iterations of  404400 the loss is 0.26397085768062445 theta= [[-10.68711744]\n",
      " [  0.09088544]\n",
      " [  0.08469034]]\n",
      "the iterations of  404500 the loss is 0.2639560684497966 theta= [[-10.68833353]\n",
      " [  0.09089504]\n",
      " [  0.08470004]]\n",
      "the iterations of  404600 the loss is 0.26394128533656663 theta= [[-10.68954937]\n",
      " [  0.09090465]\n",
      " [  0.08470974]]\n",
      "the iterations of  404700 the loss is 0.2639265083371238 theta= [[-10.69076496]\n",
      " [  0.09091425]\n",
      " [  0.08471944]]\n",
      "the iterations of  404800 the loss is 0.26391173744766105 theta= [[-10.6919803 ]\n",
      " [  0.09092385]\n",
      " [  0.08472914]]\n",
      "the iterations of  404900 the loss is 0.26389697266437434 theta= [[-10.69319538]\n",
      " [  0.09093344]\n",
      " [  0.08473884]]\n",
      "the iterations of  405000 the loss is 0.2638822139834626 theta= [[-10.69441022]\n",
      " [  0.09094304]\n",
      " [  0.08474853]]\n",
      "the iterations of  405100 the loss is 0.2638674614011282 theta= [[-10.6956248 ]\n",
      " [  0.09095263]\n",
      " [  0.08475822]]\n",
      "the iterations of  405200 the loss is 0.26385271491357665 theta= [[-10.69683914]\n",
      " [  0.09096222]\n",
      " [  0.08476792]]\n",
      "the iterations of  405300 the loss is 0.2638379745170167 theta= [[-10.69805322]\n",
      " [  0.09097181]\n",
      " [  0.0847776 ]]\n",
      "the iterations of  405400 the loss is 0.2638232402076603 theta= [[-10.69926705]\n",
      " [  0.0909814 ]\n",
      " [  0.08478729]]\n",
      "the iterations of  405500 the loss is 0.2638085119817222 theta= [[-10.70048063]\n",
      " [  0.09099098]\n",
      " [  0.08479697]]\n",
      "the iterations of  405600 the loss is 0.2637937898354209 theta= [[-10.70169396]\n",
      " [  0.09100057]\n",
      " [  0.08480666]]\n",
      "the iterations of  405700 the loss is 0.2637790737649778 theta= [[-10.70290704]\n",
      " [  0.09101015]\n",
      " [  0.08481634]]\n",
      "the iterations of  405800 the loss is 0.26376436376661744 theta= [[-10.70411987]\n",
      " [  0.09101973]\n",
      " [  0.08482602]]\n",
      "the iterations of  405900 the loss is 0.2637496598365676 theta= [[-10.70533245]\n",
      " [  0.09102931]\n",
      " [  0.08483569]]\n",
      "the iterations of  406000 the loss is 0.26373496197105917 theta= [[-10.70654478]\n",
      " [  0.09103888]\n",
      " [  0.08484537]]\n",
      "the iterations of  406100 the loss is 0.26372027016632615 theta= [[-10.70775686]\n",
      " [  0.09104846]\n",
      " [  0.08485504]]\n",
      "the iterations of  406200 the loss is 0.26370558441860614 theta= [[-10.70896869]\n",
      " [  0.09105803]\n",
      " [  0.08486471]]\n",
      "the iterations of  406300 the loss is 0.2636909047241394 theta= [[-10.71018027]\n",
      " [  0.0910676 ]\n",
      " [  0.08487438]]\n",
      "the iterations of  406400 the loss is 0.26367623107916927 theta= [[-10.7113916 ]\n",
      " [  0.09107717]\n",
      " [  0.08488405]]\n",
      "the iterations of  406500 the loss is 0.26366156347994263 theta= [[-10.71260268]\n",
      " [  0.09108673]\n",
      " [  0.08489371]]\n",
      "the iterations of  406600 the loss is 0.2636469019227093 theta= [[-10.71381351]\n",
      " [  0.0910963 ]\n",
      " [  0.08490337]]\n",
      "the iterations of  406700 the loss is 0.2636322464037222 theta= [[-10.71502409]\n",
      " [  0.09110586]\n",
      " [  0.08491304]]\n",
      "the iterations of  406800 the loss is 0.2636175969192375 theta= [[-10.71623442]\n",
      " [  0.09111542]\n",
      " [  0.08492269]]\n",
      "the iterations of  406900 the loss is 0.26360295346551443 theta= [[-10.71744451]\n",
      " [  0.09112498]\n",
      " [  0.08493235]]\n",
      "the iterations of  407000 the loss is 0.26358831603881555 theta= [[-10.71865434]\n",
      " [  0.09113453]\n",
      " [  0.08494201]]\n",
      "the iterations of  407100 the loss is 0.26357368463540615 theta= [[-10.71986393]\n",
      " [  0.09114409]\n",
      " [  0.08495166]]\n",
      "the iterations of  407200 the loss is 0.2635590592515551 theta= [[-10.72107326]\n",
      " [  0.09115364]\n",
      " [  0.08496131]]\n",
      "the iterations of  407300 the loss is 0.2635444398835341 theta= [[-10.72228235]\n",
      " [  0.09116319]\n",
      " [  0.08497096]]\n",
      "the iterations of  407400 the loss is 0.26352982652761786 theta= [[-10.72349119]\n",
      " [  0.09117274]\n",
      " [  0.08498061]]\n",
      "the iterations of  407500 the loss is 0.26351521918008464 theta= [[-10.72469978]\n",
      " [  0.09118229]\n",
      " [  0.08499025]]\n",
      "the iterations of  407600 the loss is 0.2635006178372154 theta= [[-10.72590812]\n",
      " [  0.09119183]\n",
      " [  0.0849999 ]]\n",
      "the iterations of  407700 the loss is 0.26348602249529446 theta= [[-10.72711622]\n",
      " [  0.09120137]\n",
      " [  0.08500954]]\n",
      "the iterations of  407800 the loss is 0.2634714331506091 theta= [[-10.72832406]\n",
      " [  0.09121091]\n",
      " [  0.08501918]]\n",
      "the iterations of  407900 the loss is 0.2634568497994499 theta= [[-10.72953166]\n",
      " [  0.09122045]\n",
      " [  0.08502881]]\n",
      "the iterations of  408000 the loss is 0.26344227243811014 theta= [[-10.73073901]\n",
      " [  0.09122999]\n",
      " [  0.08503845]]\n",
      "the iterations of  408100 the loss is 0.2634277010628866 theta= [[-10.73194611]\n",
      " [  0.09123953]\n",
      " [  0.08504808]]\n",
      "the iterations of  408200 the loss is 0.2634131356700791 theta= [[-10.73315296]\n",
      " [  0.09124906]\n",
      " [  0.08505772]]\n",
      "the iterations of  408300 the loss is 0.2633985762559904 theta= [[-10.73435957]\n",
      " [  0.09125859]\n",
      " [  0.08506735]]\n",
      "the iterations of  408400 the loss is 0.26338402281692636 theta= [[-10.73556593]\n",
      " [  0.09126812]\n",
      " [  0.08507697]]\n",
      "the iterations of  408500 the loss is 0.26336947534919597 theta= [[-10.73677204]\n",
      " [  0.09127765]\n",
      " [  0.0850866 ]]\n",
      "the iterations of  408600 the loss is 0.2633549338491114 theta= [[-10.7379779 ]\n",
      " [  0.09128717]\n",
      " [  0.08509622]]\n",
      "the iterations of  408700 the loss is 0.26334039831298767 theta= [[-10.73918352]\n",
      " [  0.0912967 ]\n",
      " [  0.08510585]]\n",
      "the iterations of  408800 the loss is 0.26332586873714303 theta= [[-10.74038889]\n",
      " [  0.09130622]\n",
      " [  0.08511547]]\n",
      "the iterations of  408900 the loss is 0.2633113451178987 theta= [[-10.74159401]\n",
      " [  0.09131574]\n",
      " [  0.08512508]]\n",
      "the iterations of  409000 the loss is 0.2632968274515791 theta= [[-10.74279888]\n",
      " [  0.09132526]\n",
      " [  0.0851347 ]]\n",
      "the iterations of  409100 the loss is 0.26328231573451166 theta= [[-10.74400351]\n",
      " [  0.09133477]\n",
      " [  0.08514431]]\n",
      "the iterations of  409200 the loss is 0.2632678099630268 theta= [[-10.74520789]\n",
      " [  0.09134429]\n",
      " [  0.08515393]]\n",
      "the iterations of  409300 the loss is 0.26325331013345804 theta= [[-10.74641203]\n",
      " [  0.0913538 ]\n",
      " [  0.08516354]]\n",
      "the iterations of  409400 the loss is 0.2632388162421422 theta= [[-10.74761591]\n",
      " [  0.09136331]\n",
      " [  0.08517315]]\n",
      "the iterations of  409500 the loss is 0.26322432828541875 theta= [[-10.74881955]\n",
      " [  0.09137282]\n",
      " [  0.08518275]]\n",
      "the iterations of  409600 the loss is 0.26320984625963023 theta= [[-10.75002295]\n",
      " [  0.09138232]\n",
      " [  0.08519236]]\n",
      "the iterations of  409700 the loss is 0.2631953701611225 theta= [[-10.7512261 ]\n",
      " [  0.09139183]\n",
      " [  0.08520196]]\n",
      "the iterations of  409800 the loss is 0.2631808999862445 theta= [[-10.752429  ]\n",
      " [  0.09140133]\n",
      " [  0.08521156]]\n",
      "the iterations of  409900 the loss is 0.26316643573134785 theta= [[-10.75363166]\n",
      " [  0.09141083]\n",
      " [  0.08522116]]\n",
      "the iterations of  410000 the loss is 0.26315197739278756 theta= [[-10.75483407]\n",
      " [  0.09142033]\n",
      " [  0.08523076]]\n",
      "the iterations of  410100 the loss is 0.26313752496692144 theta= [[-10.75603623]\n",
      " [  0.09142983]\n",
      " [  0.08524035]]\n",
      "the iterations of  410200 the loss is 0.2631230784501103 theta= [[-10.75723815]\n",
      " [  0.09143932]\n",
      " [  0.08524994]]\n",
      "the iterations of  410300 the loss is 0.2631086378387182 theta= [[-10.75843982]\n",
      " [  0.09144882]\n",
      " [  0.08525953]]\n",
      "the iterations of  410400 the loss is 0.26309420312911186 theta= [[-10.75964125]\n",
      " [  0.09145831]\n",
      " [  0.08526912]]\n",
      "the iterations of  410500 the loss is 0.2630797743176615 theta= [[-10.76084243]\n",
      " [  0.0914678 ]\n",
      " [  0.08527871]]\n",
      "the iterations of  410600 the loss is 0.2630653514007403 theta= [[-10.76204337]\n",
      " [  0.09147728]\n",
      " [  0.0852883 ]]\n",
      "the iterations of  410700 the loss is 0.2630509343747238 theta= [[-10.76324406]\n",
      " [  0.09148677]\n",
      " [  0.08529788]]\n",
      "the iterations of  410800 the loss is 0.2630365232359914 theta= [[-10.7644445 ]\n",
      " [  0.09149625]\n",
      " [  0.08530746]]\n",
      "the iterations of  410900 the loss is 0.26302211798092484 theta= [[-10.7656447 ]\n",
      " [  0.09150573]\n",
      " [  0.08531704]]\n",
      "the iterations of  411000 the loss is 0.2630077186059092 theta= [[-10.76684466]\n",
      " [  0.09151521]\n",
      " [  0.08532662]]\n",
      "the iterations of  411100 the loss is 0.26299332510733264 theta= [[-10.76804437]\n",
      " [  0.09152469]\n",
      " [  0.08533619]]\n",
      "the iterations of  411200 the loss is 0.2629789374815862 theta= [[-10.76924384]\n",
      " [  0.09153417]\n",
      " [  0.08534577]]\n",
      "the iterations of  411300 the loss is 0.26296455572506366 theta= [[-10.77044306]\n",
      " [  0.09154364]\n",
      " [  0.08535534]]\n",
      "the iterations of  411400 the loss is 0.2629501798341622 theta= [[-10.77164203]\n",
      " [  0.09155311]\n",
      " [  0.08536491]]\n",
      "the iterations of  411500 the loss is 0.2629358098052819 theta= [[-10.77284077]\n",
      " [  0.09156258]\n",
      " [  0.08537448]]\n",
      "the iterations of  411600 the loss is 0.26292144563482567 theta= [[-10.77403926]\n",
      " [  0.09157205]\n",
      " [  0.08538404]]\n",
      "the iterations of  411700 the loss is 0.26290708731919943 theta= [[-10.7752375 ]\n",
      " [  0.09158152]\n",
      " [  0.08539361]]\n",
      "the iterations of  411800 the loss is 0.26289273485481224 theta= [[-10.7764355 ]\n",
      " [  0.09159098]\n",
      " [  0.08540317]]\n",
      "the iterations of  411900 the loss is 0.26287838823807574 theta= [[-10.77763325]\n",
      " [  0.09160045]\n",
      " [  0.08541273]]\n",
      "the iterations of  412000 the loss is 0.262864047465405 theta= [[-10.77883077]\n",
      " [  0.09160991]\n",
      " [  0.08542229]]\n",
      "the iterations of  412100 the loss is 0.262849712533218 theta= [[-10.78002803]\n",
      " [  0.09161937]\n",
      " [  0.08543185]]\n",
      "the iterations of  412200 the loss is 0.2628353834379352 theta= [[-10.78122506]\n",
      " [  0.09162882]\n",
      " [  0.0854414 ]]\n",
      "the iterations of  412300 the loss is 0.26282106017598084 theta= [[-10.78242184]\n",
      " [  0.09163828]\n",
      " [  0.08545095]]\n",
      "the iterations of  412400 the loss is 0.26280674274378135 theta= [[-10.78361837]\n",
      " [  0.09164773]\n",
      " [  0.0854605 ]]\n",
      "the iterations of  412500 the loss is 0.2627924311377665 theta= [[-10.78481467]\n",
      " [  0.09165718]\n",
      " [  0.08547005]]\n",
      "the iterations of  412600 the loss is 0.2627781253543691 theta= [[-10.78601072]\n",
      " [  0.09166663]\n",
      " [  0.0854796 ]]\n",
      "the iterations of  412700 the loss is 0.26276382539002463 theta= [[-10.78720652]\n",
      " [  0.09167608]\n",
      " [  0.08548915]]\n",
      "the iterations of  412800 the loss is 0.26274953124117156 theta= [[-10.78840209]\n",
      " [  0.09168553]\n",
      " [  0.08549869]]\n",
      "the iterations of  412900 the loss is 0.2627352429042517 theta= [[-10.78959741]\n",
      " [  0.09169497]\n",
      " [  0.08550823]]\n",
      "the iterations of  413000 the loss is 0.262720960375709 theta= [[-10.79079248]\n",
      " [  0.09170441]\n",
      " [  0.08551777]]\n",
      "the iterations of  413100 the loss is 0.2627066836519911 theta= [[-10.79198732]\n",
      " [  0.09171385]\n",
      " [  0.08552731]]\n",
      "the iterations of  413200 the loss is 0.2626924127295486 theta= [[-10.79318191]\n",
      " [  0.09172329]\n",
      " [  0.08553684]]\n",
      "the iterations of  413300 the loss is 0.2626781476048344 theta= [[-10.79437626]\n",
      " [  0.09173273]\n",
      " [  0.08554638]]\n",
      "the iterations of  413400 the loss is 0.2626638882743048 theta= [[-10.79557036]\n",
      " [  0.09174216]\n",
      " [  0.08555591]]\n",
      "the iterations of  413500 the loss is 0.2626496347344187 theta= [[-10.79676423]\n",
      " [  0.09175159]\n",
      " [  0.08556544]]\n",
      "the iterations of  413600 the loss is 0.2626353869816385 theta= [[-10.79795785]\n",
      " [  0.09176102]\n",
      " [  0.08557497]]\n",
      "the iterations of  413700 the loss is 0.26262114501242895 theta= [[-10.79915123]\n",
      " [  0.09177045]\n",
      " [  0.08558449]]\n",
      "the iterations of  413800 the loss is 0.2626069088232577 theta= [[-10.80034436]\n",
      " [  0.09177988]\n",
      " [  0.08559402]]\n",
      "the iterations of  413900 the loss is 0.26259267841059597 theta= [[-10.80153726]\n",
      " [  0.0917893 ]\n",
      " [  0.08560354]]\n",
      "the iterations of  414000 the loss is 0.2625784537709172 theta= [[-10.80272991]\n",
      " [  0.09179873]\n",
      " [  0.08561306]]\n",
      "the iterations of  414100 the loss is 0.2625642349006981 theta= [[-10.80392232]\n",
      " [  0.09180815]\n",
      " [  0.08562258]]\n",
      "the iterations of  414200 the loss is 0.26255002179641795 theta= [[-10.80511449]\n",
      " [  0.09181757]\n",
      " [  0.08563209]]\n",
      "the iterations of  414300 the loss is 0.26253581445455926 theta= [[-10.80630642]\n",
      " [  0.09182699]\n",
      " [  0.08564161]]\n",
      "the iterations of  414400 the loss is 0.2625216128716074 theta= [[-10.8074981 ]\n",
      " [  0.0918364 ]\n",
      " [  0.08565112]]\n",
      "the iterations of  414500 the loss is 0.2625074170440507 theta= [[-10.80868955]\n",
      " [  0.09184582]\n",
      " [  0.08566063]]\n",
      "the iterations of  414600 the loss is 0.26249322696837984 theta= [[-10.80988075]\n",
      " [  0.09185523]\n",
      " [  0.08567014]]\n",
      "the iterations of  414700 the loss is 0.2624790426410891 theta= [[-10.81107171]\n",
      " [  0.09186464]\n",
      " [  0.08567965]]\n",
      "the iterations of  414800 the loss is 0.2624648640586753 theta= [[-10.81226243]\n",
      " [  0.09187405]\n",
      " [  0.08568915]]\n",
      "the iterations of  414900 the loss is 0.26245069121763825 theta= [[-10.81345291]\n",
      " [  0.09188345]\n",
      " [  0.08569866]]\n",
      "the iterations of  415000 the loss is 0.26243652411448054 theta= [[-10.81464315]\n",
      " [  0.09189286]\n",
      " [  0.08570816]]\n",
      "the iterations of  415100 the loss is 0.2624223627457077 theta= [[-10.81583315]\n",
      " [  0.09190226]\n",
      " [  0.08571766]]\n",
      "the iterations of  415200 the loss is 0.26240820710782814 theta= [[-10.8170229 ]\n",
      " [  0.09191166]\n",
      " [  0.08572715]]\n",
      "the iterations of  415300 the loss is 0.26239405719735304 theta= [[-10.81821242]\n",
      " [  0.09192106]\n",
      " [  0.08573665]]\n",
      "the iterations of  415400 the loss is 0.26237991301079655 theta= [[-10.8194017 ]\n",
      " [  0.09193046]\n",
      " [  0.08574614]]\n",
      "the iterations of  415500 the loss is 0.2623657745446757 theta= [[-10.82059073]\n",
      " [  0.09193985]\n",
      " [  0.08575564]]\n",
      "the iterations of  415600 the loss is 0.2623516417955104 theta= [[-10.82177952]\n",
      " [  0.09194924]\n",
      " [  0.08576513]]\n",
      "the iterations of  415700 the loss is 0.26233751475982314 theta= [[-10.82296808]\n",
      " [  0.09195864]\n",
      " [  0.08577461]]\n",
      "the iterations of  415800 the loss is 0.26232339343413985 theta= [[-10.82415639]\n",
      " [  0.09196802]\n",
      " [  0.0857841 ]]\n",
      "the iterations of  415900 the loss is 0.2623092778149886 theta= [[-10.82534447]\n",
      " [  0.09197741]\n",
      " [  0.08579358]]\n",
      "the iterations of  416000 the loss is 0.262295167898901 theta= [[-10.8265323 ]\n",
      " [  0.0919868 ]\n",
      " [  0.08580307]]\n",
      "the iterations of  416100 the loss is 0.26228106368241094 theta= [[-10.82771989]\n",
      " [  0.09199618]\n",
      " [  0.08581255]]\n",
      "the iterations of  416200 the loss is 0.2622669651620556 theta= [[-10.82890725]\n",
      " [  0.09200556]\n",
      " [  0.08582203]]\n",
      "the iterations of  416300 the loss is 0.2622528723343746 theta= [[-10.83009436]\n",
      " [  0.09201494]\n",
      " [  0.0858315 ]]\n",
      "the iterations of  416400 the loss is 0.2622387851959107 theta= [[-10.83128123]\n",
      " [  0.09202432]\n",
      " [  0.08584098]]\n",
      "the iterations of  416500 the loss is 0.26222470374320944 theta= [[-10.83246787]\n",
      " [  0.0920337 ]\n",
      " [  0.08585045]]\n",
      "the iterations of  416600 the loss is 0.26221062797281897 theta= [[-10.83365426]\n",
      " [  0.09204307]\n",
      " [  0.08585992]]\n",
      "the iterations of  416700 the loss is 0.26219655788129065 theta= [[-10.83484042]\n",
      " [  0.09205245]\n",
      " [  0.08586939]]\n",
      "the iterations of  416800 the loss is 0.26218249346517847 theta= [[-10.83602634]\n",
      " [  0.09206182]\n",
      " [  0.08587886]]\n",
      "the iterations of  416900 the loss is 0.26216843472103923 theta= [[-10.83721201]\n",
      " [  0.09207119]\n",
      " [  0.08588833]]\n",
      "the iterations of  417000 the loss is 0.2621543816454323 theta= [[-10.83839745]\n",
      " [  0.09208055]\n",
      " [  0.08589779]]\n",
      "the iterations of  417100 the loss is 0.26214033423492067 theta= [[-10.83958265]\n",
      " [  0.09208992]\n",
      " [  0.08590725]]\n",
      "the iterations of  417200 the loss is 0.262126292486069 theta= [[-10.84076761]\n",
      " [  0.09209928]\n",
      " [  0.08591671]]\n",
      "the iterations of  417300 the loss is 0.26211225639544583 theta= [[-10.84195233]\n",
      " [  0.09210864]\n",
      " [  0.08592617]]\n",
      "the iterations of  417400 the loss is 0.2620982259596218 theta= [[-10.84313682]\n",
      " [  0.092118  ]\n",
      " [  0.08593563]]\n",
      "the iterations of  417500 the loss is 0.2620842011751707 theta= [[-10.84432106]\n",
      " [  0.09212736]\n",
      " [  0.08594508]]\n",
      "the iterations of  417600 the loss is 0.26207018203866905 theta= [[-10.84550507]\n",
      " [  0.09213672]\n",
      " [  0.08595453]]\n",
      "the iterations of  417700 the loss is 0.26205616854669606 theta= [[-10.84668884]\n",
      " [  0.09214607]\n",
      " [  0.08596398]]\n",
      "the iterations of  417800 the loss is 0.26204216069583397 theta= [[-10.84787237]\n",
      " [  0.09215542]\n",
      " [  0.08597343]]\n",
      "the iterations of  417900 the loss is 0.26202815848266753 theta= [[-10.84905566]\n",
      " [  0.09216478]\n",
      " [  0.08598288]]\n",
      "the iterations of  418000 the loss is 0.2620141619037847 theta= [[-10.85023871]\n",
      " [  0.09217412]\n",
      " [  0.08599232]]\n",
      "the iterations of  418100 the loss is 0.2620001709557758 theta= [[-10.85142152]\n",
      " [  0.09218347]\n",
      " [  0.08600177]]\n",
      "the iterations of  418200 the loss is 0.26198618563523424 theta= [[-10.8526041 ]\n",
      " [  0.09219282]\n",
      " [  0.08601121]]\n",
      "the iterations of  418300 the loss is 0.2619722059387559 theta= [[-10.85378644]\n",
      " [  0.09220216]\n",
      " [  0.08602065]]\n",
      "the iterations of  418400 the loss is 0.2619582318629397 theta= [[-10.85496854]\n",
      " [  0.0922115 ]\n",
      " [  0.08603008]]\n",
      "the iterations of  418500 the loss is 0.2619442634043871 theta= [[-10.85615041]\n",
      " [  0.09222084]\n",
      " [  0.08603952]]\n",
      "the iterations of  418600 the loss is 0.2619303005597027 theta= [[-10.85733203]\n",
      " [  0.09223018]\n",
      " [  0.08604895]]\n",
      "the iterations of  418700 the loss is 0.2619163433254937 theta= [[-10.85851342]\n",
      " [  0.09223951]\n",
      " [  0.08605839]]\n",
      "the iterations of  418800 the loss is 0.2619023916983699 theta= [[-10.85969457]\n",
      " [  0.09224885]\n",
      " [  0.08606782]]\n",
      "the iterations of  418900 the loss is 0.26188844567494385 theta= [[-10.86087549]\n",
      " [  0.09225818]\n",
      " [  0.08607724]]\n",
      "the iterations of  419000 the loss is 0.2618745052518314 theta= [[-10.86205617]\n",
      " [  0.09226751]\n",
      " [  0.08608667]]\n",
      "the iterations of  419100 the loss is 0.2618605704256507 theta= [[-10.86323661]\n",
      " [  0.09227684]\n",
      " [  0.0860961 ]]\n",
      "the iterations of  419200 the loss is 0.26184664119302253 theta= [[-10.86441681]\n",
      " [  0.09228616]\n",
      " [  0.08610552]]\n",
      "the iterations of  419300 the loss is 0.2618327175505709 theta= [[-10.86559678]\n",
      " [  0.09229549]\n",
      " [  0.08611494]]\n",
      "the iterations of  419400 the loss is 0.261818799494922 theta= [[-10.86677651]\n",
      " [  0.09230481]\n",
      " [  0.08612436]]\n",
      "the iterations of  419500 the loss is 0.26180488702270543 theta= [[-10.867956  ]\n",
      " [  0.09231413]\n",
      " [  0.08613377]]\n",
      "the iterations of  419600 the loss is 0.26179098013055296 theta= [[-10.86913526]\n",
      " [  0.09232345]\n",
      " [  0.08614319]]\n",
      "the iterations of  419700 the loss is 0.26177707881509954 theta= [[-10.87031428]\n",
      " [  0.09233277]\n",
      " [  0.0861526 ]]\n",
      "the iterations of  419800 the loss is 0.2617631830729824 theta= [[-10.87149306]\n",
      " [  0.09234209]\n",
      " [  0.08616201]]\n",
      "the iterations of  419900 the loss is 0.2617492929008419 theta= [[-10.87267161]\n",
      " [  0.0923514 ]\n",
      " [  0.08617142]]\n",
      "the iterations of  420000 the loss is 0.261735408295321 theta= [[-10.87384992]\n",
      " [  0.09236071]\n",
      " [  0.08618083]]\n",
      "the iterations of  420100 the loss is 0.2617215292530653 theta= [[-10.87502799]\n",
      " [  0.09237002]\n",
      " [  0.08619024]]\n",
      "the iterations of  420200 the loss is 0.2617076557707234 theta= [[-10.87620583]\n",
      " [  0.09237933]\n",
      " [  0.08619964]]\n",
      "the iterations of  420300 the loss is 0.26169378784494624 theta= [[-10.87738344]\n",
      " [  0.09238864]\n",
      " [  0.08620904]]\n",
      "the iterations of  420400 the loss is 0.26167992547238783 theta= [[-10.8785608 ]\n",
      " [  0.09239794]\n",
      " [  0.08621845]]\n",
      "the iterations of  420500 the loss is 0.2616660686497049 theta= [[-10.87973794]\n",
      " [  0.09240724]\n",
      " [  0.08622784]]\n",
      "the iterations of  420600 the loss is 0.2616522173735566 theta= [[-10.88091483]\n",
      " [  0.09241654]\n",
      " [  0.08623724]]\n",
      "the iterations of  420700 the loss is 0.26163837164060505 theta= [[-10.88209149]\n",
      " [  0.09242584]\n",
      " [  0.08624664]]\n",
      "the iterations of  420800 the loss is 0.26162453144751496 theta= [[-10.88326792]\n",
      " [  0.09243514]\n",
      " [  0.08625603]]\n",
      "the iterations of  420900 the loss is 0.2616106967909536 theta= [[-10.88444411]\n",
      " [  0.09244444]\n",
      " [  0.08626542]]\n",
      "the iterations of  421000 the loss is 0.2615968676675914 theta= [[-10.88562006]\n",
      " [  0.09245373]\n",
      " [  0.08627481]]\n",
      "the iterations of  421100 the loss is 0.2615830440741014 theta= [[-10.88679578]\n",
      " [  0.09246302]\n",
      " [  0.0862842 ]]\n",
      "the iterations of  421200 the loss is 0.2615692260071587 theta= [[-10.88797127]\n",
      " [  0.09247231]\n",
      " [  0.08629358]]\n",
      "the iterations of  421300 the loss is 0.26155541346344185 theta= [[-10.88914652]\n",
      " [  0.0924816 ]\n",
      " [  0.08630297]]\n",
      "the iterations of  421400 the loss is 0.2615416064396318 theta= [[-10.89032153]\n",
      " [  0.09249089]\n",
      " [  0.08631235]]\n",
      "the iterations of  421500 the loss is 0.2615278049324124 theta= [[-10.89149631]\n",
      " [  0.09250017]\n",
      " [  0.08632173]]\n",
      "the iterations of  421600 the loss is 0.2615140089384699 theta= [[-10.89267086]\n",
      " [  0.09250945]\n",
      " [  0.08633111]]\n",
      "the iterations of  421700 the loss is 0.2615002184544931 theta= [[-10.89384517]\n",
      " [  0.09251873]\n",
      " [  0.08634048]]\n",
      "the iterations of  421800 the loss is 0.261486433477174 theta= [[-10.89501924]\n",
      " [  0.09252801]\n",
      " [  0.08634986]]\n",
      "the iterations of  421900 the loss is 0.26147265400320696 theta= [[-10.89619308]\n",
      " [  0.09253729]\n",
      " [  0.08635923]]\n",
      "the iterations of  422000 the loss is 0.26145888002928924 theta= [[-10.89736669]\n",
      " [  0.09254657]\n",
      " [  0.0863686 ]]\n",
      "the iterations of  422100 the loss is 0.2614451115521204 theta= [[-10.89854007]\n",
      " [  0.09255584]\n",
      " [  0.08637797]]\n",
      "the iterations of  422200 the loss is 0.2614313485684031 theta= [[-10.8997132 ]\n",
      " [  0.09256511]\n",
      " [  0.08638734]]\n",
      "the iterations of  422300 the loss is 0.26141759107484225 theta= [[-10.90088611]\n",
      " [  0.09257438]\n",
      " [  0.0863967 ]]\n",
      "the iterations of  422400 the loss is 0.2614038390681457 theta= [[-10.90205878]\n",
      " [  0.09258365]\n",
      " [  0.08640607]]\n",
      "the iterations of  422500 the loss is 0.2613900925450241 theta= [[-10.90323122]\n",
      " [  0.09259291]\n",
      " [  0.08641543]]\n",
      "the iterations of  422600 the loss is 0.26137635150219035 theta= [[-10.90440342]\n",
      " [  0.09260218]\n",
      " [  0.08642479]]\n",
      "the iterations of  422700 the loss is 0.26136261593636045 theta= [[-10.90557539]\n",
      " [  0.09261144]\n",
      " [  0.08643415]]\n",
      "the iterations of  422800 the loss is 0.2613488858442528 theta= [[-10.90674713]\n",
      " [  0.0926207 ]\n",
      " [  0.0864435 ]]\n",
      "the iterations of  422900 the loss is 0.2613351612225884 theta= [[-10.90791863]\n",
      " [  0.09262996]\n",
      " [  0.08645286]]\n",
      "the iterations of  423000 the loss is 0.2613214420680911 theta= [[-10.9090899 ]\n",
      " [  0.09263922]\n",
      " [  0.08646221]]\n",
      "the iterations of  423100 the loss is 0.26130772837748717 theta= [[-10.91026094]\n",
      " [  0.09264847]\n",
      " [  0.08647156]]\n",
      "the iterations of  423200 the loss is 0.2612940201475061 theta= [[-10.91143174]\n",
      " [  0.09265773]\n",
      " [  0.08648091]]\n",
      "the iterations of  423300 the loss is 0.2612803173748791 theta= [[-10.91260231]\n",
      " [  0.09266698]\n",
      " [  0.08649025]]\n",
      "the iterations of  423400 the loss is 0.2612666200563407 theta= [[-10.91377265]\n",
      " [  0.09267623]\n",
      " [  0.0864996 ]]\n",
      "the iterations of  423500 the loss is 0.261252928188628 theta= [[-10.91494275]\n",
      " [  0.09268548]\n",
      " [  0.08650894]]\n",
      "the iterations of  423600 the loss is 0.2612392417684806 theta= [[-10.91611262]\n",
      " [  0.09269472]\n",
      " [  0.08651828]]\n",
      "the iterations of  423700 the loss is 0.2612255607926407 theta= [[-10.91728226]\n",
      " [  0.09270397]\n",
      " [  0.08652762]]\n",
      "the iterations of  423800 the loss is 0.26121188525785316 theta= [[-10.91845167]\n",
      " [  0.09271321]\n",
      " [  0.08653696]]\n",
      "the iterations of  423900 the loss is 0.2611982151608657 theta= [[-10.91962084]\n",
      " [  0.09272245]\n",
      " [  0.0865463 ]]\n",
      "the iterations of  424000 the loss is 0.2611845504984285 theta= [[-10.92078978]\n",
      " [  0.09273169]\n",
      " [  0.08655563]]\n",
      "the iterations of  424100 the loss is 0.261170891267294 theta= [[-10.92195849]\n",
      " [  0.09274093]\n",
      " [  0.08656497]]\n",
      "the iterations of  424200 the loss is 0.2611572374642176 theta= [[-10.92312697]\n",
      " [  0.09275017]\n",
      " [  0.0865743 ]]\n",
      "the iterations of  424300 the loss is 0.26114358908595764 theta= [[-10.92429521]\n",
      " [  0.0927594 ]\n",
      " [  0.08658362]]\n",
      "the iterations of  424400 the loss is 0.2611299461292747 theta= [[-10.92546323]\n",
      " [  0.09276863]\n",
      " [  0.08659295]]\n",
      "the iterations of  424500 the loss is 0.2611163085909318 theta= [[-10.92663101]\n",
      " [  0.09277786]\n",
      " [  0.08660228]]\n",
      "the iterations of  424600 the loss is 0.261102676467695 theta= [[-10.92779855]\n",
      " [  0.09278709]\n",
      " [  0.0866116 ]]\n",
      "the iterations of  424700 the loss is 0.26108904975633285 theta= [[-10.92896587]\n",
      " [  0.09279632]\n",
      " [  0.08662092]]\n",
      "the iterations of  424800 the loss is 0.2610754284536161 theta= [[-10.93013296]\n",
      " [  0.09280554]\n",
      " [  0.08663024]]\n",
      "the iterations of  424900 the loss is 0.2610618125563187 theta= [[-10.93129981]\n",
      " [  0.09281476]\n",
      " [  0.08663956]]\n",
      "the iterations of  425000 the loss is 0.2610482020612168 theta= [[-10.93246643]\n",
      " [  0.09282399]\n",
      " [  0.08664888]]\n",
      "the iterations of  425100 the loss is 0.26103459696508946 theta= [[-10.93363282]\n",
      " [  0.0928332 ]\n",
      " [  0.08665819]]\n",
      "the iterations of  425200 the loss is 0.26102099726471784 theta= [[-10.93479898]\n",
      " [  0.09284242]\n",
      " [  0.0866675 ]]\n",
      "the iterations of  425300 the loss is 0.2610074029568862 theta= [[-10.93596491]\n",
      " [  0.09285164]\n",
      " [  0.08667681]]\n",
      "the iterations of  425400 the loss is 0.2609938140383811 theta= [[-10.93713061]\n",
      " [  0.09286085]\n",
      " [  0.08668612]]\n",
      "the iterations of  425500 the loss is 0.260980230505992 theta= [[-10.93829607]\n",
      " [  0.09287006]\n",
      " [  0.08669543]]\n",
      "the iterations of  425600 the loss is 0.26096665235651045 theta= [[-10.93946131]\n",
      " [  0.09287927]\n",
      " [  0.08670473]]\n",
      "the iterations of  425700 the loss is 0.260953079586731 theta= [[-10.94062631]\n",
      " [  0.09288848]\n",
      " [  0.08671404]]\n",
      "the iterations of  425800 the loss is 0.2609395121934507 theta= [[-10.94179108]\n",
      " [  0.09289769]\n",
      " [  0.08672334]]\n",
      "the iterations of  425900 the loss is 0.26092595017346887 theta= [[-10.94295563]\n",
      " [  0.09290689]\n",
      " [  0.08673264]]\n",
      "the iterations of  426000 the loss is 0.26091239352358775 theta= [[-10.94411994]\n",
      " [  0.0929161 ]\n",
      " [  0.08674194]]\n",
      "the iterations of  426100 the loss is 0.26089884224061227 theta= [[-10.94528402]\n",
      " [  0.0929253 ]\n",
      " [  0.08675123]]\n",
      "the iterations of  426200 the loss is 0.2608852963213494 theta= [[-10.94644787]\n",
      " [  0.0929345 ]\n",
      " [  0.08676053]]\n",
      "the iterations of  426300 the loss is 0.2608717557626091 theta= [[-10.94761149]\n",
      " [  0.0929437 ]\n",
      " [  0.08676982]]\n",
      "the iterations of  426400 the loss is 0.26085822056120384 theta= [[-10.94877488]\n",
      " [  0.09295289]\n",
      " [  0.08677911]]\n",
      "the iterations of  426500 the loss is 0.2608446907139485 theta= [[-10.94993804]\n",
      " [  0.09296209]\n",
      " [  0.0867884 ]]\n",
      "the iterations of  426600 the loss is 0.26083116621766067 theta= [[-10.95110097]\n",
      " [  0.09297128]\n",
      " [  0.08679769]]\n",
      "the iterations of  426700 the loss is 0.2608176470691604 theta= [[-10.95226367]\n",
      " [  0.09298047]\n",
      " [  0.08680697]]\n",
      "the iterations of  426800 the loss is 0.26080413326527013 theta= [[-10.95342614]\n",
      " [  0.09298966]\n",
      " [  0.08681625]]\n",
      "the iterations of  426900 the loss is 0.26079062480281545 theta= [[-10.95458838]\n",
      " [  0.09299885]\n",
      " [  0.08682554]]\n",
      "the iterations of  427000 the loss is 0.2607771216786238 theta= [[-10.9557504 ]\n",
      " [  0.09300803]\n",
      " [  0.08683482]]\n",
      "the iterations of  427100 the loss is 0.26076362388952545 theta= [[-10.95691218]\n",
      " [  0.09301722]\n",
      " [  0.08684409]]\n",
      "the iterations of  427200 the loss is 0.2607501314323533 theta= [[-10.95807373]\n",
      " [  0.0930264 ]\n",
      " [  0.08685337]]\n",
      "the iterations of  427300 the loss is 0.2607366443039428 theta= [[-10.95923505]\n",
      " [  0.09303558]\n",
      " [  0.08686264]]\n",
      "the iterations of  427400 the loss is 0.26072316250113176 theta= [[-10.96039615]\n",
      " [  0.09304476]\n",
      " [  0.08687192]]\n",
      "the iterations of  427500 the loss is 0.2607096860207605 theta= [[-10.96155701]\n",
      " [  0.09305393]\n",
      " [  0.08688119]]\n",
      "the iterations of  427600 the loss is 0.26069621485967215 theta= [[-10.96271764]\n",
      " [  0.09306311]\n",
      " [  0.08689046]]\n",
      "the iterations of  427700 the loss is 0.2606827490147121 theta= [[-10.96387805]\n",
      " [  0.09307228]\n",
      " [  0.08689972]]\n",
      "the iterations of  427800 the loss is 0.2606692884827286 theta= [[-10.96503823]\n",
      " [  0.09308145]\n",
      " [  0.08690899]]\n",
      "the iterations of  427900 the loss is 0.2606558332605719 theta= [[-10.96619818]\n",
      " [  0.09309062]\n",
      " [  0.08691825]]\n",
      "the iterations of  428000 the loss is 0.26064238334509526 theta= [[-10.96735789]\n",
      " [  0.09309979]\n",
      " [  0.08692752]]\n",
      "the iterations of  428100 the loss is 0.2606289387331543 theta= [[-10.96851739]\n",
      " [  0.09310895]\n",
      " [  0.08693678]]\n",
      "the iterations of  428200 the loss is 0.26061549942160706 theta= [[-10.96967665]\n",
      " [  0.09311812]\n",
      " [  0.08694603]]\n",
      "the iterations of  428300 the loss is 0.2606020654073141 theta= [[-10.97083568]\n",
      " [  0.09312728]\n",
      " [  0.08695529]]\n",
      "the iterations of  428400 the loss is 0.2605886366871386 theta= [[-10.97199449]\n",
      " [  0.09313644]\n",
      " [  0.08696454]]\n",
      "the iterations of  428500 the loss is 0.2605752132579463 theta= [[-10.97315306]\n",
      " [  0.0931456 ]\n",
      " [  0.0869738 ]]\n",
      "the iterations of  428600 the loss is 0.26056179511660527 theta= [[-10.97431141]\n",
      " [  0.09315476]\n",
      " [  0.08698305]]\n",
      "the iterations of  428700 the loss is 0.26054838225998633 theta= [[-10.97546953]\n",
      " [  0.09316391]\n",
      " [  0.0869923 ]]\n",
      "the iterations of  428800 the loss is 0.2605349746849625 theta= [[-10.97662742]\n",
      " [  0.09317306]\n",
      " [  0.08700155]]\n",
      "the iterations of  428900 the loss is 0.2605215723884094 theta= [[-10.97778509]\n",
      " [  0.09318222]\n",
      " [  0.08701079]]\n",
      "the iterations of  429000 the loss is 0.2605081753672052 theta= [[-10.97894253]\n",
      " [  0.09319137]\n",
      " [  0.08702004]]\n",
      "the iterations of  429100 the loss is 0.2604947836182306 theta= [[-10.98009973]\n",
      " [  0.09320051]\n",
      " [  0.08702928]]\n",
      "the iterations of  429200 the loss is 0.26048139713836865 theta= [[-10.98125672]\n",
      " [  0.09320966]\n",
      " [  0.08703852]]\n",
      "the iterations of  429300 the loss is 0.26046801592450514 theta= [[-10.98241347]\n",
      " [  0.09321881]\n",
      " [  0.08704776]]\n",
      "the iterations of  429400 the loss is 0.26045463997352813 theta= [[-10.98357   ]\n",
      " [  0.09322795]\n",
      " [  0.08705699]]\n",
      "the iterations of  429500 the loss is 0.26044126928232825 theta= [[-10.98472629]\n",
      " [  0.09323709]\n",
      " [  0.08706623]]\n",
      "the iterations of  429600 the loss is 0.2604279038477987 theta= [[-10.98588237]\n",
      " [  0.09324623]\n",
      " [  0.08707546]]\n",
      "the iterations of  429700 the loss is 0.2604145436668348 theta= [[-10.98703821]\n",
      " [  0.09325537]\n",
      " [  0.08708469]]\n",
      "the iterations of  429800 the loss is 0.26040118873633483 theta= [[-10.98819383]\n",
      " [  0.0932645 ]\n",
      " [  0.08709392]]\n",
      "the iterations of  429900 the loss is 0.2603878390531991 theta= [[-10.98934922]\n",
      " [  0.09327364]\n",
      " [  0.08710315]]\n",
      "the iterations of  430000 the loss is 0.26037449461433076 theta= [[-10.99050438]\n",
      " [  0.09328277]\n",
      " [  0.08711238]]\n",
      "the iterations of  430100 the loss is 0.26036115541663507 theta= [[-10.99165932]\n",
      " [  0.0932919 ]\n",
      " [  0.0871216 ]]\n",
      "the iterations of  430200 the loss is 0.26034782145702035 theta= [[-10.99281403]\n",
      " [  0.09330103]\n",
      " [  0.08713082]]\n",
      "the iterations of  430300 the loss is 0.26033449273239667 theta= [[-10.99396851]\n",
      " [  0.09331015]\n",
      " [  0.08714004]]\n",
      "the iterations of  430400 the loss is 0.260321169239677 theta= [[-10.99512276]\n",
      " [  0.09331928]\n",
      " [  0.08714926]]\n",
      "the iterations of  430500 the loss is 0.26030785097577686 theta= [[-10.99627679]\n",
      " [  0.0933284 ]\n",
      " [  0.08715848]]\n",
      "the iterations of  430600 the loss is 0.2602945379376134 theta= [[-10.9974306 ]\n",
      " [  0.09333752]\n",
      " [  0.0871677 ]]\n",
      "the iterations of  430700 the loss is 0.26028123012210747 theta= [[-10.99858417]\n",
      " [  0.09334664]\n",
      " [  0.08717691]]\n",
      "the iterations of  430800 the loss is 0.26026792752618155 theta= [[-10.99973752]\n",
      " [  0.09335576]\n",
      " [  0.08718612]]\n",
      "the iterations of  430900 the loss is 0.26025463014676076 theta= [[-11.00089065]\n",
      " [  0.09336488]\n",
      " [  0.08719533]]\n",
      "the iterations of  431000 the loss is 0.2602413379807728 theta= [[-11.00204355]\n",
      " [  0.09337399]\n",
      " [  0.08720454]]\n",
      "the iterations of  431100 the loss is 0.2602280510251475 theta= [[-11.00319622]\n",
      " [  0.09338311]\n",
      " [  0.08721375]]\n",
      "the iterations of  431200 the loss is 0.2602147692768175 theta= [[-11.00434866]\n",
      " [  0.09339222]\n",
      " [  0.08722295]]\n",
      "the iterations of  431300 the loss is 0.2602014927327177 theta= [[-11.00550089]\n",
      " [  0.09340133]\n",
      " [  0.08723215]]\n",
      "the iterations of  431400 the loss is 0.26018822138978503 theta= [[-11.00665288]\n",
      " [  0.09341043]\n",
      " [  0.08724136]]\n",
      "the iterations of  431500 the loss is 0.2601749552449599 theta= [[-11.00780465]\n",
      " [  0.09341954]\n",
      " [  0.08725055]]\n",
      "the iterations of  431600 the loss is 0.26016169429518415 theta= [[-11.00895619]\n",
      " [  0.09342864]\n",
      " [  0.08725975]]\n",
      "the iterations of  431700 the loss is 0.2601484385374028 theta= [[-11.01010751]\n",
      " [  0.09343775]\n",
      " [  0.08726895]]\n",
      "the iterations of  431800 the loss is 0.26013518796856255 theta= [[-11.0112586 ]\n",
      " [  0.09344685]\n",
      " [  0.08727814]]\n",
      "the iterations of  431900 the loss is 0.2601219425856129 theta= [[-11.01240947]\n",
      " [  0.09345594]\n",
      " [  0.08728733]]\n",
      "the iterations of  432000 the loss is 0.2601087023855059 theta= [[-11.01356011]\n",
      " [  0.09346504]\n",
      " [  0.08729653]]\n",
      "the iterations of  432100 the loss is 0.2600954673651959 theta= [[-11.01471053]\n",
      " [  0.09347414]\n",
      " [  0.08730571]]\n",
      "the iterations of  432200 the loss is 0.26008223752163956 theta= [[-11.01586072]\n",
      " [  0.09348323]\n",
      " [  0.0873149 ]]\n",
      "the iterations of  432300 the loss is 0.2600690128517962 theta= [[-11.01701069]\n",
      " [  0.09349232]\n",
      " [  0.08732409]]\n",
      "the iterations of  432400 the loss is 0.2600557933526275 theta= [[-11.01816043]\n",
      " [  0.09350141]\n",
      " [  0.08733327]]\n",
      "the iterations of  432500 the loss is 0.2600425790210971 theta= [[-11.01930995]\n",
      " [  0.0935105 ]\n",
      " [  0.08734245]]\n",
      "the iterations of  432600 the loss is 0.2600293698541718 theta= [[-11.02045924]\n",
      " [  0.09351959]\n",
      " [  0.08735163]]\n",
      "the iterations of  432700 the loss is 0.2600161658488201 theta= [[-11.02160831]\n",
      " [  0.09352867]\n",
      " [  0.08736081]]\n",
      "the iterations of  432800 the loss is 0.2600029670020134 theta= [[-11.02275716]\n",
      " [  0.09353776]\n",
      " [  0.08736999]]\n",
      "the iterations of  432900 the loss is 0.25998977331072537 theta= [[-11.02390578]\n",
      " [  0.09354684]\n",
      " [  0.08737916]]\n",
      "the iterations of  433000 the loss is 0.259976584771932 theta= [[-11.02505417]\n",
      " [  0.09355592]\n",
      " [  0.08738833]]\n",
      "the iterations of  433100 the loss is 0.25996340138261154 theta= [[-11.02620234]\n",
      " [  0.09356499]\n",
      " [  0.08739751]]\n",
      "the iterations of  433200 the loss is 0.25995022313974503 theta= [[-11.02735029]\n",
      " [  0.09357407]\n",
      " [  0.08740667]]\n",
      "the iterations of  433300 the loss is 0.2599370500403156 theta= [[-11.02849801]\n",
      " [  0.09358315]\n",
      " [  0.08741584]]\n",
      "the iterations of  433400 the loss is 0.25992388208130873 theta= [[-11.02964551]\n",
      " [  0.09359222]\n",
      " [  0.08742501]]\n",
      "the iterations of  433500 the loss is 0.2599107192597124 theta= [[-11.03079279]\n",
      " [  0.09360129]\n",
      " [  0.08743417]]\n",
      "the iterations of  433600 the loss is 0.2598975615725172 theta= [[-11.03193984]\n",
      " [  0.09361036]\n",
      " [  0.08744333]]\n",
      "the iterations of  433700 the loss is 0.2598844090167157 theta= [[-11.03308666]\n",
      " [  0.09361943]\n",
      " [  0.0874525 ]]\n",
      "the iterations of  433800 the loss is 0.2598712615893032 theta= [[-11.03423327]\n",
      " [  0.09362849]\n",
      " [  0.08746165]]\n",
      "the iterations of  433900 the loss is 0.25985811928727703 theta= [[-11.03537965]\n",
      " [  0.09363755]\n",
      " [  0.08747081]]\n",
      "the iterations of  434000 the loss is 0.25984498210763723 theta= [[-11.03652581]\n",
      " [  0.09364662]\n",
      " [  0.08747997]]\n",
      "the iterations of  434100 the loss is 0.25983185004738585 theta= [[-11.03767174]\n",
      " [  0.09365568]\n",
      " [  0.08748912]]\n",
      "the iterations of  434200 the loss is 0.2598187231035277 theta= [[-11.03881745]\n",
      " [  0.09366474]\n",
      " [  0.08749827]]\n",
      "the iterations of  434300 the loss is 0.25980560127306973 theta= [[-11.03996294]\n",
      " [  0.09367379]\n",
      " [  0.08750742]]\n",
      "the iterations of  434400 the loss is 0.2597924845530212 theta= [[-11.0411082 ]\n",
      " [  0.09368285]\n",
      " [  0.08751657]]\n",
      "the iterations of  434500 the loss is 0.25977937294039394 theta= [[-11.04225324]\n",
      " [  0.0936919 ]\n",
      " [  0.08752572]]\n",
      "the iterations of  434600 the loss is 0.259766266432202 theta= [[-11.04339806]\n",
      " [  0.09370095]\n",
      " [  0.08753486]]\n",
      "the iterations of  434700 the loss is 0.25975316502546186 theta= [[-11.04454266]\n",
      " [  0.09371   ]\n",
      " [  0.08754401]]\n",
      "the iterations of  434800 the loss is 0.25974006871719224 theta= [[-11.04568703]\n",
      " [  0.09371905]\n",
      " [  0.08755315]]\n",
      "the iterations of  434900 the loss is 0.25972697750441454 theta= [[-11.04683118]\n",
      " [  0.0937281 ]\n",
      " [  0.08756229]]\n",
      "the iterations of  435000 the loss is 0.2597138913841517 theta= [[-11.04797511]\n",
      " [  0.09373714]\n",
      " [  0.08757142]]\n",
      "the iterations of  435100 the loss is 0.25970081035343023 theta= [[-11.04911881]\n",
      " [  0.09374619]\n",
      " [  0.08758056]]\n",
      "the iterations of  435200 the loss is 0.25968773440927806 theta= [[-11.05026229]\n",
      " [  0.09375523]\n",
      " [  0.08758969]]\n",
      "the iterations of  435300 the loss is 0.25967466354872565 theta= [[-11.05140555]\n",
      " [  0.09376427]\n",
      " [  0.08759883]]\n",
      "the iterations of  435400 the loss is 0.25966159776880604 theta= [[-11.05254859]\n",
      " [  0.09377331]\n",
      " [  0.08760796]]\n",
      "the iterations of  435500 the loss is 0.25964853706655433 theta= [[-11.05369141]\n",
      " [  0.09378234]\n",
      " [  0.08761709]]\n",
      "the iterations of  435600 the loss is 0.2596354814390082 theta= [[-11.054834  ]\n",
      " [  0.09379138]\n",
      " [  0.08762621]]\n",
      "the iterations of  435700 the loss is 0.25962243088320763 theta= [[-11.05597637]\n",
      " [  0.09380041]\n",
      " [  0.08763534]]\n",
      "the iterations of  435800 the loss is 0.2596093853961946 theta= [[-11.05711852]\n",
      " [  0.09380944]\n",
      " [  0.08764446]]\n",
      "the iterations of  435900 the loss is 0.2595963449750139 theta= [[-11.05826045]\n",
      " [  0.09381847]\n",
      " [  0.08765359]]\n",
      "the iterations of  436000 the loss is 0.25958330961671217 theta= [[-11.05940216]\n",
      " [  0.0938275 ]\n",
      " [  0.08766271]]\n",
      "the iterations of  436100 the loss is 0.25957027931833876 theta= [[-11.06054364]\n",
      " [  0.09383652]\n",
      " [  0.08767183]]\n",
      "the iterations of  436200 the loss is 0.25955725407694535 theta= [[-11.06168491]\n",
      " [  0.09384555]\n",
      " [  0.08768094]]\n",
      "the iterations of  436300 the loss is 0.2595442338895857 theta= [[-11.06282595]\n",
      " [  0.09385457]\n",
      " [  0.08769006]]\n",
      "the iterations of  436400 the loss is 0.25953121875331603 theta= [[-11.06396677]\n",
      " [  0.09386359]\n",
      " [  0.08769917]]\n",
      "the iterations of  436500 the loss is 0.2595182086651948 theta= [[-11.06510737]\n",
      " [  0.09387261]\n",
      " [  0.08770828]]\n",
      "the iterations of  436600 the loss is 0.2595052036222829 theta= [[-11.06624775]\n",
      " [  0.09388163]\n",
      " [  0.08771739]]\n",
      "the iterations of  436700 the loss is 0.2594922036216434 theta= [[-11.0673879 ]\n",
      " [  0.09389064]\n",
      " [  0.0877265 ]]\n",
      "the iterations of  436800 the loss is 0.2594792086603418 theta= [[-11.06852784]\n",
      " [  0.09389966]\n",
      " [  0.08773561]]\n",
      "the iterations of  436900 the loss is 0.25946621873544584 theta= [[-11.06966755]\n",
      " [  0.09390867]\n",
      " [  0.08774471]]\n",
      "the iterations of  437000 the loss is 0.2594532338440255 theta= [[-11.07080705]\n",
      " [  0.09391768]\n",
      " [  0.08775381]]\n",
      "the iterations of  437100 the loss is 0.2594402539831533 theta= [[-11.07194632]\n",
      " [  0.09392669]\n",
      " [  0.08776292]]\n",
      "the iterations of  437200 the loss is 0.25942727914990366 theta= [[-11.07308538]\n",
      " [  0.0939357 ]\n",
      " [  0.08777202]]\n",
      "the iterations of  437300 the loss is 0.2594143093413539 theta= [[-11.07422421]\n",
      " [  0.0939447 ]\n",
      " [  0.08778111]]\n",
      "the iterations of  437400 the loss is 0.2594013445545828 theta= [[-11.07536282]\n",
      " [  0.0939537 ]\n",
      " [  0.08779021]]\n",
      "the iterations of  437500 the loss is 0.2593883847866722 theta= [[-11.07650121]\n",
      " [  0.09396271]\n",
      " [  0.0877993 ]]\n",
      "the iterations of  437600 the loss is 0.25937543003470603 theta= [[-11.07763938]\n",
      " [  0.09397171]\n",
      " [  0.0878084 ]]\n",
      "the iterations of  437700 the loss is 0.25936248029577014 theta= [[-11.07877733]\n",
      " [  0.0939807 ]\n",
      " [  0.08781749]]\n",
      "the iterations of  437800 the loss is 0.25934953556695306 theta= [[-11.07991506]\n",
      " [  0.0939897 ]\n",
      " [  0.08782658]]\n",
      "the iterations of  437900 the loss is 0.2593365958453457 theta= [[-11.08105257]\n",
      " [  0.0939987 ]\n",
      " [  0.08783566]]\n",
      "the iterations of  438000 the loss is 0.2593236611280406 theta= [[-11.08218986]\n",
      " [  0.09400769]\n",
      " [  0.08784475]]\n",
      "the iterations of  438100 the loss is 0.25931073141213346 theta= [[-11.08332694]\n",
      " [  0.09401668]\n",
      " [  0.08785383]]\n",
      "the iterations of  438200 the loss is 0.25929780669472163 theta= [[-11.08446379]\n",
      " [  0.09402567]\n",
      " [  0.08786292]]\n",
      "the iterations of  438300 the loss is 0.259284886972905 theta= [[-11.08560042]\n",
      " [  0.09403466]\n",
      " [  0.087872  ]]\n",
      "the iterations of  438400 the loss is 0.25927197224378573 theta= [[-11.08673683]\n",
      " [  0.09404365]\n",
      " [  0.08788108]]\n",
      "the iterations of  438500 the loss is 0.25925906250446784 theta= [[-11.08787302]\n",
      " [  0.09405263]\n",
      " [  0.08789015]]\n",
      "the iterations of  438600 the loss is 0.2592461577520584 theta= [[-11.08900899]\n",
      " [  0.09406161]\n",
      " [  0.08789923]]\n",
      "the iterations of  438700 the loss is 0.2592332579836659 theta= [[-11.09014475]\n",
      " [  0.0940706 ]\n",
      " [  0.0879083 ]]\n",
      "the iterations of  438800 the loss is 0.25922036319640185 theta= [[-11.09128028]\n",
      " [  0.09407958]\n",
      " [  0.08791737]]\n",
      "the iterations of  438900 the loss is 0.25920747338737965 theta= [[-11.09241559]\n",
      " [  0.09408855]\n",
      " [  0.08792644]]\n",
      "the iterations of  439000 the loss is 0.2591945885537148 theta= [[-11.09355069]\n",
      " [  0.09409753]\n",
      " [  0.08793551]]\n",
      "the iterations of  439100 the loss is 0.25918170869252544 theta= [[-11.09468557]\n",
      " [  0.0941065 ]\n",
      " [  0.08794458]]\n",
      "the iterations of  439200 the loss is 0.2591688338009317 theta= [[-11.09582022]\n",
      " [  0.09411548]\n",
      " [  0.08795364]]\n",
      "the iterations of  439300 the loss is 0.259155963876056 theta= [[-11.09695466]\n",
      " [  0.09412445]\n",
      " [  0.08796271]]\n",
      "the iterations of  439400 the loss is 0.25914309891502313 theta= [[-11.09808888]\n",
      " [  0.09413342]\n",
      " [  0.08797177]]\n",
      "the iterations of  439500 the loss is 0.25913023891495984 theta= [[-11.09922288]\n",
      " [  0.09414239]\n",
      " [  0.08798083]]\n",
      "the iterations of  439600 the loss is 0.2591173838729956 theta= [[-11.10035667]\n",
      " [  0.09415135]\n",
      " [  0.08798989]]\n",
      "the iterations of  439700 the loss is 0.25910453378626186 theta= [[-11.10149023]\n",
      " [  0.09416032]\n",
      " [  0.08799894]]\n",
      "the iterations of  439800 the loss is 0.259091688651892 theta= [[-11.10262357]\n",
      " [  0.09416928]\n",
      " [  0.088008  ]]\n",
      "the iterations of  439900 the loss is 0.25907884846702217 theta= [[-11.1037567 ]\n",
      " [  0.09417824]\n",
      " [  0.08801705]]\n",
      "the iterations of  440000 the loss is 0.25906601322879075 theta= [[-11.10488961]\n",
      " [  0.0941872 ]\n",
      " [  0.0880261 ]]\n",
      "the iterations of  440100 the loss is 0.2590531829343378 theta= [[-11.1060223 ]\n",
      " [  0.09419616]\n",
      " [  0.08803515]]\n",
      "the iterations of  440200 the loss is 0.2590403575808062 theta= [[-11.10715477]\n",
      " [  0.09420511]\n",
      " [  0.0880442 ]]\n",
      "the iterations of  440300 the loss is 0.25902753716534066 theta= [[-11.10828703]\n",
      " [  0.09421407]\n",
      " [  0.08805325]]\n",
      "the iterations of  440400 the loss is 0.25901472168508843 theta= [[-11.10941906]\n",
      " [  0.09422302]\n",
      " [  0.08806229]]\n",
      "the iterations of  440500 the loss is 0.2590019111371986 theta= [[-11.11055088]\n",
      " [  0.09423197]\n",
      " [  0.08807133]]\n",
      "the iterations of  440600 the loss is 0.25898910551882315 theta= [[-11.11168248]\n",
      " [  0.09424092]\n",
      " [  0.08808037]]\n",
      "the iterations of  440700 the loss is 0.25897630482711553 theta= [[-11.11281387]\n",
      " [  0.09424987]\n",
      " [  0.08808941]]\n",
      "the iterations of  440800 the loss is 0.25896350905923177 theta= [[-11.11394503]\n",
      " [  0.09425881]\n",
      " [  0.08809845]]\n",
      "the iterations of  440900 the loss is 0.25895071821233 theta= [[-11.11507598]\n",
      " [  0.09426776]\n",
      " [  0.08810749]]\n",
      "the iterations of  441000 the loss is 0.25893793228357087 theta= [[-11.11620671]\n",
      " [  0.0942767 ]\n",
      " [  0.08811652]]\n",
      "the iterations of  441100 the loss is 0.25892515127011695 theta= [[-11.11733722]\n",
      " [  0.09428564]\n",
      " [  0.08812555]]\n",
      "the iterations of  441200 the loss is 0.25891237516913324 theta= [[-11.11846752]\n",
      " [  0.09429458]\n",
      " [  0.08813458]]\n",
      "the iterations of  441300 the loss is 0.2588996039777864 theta= [[-11.1195976 ]\n",
      " [  0.09430351]\n",
      " [  0.08814361]]\n",
      "the iterations of  441400 the loss is 0.25888683769324616 theta= [[-11.12072746]\n",
      " [  0.09431245]\n",
      " [  0.08815264]]\n",
      "the iterations of  441500 the loss is 0.25887407631268394 theta= [[-11.1218571 ]\n",
      " [  0.09432138]\n",
      " [  0.08816167]]\n",
      "the iterations of  441600 the loss is 0.25886131983327304 theta= [[-11.12298653]\n",
      " [  0.09433032]\n",
      " [  0.08817069]]\n",
      "the iterations of  441700 the loss is 0.25884856825218966 theta= [[-11.12411574]\n",
      " [  0.09433925]\n",
      " [  0.08817971]]\n",
      "the iterations of  441800 the loss is 0.25883582156661183 theta= [[-11.12524473]\n",
      " [  0.09434818]\n",
      " [  0.08818873]]\n",
      "the iterations of  441900 the loss is 0.2588230797737199 theta= [[-11.12637351]\n",
      " [  0.0943571 ]\n",
      " [  0.08819775]]\n",
      "the iterations of  442000 the loss is 0.2588103428706964 theta= [[-11.12750207]\n",
      " [  0.09436603]\n",
      " [  0.08820677]]\n",
      "the iterations of  442100 the loss is 0.25879761085472575 theta= [[-11.12863042]\n",
      " [  0.09437495]\n",
      " [  0.08821579]]\n",
      "the iterations of  442200 the loss is 0.25878488372299513 theta= [[-11.12975854]\n",
      " [  0.09438387]\n",
      " [  0.0882248 ]]\n",
      "the iterations of  442300 the loss is 0.2587721614726935 theta= [[-11.13088645]\n",
      " [  0.09439279]\n",
      " [  0.08823381]]\n",
      "the iterations of  442400 the loss is 0.25875944410101187 theta= [[-11.13201415]\n",
      " [  0.09440171]\n",
      " [  0.08824282]]\n",
      "the iterations of  442500 the loss is 0.258746731605144 theta= [[-11.13314163]\n",
      " [  0.09441063]\n",
      " [  0.08825183]]\n",
      "the iterations of  442600 the loss is 0.25873402398228523 theta= [[-11.13426889]\n",
      " [  0.09441955]\n",
      " [  0.08826084]]\n",
      "the iterations of  442700 the loss is 0.2587213212296333 theta= [[-11.13539594]\n",
      " [  0.09442846]\n",
      " [  0.08826984]]\n",
      "the iterations of  442800 the loss is 0.2587086233443884 theta= [[-11.13652277]\n",
      " [  0.09443737]\n",
      " [  0.08827885]]\n",
      "the iterations of  442900 the loss is 0.2586959303237526 theta= [[-11.13764938]\n",
      " [  0.09444628]\n",
      " [  0.08828785]]\n",
      "the iterations of  443000 the loss is 0.2586832421649301 theta= [[-11.13877578]\n",
      " [  0.09445519]\n",
      " [  0.08829685]]\n",
      "the iterations of  443100 the loss is 0.25867055886512746 theta= [[-11.13990196]\n",
      " [  0.0944641 ]\n",
      " [  0.08830585]]\n",
      "the iterations of  443200 the loss is 0.25865788042155347 theta= [[-11.14102793]\n",
      " [  0.094473  ]\n",
      " [  0.08831484]]\n",
      "the iterations of  443300 the loss is 0.2586452068314187 theta= [[-11.14215368]\n",
      " [  0.09448191]\n",
      " [  0.08832384]]\n",
      "the iterations of  443400 the loss is 0.2586325380919362 theta= [[-11.14327922]\n",
      " [  0.09449081]\n",
      " [  0.08833283]]\n",
      "the iterations of  443500 the loss is 0.2586198742003212 theta= [[-11.14440454]\n",
      " [  0.09449971]\n",
      " [  0.08834182]]\n",
      "the iterations of  443600 the loss is 0.2586072151537909 theta= [[-11.14552965]\n",
      " [  0.09450861]\n",
      " [  0.08835081]]\n",
      "the iterations of  443700 the loss is 0.2585945609495649 theta= [[-11.14665454]\n",
      " [  0.0945175 ]\n",
      " [  0.0883598 ]]\n",
      "the iterations of  443800 the loss is 0.25858191158486454 theta= [[-11.14777921]\n",
      " [  0.0945264 ]\n",
      " [  0.08836879]]\n",
      "the iterations of  443900 the loss is 0.25856926705691374 theta= [[-11.14890367]\n",
      " [  0.09453529]\n",
      " [  0.08837777]]\n",
      "the iterations of  444000 the loss is 0.25855662736293855 theta= [[-11.15002792]\n",
      " [  0.09454418]\n",
      " [  0.08838676]]\n",
      "the iterations of  444100 the loss is 0.258543992500167 theta= [[-11.15115195]\n",
      " [  0.09455307]\n",
      " [  0.08839574]]\n",
      "the iterations of  444200 the loss is 0.2585313624658295 theta= [[-11.15227577]\n",
      " [  0.09456196]\n",
      " [  0.08840472]]\n",
      "the iterations of  444300 the loss is 0.258518737257158 theta= [[-11.15339937]\n",
      " [  0.09457085]\n",
      " [  0.0884137 ]]\n",
      "the iterations of  444400 the loss is 0.25850611687138725 theta= [[-11.15452275]\n",
      " [  0.09457973]\n",
      " [  0.08842267]]\n",
      "the iterations of  444500 the loss is 0.25849350130575377 theta= [[-11.15564593]\n",
      " [  0.09458862]\n",
      " [  0.08843165]]\n",
      "the iterations of  444600 the loss is 0.2584808905574965 theta= [[-11.15676888]\n",
      " [  0.0945975 ]\n",
      " [  0.08844062]]\n",
      "the iterations of  444700 the loss is 0.2584682846238563 theta= [[-11.15789163]\n",
      " [  0.09460638]\n",
      " [  0.08844959]]\n",
      "the iterations of  444800 the loss is 0.25845568350207637 theta= [[-11.15901415]\n",
      " [  0.09461526]\n",
      " [  0.08845856]]\n",
      "the iterations of  444900 the loss is 0.2584430871894017 theta= [[-11.16013647]\n",
      " [  0.09462414]\n",
      " [  0.08846753]]\n",
      "the iterations of  445000 the loss is 0.25843049568307963 theta= [[-11.16125857]\n",
      " [  0.09463301]\n",
      " [  0.0884765 ]]\n",
      "the iterations of  445100 the loss is 0.25841790898035993 theta= [[-11.16238046]\n",
      " [  0.09464188]\n",
      " [  0.08848546]]\n",
      "the iterations of  445200 the loss is 0.25840532707849395 theta= [[-11.16350213]\n",
      " [  0.09465076]\n",
      " [  0.08849442]]\n",
      "the iterations of  445300 the loss is 0.2583927499747353 theta= [[-11.16462359]\n",
      " [  0.09465963]\n",
      " [  0.08850338]]\n",
      "the iterations of  445400 the loss is 0.2583801776663399 theta= [[-11.16574483]\n",
      " [  0.0946685 ]\n",
      " [  0.08851234]]\n",
      "the iterations of  445500 the loss is 0.2583676101505658 theta= [[-11.16686586]\n",
      " [  0.09467736]\n",
      " [  0.0885213 ]]\n",
      "the iterations of  445600 the loss is 0.25835504742467313 theta= [[-11.16798668]\n",
      " [  0.09468623]\n",
      " [  0.08853026]]\n",
      "the iterations of  445700 the loss is 0.258342489485924 theta= [[-11.16910728]\n",
      " [  0.09469509]\n",
      " [  0.08853921]]\n",
      "the iterations of  445800 the loss is 0.2583299363315827 theta= [[-11.17022767]\n",
      " [  0.09470395]\n",
      " [  0.08854817]]\n",
      "the iterations of  445900 the loss is 0.25831738795891557 theta= [[-11.17134785]\n",
      " [  0.09471281]\n",
      " [  0.08855712]]\n",
      "the iterations of  446000 the loss is 0.2583048443651914 theta= [[-11.17246781]\n",
      " [  0.09472167]\n",
      " [  0.08856607]]\n",
      "the iterations of  446100 the loss is 0.2582923055476805 theta= [[-11.17358756]\n",
      " [  0.09473053]\n",
      " [  0.08857502]]\n",
      "the iterations of  446200 the loss is 0.2582797715036557 theta= [[-11.1747071 ]\n",
      " [  0.09473938]\n",
      " [  0.08858396]]\n",
      "the iterations of  446300 the loss is 0.2582672422303921 theta= [[-11.17582642]\n",
      " [  0.09474824]\n",
      " [  0.08859291]]\n",
      "the iterations of  446400 the loss is 0.25825471772516634 theta= [[-11.17694554]\n",
      " [  0.09475709]\n",
      " [  0.08860185]]\n",
      "the iterations of  446500 the loss is 0.25824219798525777 theta= [[-11.17806443]\n",
      " [  0.09476594]\n",
      " [  0.08861079]]\n",
      "the iterations of  446600 the loss is 0.2582296830079473 theta= [[-11.17918312]\n",
      " [  0.09477479]\n",
      " [  0.08861973]]\n",
      "the iterations of  446700 the loss is 0.2582171727905182 theta= [[-11.18030159]\n",
      " [  0.09478364]\n",
      " [  0.08862867]]\n",
      "the iterations of  446800 the loss is 0.258204667330256 theta= [[-11.18141985]\n",
      " [  0.09479248]\n",
      " [  0.0886376 ]]\n",
      "the iterations of  446900 the loss is 0.2581921666244478 theta= [[-11.1825379 ]\n",
      " [  0.09480132]\n",
      " [  0.08864654]]\n",
      "the iterations of  447000 the loss is 0.25817967067038344 theta= [[-11.18365573]\n",
      " [  0.09481017]\n",
      " [  0.08865547]]\n",
      "the iterations of  447100 the loss is 0.2581671794653545 theta= [[-11.18477335]\n",
      " [  0.09481901]\n",
      " [  0.0886644 ]]\n",
      "the iterations of  447200 the loss is 0.2581546930066546 theta= [[-11.18589076]\n",
      " [  0.09482785]\n",
      " [  0.08867333]]\n",
      "the iterations of  447300 the loss is 0.2581422112915795 theta= [[-11.18700796]\n",
      " [  0.09483668]\n",
      " [  0.08868226]]\n",
      "the iterations of  447400 the loss is 0.25812973431742714 theta= [[-11.18812495]\n",
      " [  0.09484552]\n",
      " [  0.08869119]]\n",
      "the iterations of  447500 the loss is 0.2581172620814973 theta= [[-11.18924172]\n",
      " [  0.09485435]\n",
      " [  0.08870011]]\n",
      "the iterations of  447600 the loss is 0.25810479458109226 theta= [[-11.19035828]\n",
      " [  0.09486318]\n",
      " [  0.08870903]]\n",
      "the iterations of  447700 the loss is 0.258092331813516 theta= [[-11.19147463]\n",
      " [  0.09487201]\n",
      " [  0.08871795]]\n",
      "the iterations of  447800 the loss is 0.2580798737760748 theta= [[-11.19259077]\n",
      " [  0.09488084]\n",
      " [  0.08872687]]\n",
      "the iterations of  447900 the loss is 0.25806742046607656 theta= [[-11.19370669]\n",
      " [  0.09488967]\n",
      " [  0.08873579]]\n",
      "the iterations of  448000 the loss is 0.25805497188083193 theta= [[-11.19482241]\n",
      " [  0.0948985 ]\n",
      " [  0.08874471]]\n",
      "the iterations of  448100 the loss is 0.2580425280176532 theta= [[-11.19593791]\n",
      " [  0.09490732]\n",
      " [  0.08875362]]\n",
      "the iterations of  448200 the loss is 0.2580300888738548 theta= [[-11.1970532 ]\n",
      " [  0.09491614]\n",
      " [  0.08876254]]\n",
      "the iterations of  448300 the loss is 0.2580176544467532 theta= [[-11.19816828]\n",
      " [  0.09492496]\n",
      " [  0.08877145]]\n",
      "the iterations of  448400 the loss is 0.2580052247336669 theta= [[-11.19928315]\n",
      " [  0.09493378]\n",
      " [  0.08878036]]\n",
      "the iterations of  448500 the loss is 0.25799279973191674 theta= [[-11.2003978 ]\n",
      " [  0.0949426 ]\n",
      " [  0.08878926]]\n",
      "the iterations of  448600 the loss is 0.25798037943882535 theta= [[-11.20151225]\n",
      " [  0.09495141]\n",
      " [  0.08879817]]\n",
      "the iterations of  448700 the loss is 0.2579679638517176 theta= [[-11.20262648]\n",
      " [  0.09496023]\n",
      " [  0.08880707]]\n",
      "the iterations of  448800 the loss is 0.2579555529679201 theta= [[-11.20374051]\n",
      " [  0.09496904]\n",
      " [  0.08881598]]\n",
      "the iterations of  448900 the loss is 0.2579431467847617 theta= [[-11.20485432]\n",
      " [  0.09497785]\n",
      " [  0.08882488]]\n",
      "the iterations of  449000 the loss is 0.2579307452995735 theta= [[-11.20596792]\n",
      " [  0.09498666]\n",
      " [  0.08883378]]\n",
      "the iterations of  449100 the loss is 0.2579183485096882 theta= [[-11.20708131]\n",
      " [  0.09499547]\n",
      " [  0.08884268]]\n",
      "the iterations of  449200 the loss is 0.2579059564124411 theta= [[-11.20819449]\n",
      " [  0.09500427]\n",
      " [  0.08885157]]\n",
      "the iterations of  449300 the loss is 0.257893569005169 theta= [[-11.20930746]\n",
      " [  0.09501308]\n",
      " [  0.08886047]]\n",
      "the iterations of  449400 the loss is 0.25788118628521106 theta= [[-11.21042022]\n",
      " [  0.09502188]\n",
      " [  0.08886936]]\n",
      "the iterations of  449500 the loss is 0.25786880824990843 theta= [[-11.21153276]\n",
      " [  0.09503068]\n",
      " [  0.08887825]]\n",
      "the iterations of  449600 the loss is 0.25785643489660415 theta= [[-11.2126451 ]\n",
      " [  0.09503948]\n",
      " [  0.08888714]]\n",
      "the iterations of  449700 the loss is 0.25784406622264366 theta= [[-11.21375723]\n",
      " [  0.09504828]\n",
      " [  0.08889603]]\n",
      "the iterations of  449800 the loss is 0.25783170222537405 theta= [[-11.21486914]\n",
      " [  0.09505707]\n",
      " [  0.08890492]]\n",
      "the iterations of  449900 the loss is 0.25781934290214464 theta= [[-11.21598085]\n",
      " [  0.09506587]\n",
      " [  0.0889138 ]]\n",
      "the iterations of  450000 the loss is 0.25780698825030673 theta= [[-11.21709235]\n",
      " [  0.09507466]\n",
      " [  0.08892268]]\n",
      "the iterations of  450100 the loss is 0.25779463826721366 theta= [[-11.21820363]\n",
      " [  0.09508345]\n",
      " [  0.08893156]]\n",
      "the iterations of  450200 the loss is 0.25778229295022054 theta= [[-11.21931471]\n",
      " [  0.09509224]\n",
      " [  0.08894044]]\n",
      "the iterations of  450300 the loss is 0.25776995229668503 theta= [[-11.22042558]\n",
      " [  0.09510103]\n",
      " [  0.08894932]]\n",
      "the iterations of  450400 the loss is 0.25775761630396654 theta= [[-11.22153623]\n",
      " [  0.09510982]\n",
      " [  0.0889582 ]]\n",
      "the iterations of  450500 the loss is 0.2577452849694262 theta= [[-11.22264668]\n",
      " [  0.0951186 ]\n",
      " [  0.08896707]]\n",
      "the iterations of  450600 the loss is 0.2577329582904279 theta= [[-11.22375692]\n",
      " [  0.09512738]\n",
      " [  0.08897595]]\n",
      "the iterations of  450700 the loss is 0.25772063626433667 theta= [[-11.22486695]\n",
      " [  0.09513616]\n",
      " [  0.08898482]]\n",
      "the iterations of  450800 the loss is 0.2577083188885202 theta= [[-11.22597676]\n",
      " [  0.09514494]\n",
      " [  0.08899369]]\n",
      "the iterations of  450900 the loss is 0.25769600616034793 theta= [[-11.22708637]\n",
      " [  0.09515372]\n",
      " [  0.08900256]]\n",
      "the iterations of  451000 the loss is 0.2576836980771914 theta= [[-11.22819577]\n",
      " [  0.0951625 ]\n",
      " [  0.08901142]]\n",
      "the iterations of  451100 the loss is 0.257671394636424 theta= [[-11.22930496]\n",
      " [  0.09517127]\n",
      " [  0.08902029]]\n",
      "the iterations of  451200 the loss is 0.25765909583542135 theta= [[-11.23041394]\n",
      " [  0.09518005]\n",
      " [  0.08902915]]\n",
      "the iterations of  451300 the loss is 0.25764680167156095 theta= [[-11.23152272]\n",
      " [  0.09518882]\n",
      " [  0.08903801]]\n",
      "the iterations of  451400 the loss is 0.2576345121422224 theta= [[-11.23263128]\n",
      " [  0.09519759]\n",
      " [  0.08904687]]\n",
      "the iterations of  451500 the loss is 0.25762222724478706 theta= [[-11.23373963]\n",
      " [  0.09520636]\n",
      " [  0.08905573]]\n",
      "the iterations of  451600 the loss is 0.25760994697663847 theta= [[-11.23484778]\n",
      " [  0.09521512]\n",
      " [  0.08906459]]\n",
      "the iterations of  451700 the loss is 0.2575976713351622 theta= [[-11.23595571]\n",
      " [  0.09522389]\n",
      " [  0.08907344]]\n",
      "the iterations of  451800 the loss is 0.2575854003177458 theta= [[-11.23706344]\n",
      " [  0.09523265]\n",
      " [  0.0890823 ]]\n",
      "the iterations of  451900 the loss is 0.2575731339217788 theta= [[-11.23817096]\n",
      " [  0.09524141]\n",
      " [  0.08909115]]\n",
      "the iterations of  452000 the loss is 0.2575608721446525 theta= [[-11.23927827]\n",
      " [  0.09525017]\n",
      " [  0.0891    ]]\n",
      "the iterations of  452100 the loss is 0.25754861498376064 theta= [[-11.24038537]\n",
      " [  0.09525893]\n",
      " [  0.08910885]]\n",
      "the iterations of  452200 the loss is 0.25753636243649847 theta= [[-11.24149227]\n",
      " [  0.09526769]\n",
      " [  0.08911769]]\n",
      "the iterations of  452300 the loss is 0.25752411450026375 theta= [[-11.24259895]\n",
      " [  0.09527644]\n",
      " [  0.08912654]]\n",
      "the iterations of  452400 the loss is 0.25751187117245544 theta= [[-11.24370543]\n",
      " [  0.0952852 ]\n",
      " [  0.08913538]]\n",
      "the iterations of  452500 the loss is 0.2574996324504754 theta= [[-11.2448117 ]\n",
      " [  0.09529395]\n",
      " [  0.08914422]]\n",
      "the iterations of  452600 the loss is 0.2574873983317269 theta= [[-11.24591776]\n",
      " [  0.0953027 ]\n",
      " [  0.08915306]]\n",
      "the iterations of  452700 the loss is 0.2574751688136154 theta= [[-11.24702361]\n",
      " [  0.09531145]\n",
      " [  0.0891619 ]]\n",
      "the iterations of  452800 the loss is 0.2574629438935481 theta= [[-11.24812926]\n",
      " [  0.0953202 ]\n",
      " [  0.08917074]]\n",
      "the iterations of  452900 the loss is 0.2574507235689345 theta= [[-11.2492347 ]\n",
      " [  0.09532894]\n",
      " [  0.08917958]]\n",
      "the iterations of  453000 the loss is 0.2574385078371857 theta= [[-11.25033993]\n",
      " [  0.09533769]\n",
      " [  0.08918841]]\n",
      "the iterations of  453100 the loss is 0.25742629669571515 theta= [[-11.25144495]\n",
      " [  0.09534643]\n",
      " [  0.08919724]]\n",
      "the iterations of  453200 the loss is 0.25741409014193817 theta= [[-11.25254976]\n",
      " [  0.09535517]\n",
      " [  0.08920607]]\n",
      "the iterations of  453300 the loss is 0.2574018881732719 theta= [[-11.25365437]\n",
      " [  0.09536391]\n",
      " [  0.0892149 ]]\n",
      "the iterations of  453400 the loss is 0.25738969078713536 theta= [[-11.25475877]\n",
      " [  0.09537265]\n",
      " [  0.08922373]]\n",
      "the iterations of  453500 the loss is 0.25737749798094994 theta= [[-11.25586296]\n",
      " [  0.09538138]\n",
      " [  0.08923255]]\n",
      "the iterations of  453600 the loss is 0.2573653097521386 theta= [[-11.25696694]\n",
      " [  0.09539012]\n",
      " [  0.08924138]]\n",
      "the iterations of  453700 the loss is 0.2573531260981265 theta= [[-11.25807072]\n",
      " [  0.09539885]\n",
      " [  0.0892502 ]]\n",
      "the iterations of  453800 the loss is 0.25734094701634064 theta= [[-11.25917429]\n",
      " [  0.09540758]\n",
      " [  0.08925902]]\n",
      "the iterations of  453900 the loss is 0.25732877250420993 theta= [[-11.26027766]\n",
      " [  0.09541631]\n",
      " [  0.08926784]]\n",
      "the iterations of  454000 the loss is 0.25731660255916533 theta= [[-11.26138081]\n",
      " [  0.09542504]\n",
      " [  0.08927666]]\n",
      "the iterations of  454100 the loss is 0.25730443717863966 theta= [[-11.26248376]\n",
      " [  0.09543376]\n",
      " [  0.08928547]]\n",
      "the iterations of  454200 the loss is 0.25729227636006796 theta= [[-11.2635865 ]\n",
      " [  0.09544249]\n",
      " [  0.08929429]]\n",
      "the iterations of  454300 the loss is 0.25728012010088663 theta= [[-11.26468904]\n",
      " [  0.09545121]\n",
      " [  0.0893031 ]]\n",
      "the iterations of  454400 the loss is 0.25726796839853483 theta= [[-11.26579137]\n",
      " [  0.09545993]\n",
      " [  0.08931191]]\n",
      "the iterations of  454500 the loss is 0.25725582125045304 theta= [[-11.26689349]\n",
      " [  0.09546865]\n",
      " [  0.08932072]]\n",
      "the iterations of  454600 the loss is 0.2572436786540839 theta= [[-11.26799541]\n",
      " [  0.09547737]\n",
      " [  0.08932953]]\n",
      "the iterations of  454700 the loss is 0.2572315406068719 theta= [[-11.26909712]\n",
      " [  0.09548609]\n",
      " [  0.08933833]]\n",
      "the iterations of  454800 the loss is 0.25721940710626356 theta= [[-11.27019862]\n",
      " [  0.0954948 ]\n",
      " [  0.08934714]]\n",
      "the iterations of  454900 the loss is 0.2572072781497073 theta= [[-11.27129992]\n",
      " [  0.09550351]\n",
      " [  0.08935594]]\n",
      "the iterations of  455000 the loss is 0.2571951537346536 theta= [[-11.27240101]\n",
      " [  0.09551223]\n",
      " [  0.08936474]]\n",
      "the iterations of  455100 the loss is 0.2571830338585548 theta= [[-11.27350189]\n",
      " [  0.09552094]\n",
      " [  0.08937354]]\n",
      "the iterations of  455200 the loss is 0.2571709185188649 theta= [[-11.27460257]\n",
      " [  0.09552964]\n",
      " [  0.08938234]]\n",
      "the iterations of  455300 the loss is 0.2571588077130402 theta= [[-11.27570304]\n",
      " [  0.09553835]\n",
      " [  0.08939114]]\n",
      "the iterations of  455400 the loss is 0.25714670143853885 theta= [[-11.27680331]\n",
      " [  0.09554706]\n",
      " [  0.08939993]]\n",
      "the iterations of  455500 the loss is 0.25713459969282076 theta= [[-11.27790337]\n",
      " [  0.09555576]\n",
      " [  0.08940873]]\n",
      "the iterations of  455600 the loss is 0.2571225024733479 theta= [[-11.27900323]\n",
      " [  0.09556446]\n",
      " [  0.08941752]]\n",
      "the iterations of  455700 the loss is 0.2571104097775841 theta= [[-11.28010288]\n",
      " [  0.09557316]\n",
      " [  0.08942631]]\n",
      "the iterations of  455800 the loss is 0.25709832160299523 theta= [[-11.28120232]\n",
      " [  0.09558186]\n",
      " [  0.0894351 ]]\n",
      "the iterations of  455900 the loss is 0.2570862379470489 theta= [[-11.28230156]\n",
      " [  0.09559056]\n",
      " [  0.08944388]]\n",
      "the iterations of  456000 the loss is 0.2570741588072148 theta= [[-11.28340059]\n",
      " [  0.09559926]\n",
      " [  0.08945267]]\n",
      "the iterations of  456100 the loss is 0.2570620841809643 theta= [[-11.28449942]\n",
      " [  0.09560795]\n",
      " [  0.08946145]]\n",
      "the iterations of  456200 the loss is 0.2570500140657711 theta= [[-11.28559804]\n",
      " [  0.09561664]\n",
      " [  0.08947023]]\n",
      "the iterations of  456300 the loss is 0.25703794845911043 theta= [[-11.28669646]\n",
      " [  0.09562533]\n",
      " [  0.08947901]]\n",
      "the iterations of  456400 the loss is 0.2570258873584596 theta= [[-11.28779467]\n",
      " [  0.09563402]\n",
      " [  0.08948779]]\n",
      "the iterations of  456500 the loss is 0.2570138307612977 theta= [[-11.28889268]\n",
      " [  0.09564271]\n",
      " [  0.08949657]]\n",
      "the iterations of  456600 the loss is 0.2570017786651059 theta= [[-11.28999048]\n",
      " [  0.0956514 ]\n",
      " [  0.08950534]]\n",
      "the iterations of  456700 the loss is 0.2569897310673672 theta= [[-11.29108808]\n",
      " [  0.09566008]\n",
      " [  0.08951412]]\n",
      "the iterations of  456800 the loss is 0.25697768796556625 theta= [[-11.29218547]\n",
      " [  0.09566876]\n",
      " [  0.08952289]]\n",
      "the iterations of  456900 the loss is 0.25696564935719 theta= [[-11.29328265]\n",
      " [  0.09567744]\n",
      " [  0.08953166]]\n",
      "the iterations of  457000 the loss is 0.2569536152397272 theta= [[-11.29437964]\n",
      " [  0.09568612]\n",
      " [  0.08954043]]\n",
      "the iterations of  457100 the loss is 0.25694158561066827 theta= [[-11.29547642]\n",
      " [  0.0956948 ]\n",
      " [  0.0895492 ]]\n",
      "the iterations of  457200 the loss is 0.25692956046750604 theta= [[-11.29657299]\n",
      " [  0.09570348]\n",
      " [  0.08955796]]\n",
      "the iterations of  457300 the loss is 0.2569175398077347 theta= [[-11.29766936]\n",
      " [  0.09571215]\n",
      " [  0.08956673]]\n",
      "the iterations of  457400 the loss is 0.25690552362885033 theta= [[-11.29876552]\n",
      " [  0.09572083]\n",
      " [  0.08957549]]\n",
      "the iterations of  457500 the loss is 0.25689351192835125 theta= [[-11.29986148]\n",
      " [  0.0957295 ]\n",
      " [  0.08958425]]\n",
      "the iterations of  457600 the loss is 0.25688150470373733 theta= [[-11.30095724]\n",
      " [  0.09573817]\n",
      " [  0.08959301]]\n",
      "the iterations of  457700 the loss is 0.25686950195251085 theta= [[-11.30205279]\n",
      " [  0.09574684]\n",
      " [  0.08960177]]\n",
      "the iterations of  457800 the loss is 0.25685750367217525 theta= [[-11.30314814]\n",
      " [  0.09575551]\n",
      " [  0.08961052]]\n",
      "the iterations of  457900 the loss is 0.2568455098602363 theta= [[-11.30424329]\n",
      " [  0.09576417]\n",
      " [  0.08961928]]\n",
      "the iterations of  458000 the loss is 0.25683352051420183 theta= [[-11.30533823]\n",
      " [  0.09577283]\n",
      " [  0.08962803]]\n",
      "the iterations of  458100 the loss is 0.25682153563158083 theta= [[-11.30643296]\n",
      " [  0.0957815 ]\n",
      " [  0.08963678]]\n",
      "the iterations of  458200 the loss is 0.2568095552098851 theta= [[-11.30752749]\n",
      " [  0.09579016]\n",
      " [  0.08964553]]\n",
      "the iterations of  458300 the loss is 0.25679757924662766 theta= [[-11.30862182]\n",
      " [  0.09579882]\n",
      " [  0.08965428]]\n",
      "the iterations of  458400 the loss is 0.25678560773932346 theta= [[-11.30971595]\n",
      " [  0.09580747]\n",
      " [  0.08966303]]\n",
      "the iterations of  458500 the loss is 0.25677364068548963 theta= [[-11.31080987]\n",
      " [  0.09581613]\n",
      " [  0.08967177]]\n",
      "the iterations of  458600 the loss is 0.25676167808264494 theta= [[-11.31190359]\n",
      " [  0.09582478]\n",
      " [  0.08968051]]\n",
      "the iterations of  458700 the loss is 0.2567497199283101 theta= [[-11.3129971 ]\n",
      " [  0.09583344]\n",
      " [  0.08968926]]\n",
      "the iterations of  458800 the loss is 0.2567377662200076 theta= [[-11.31409042]\n",
      " [  0.09584209]\n",
      " [  0.089698  ]]\n",
      "the iterations of  458900 the loss is 0.25672581695526175 theta= [[-11.31518352]\n",
      " [  0.09585074]\n",
      " [  0.08970673]]\n",
      "the iterations of  459000 the loss is 0.256713872131599 theta= [[-11.31627643]\n",
      " [  0.09585939]\n",
      " [  0.08971547]]\n",
      "the iterations of  459100 the loss is 0.25670193174654754 theta= [[-11.31736913]\n",
      " [  0.09586803]\n",
      " [  0.08972421]]\n",
      "the iterations of  459200 the loss is 0.2566899957976372 theta= [[-11.31846163]\n",
      " [  0.09587668]\n",
      " [  0.08973294]]\n",
      "the iterations of  459300 the loss is 0.2566780642824 theta= [[-11.31955393]\n",
      " [  0.09588532]\n",
      " [  0.08974167]]\n",
      "the iterations of  459400 the loss is 0.2566661371983694 theta= [[-11.32064602]\n",
      " [  0.09589396]\n",
      " [  0.0897504 ]]\n",
      "the iterations of  459500 the loss is 0.2566542145430814 theta= [[-11.32173791]\n",
      " [  0.0959026 ]\n",
      " [  0.08975913]]\n",
      "the iterations of  459600 the loss is 0.25664229631407304 theta= [[-11.3228296 ]\n",
      " [  0.09591124]\n",
      " [  0.08976786]]\n",
      "the iterations of  459700 the loss is 0.2566303825088838 theta= [[-11.32392108]\n",
      " [  0.09591988]\n",
      " [  0.08977658]]\n",
      "the iterations of  459800 the loss is 0.25661847312505465 theta= [[-11.32501237]\n",
      " [  0.09592851]\n",
      " [  0.08978531]]\n",
      "the iterations of  459900 the loss is 0.2566065681601287 theta= [[-11.32610345]\n",
      " [  0.09593715]\n",
      " [  0.08979403]]\n",
      "the iterations of  460000 the loss is 0.25659466761165073 theta= [[-11.32719432]\n",
      " [  0.09594578]\n",
      " [  0.08980275]]\n",
      "the iterations of  460100 the loss is 0.25658277147716735 theta= [[-11.328285  ]\n",
      " [  0.09595441]\n",
      " [  0.08981147]]\n",
      "the iterations of  460200 the loss is 0.256570879754227 theta= [[-11.32937547]\n",
      " [  0.09596304]\n",
      " [  0.08982019]]\n",
      "the iterations of  460300 the loss is 0.25655899244038016 theta= [[-11.33046574]\n",
      " [  0.09597167]\n",
      " [  0.08982891]]\n",
      "the iterations of  460400 the loss is 0.25654710953317894 theta= [[-11.33155581]\n",
      " [  0.09598029]\n",
      " [  0.08983762]]\n",
      "the iterations of  460500 the loss is 0.2565352310301774 theta= [[-11.33264568]\n",
      " [  0.09598892]\n",
      " [  0.08984633]]\n",
      "the iterations of  460600 the loss is 0.2565233569289312 theta= [[-11.33373535]\n",
      " [  0.09599754]\n",
      " [  0.08985504]]\n",
      "the iterations of  460700 the loss is 0.25651148722699824 theta= [[-11.33482481]\n",
      " [  0.09600616]\n",
      " [  0.08986375]]\n",
      "the iterations of  460800 the loss is 0.2564996219219379 theta= [[-11.33591407]\n",
      " [  0.09601478]\n",
      " [  0.08987246]]\n",
      "the iterations of  460900 the loss is 0.2564877610113115 theta= [[-11.33700313]\n",
      " [  0.0960234 ]\n",
      " [  0.08988117]]\n",
      "the iterations of  461000 the loss is 0.2564759044926823 theta= [[-11.33809199]\n",
      " [  0.09603201]\n",
      " [  0.08988987]]\n",
      "the iterations of  461100 the loss is 0.25646405236361547 theta= [[-11.33918064]\n",
      " [  0.09604063]\n",
      " [  0.08989858]]\n",
      "the iterations of  461200 the loss is 0.25645220462167756 theta= [[-11.3402691 ]\n",
      " [  0.09604924]\n",
      " [  0.08990728]]\n",
      "the iterations of  461300 the loss is 0.2564403612644373 theta= [[-11.34135735]\n",
      " [  0.09605785]\n",
      " [  0.08991598]]\n",
      "the iterations of  461400 the loss is 0.2564285222894653 theta= [[-11.3424454 ]\n",
      " [  0.09606646]\n",
      " [  0.08992468]]\n",
      "the iterations of  461500 the loss is 0.25641668769433357 theta= [[-11.34353326]\n",
      " [  0.09607507]\n",
      " [  0.08993338]]\n",
      "the iterations of  461600 the loss is 0.2564048574766163 theta= [[-11.3446209 ]\n",
      " [  0.09608368]\n",
      " [  0.08994207]]\n",
      "the iterations of  461700 the loss is 0.2563930316338895 theta= [[-11.34570835]\n",
      " [  0.09609229]\n",
      " [  0.08995077]]\n",
      "the iterations of  461800 the loss is 0.2563812101637309 theta= [[-11.3467956 ]\n",
      " [  0.09610089]\n",
      " [  0.08995946]]\n",
      "the iterations of  461900 the loss is 0.25636939306371986 theta= [[-11.34788265]\n",
      " [  0.09610949]\n",
      " [  0.08996815]]\n",
      "the iterations of  462000 the loss is 0.256357580331438 theta= [[-11.34896949]\n",
      " [  0.09611809]\n",
      " [  0.08997684]]\n",
      "the iterations of  462100 the loss is 0.25634577196446834 theta= [[-11.35005614]\n",
      " [  0.09612669]\n",
      " [  0.08998553]]\n",
      "the iterations of  462200 the loss is 0.25633396796039587 theta= [[-11.35114258]\n",
      " [  0.09613529]\n",
      " [  0.08999421]]\n",
      "the iterations of  462300 the loss is 0.2563221683168073 theta= [[-11.35222882]\n",
      " [  0.09614389]\n",
      " [  0.0900029 ]]\n",
      "the iterations of  462400 the loss is 0.2563103730312913 theta= [[-11.35331487]\n",
      " [  0.09615248]\n",
      " [  0.09001158]]\n",
      "the iterations of  462500 the loss is 0.25629858210143824 theta= [[-11.35440071]\n",
      " [  0.09616107]\n",
      " [  0.09002026]]\n",
      "the iterations of  462600 the loss is 0.2562867955248403 theta= [[-11.35548635]\n",
      " [  0.09616966]\n",
      " [  0.09002894]]\n",
      "the iterations of  462700 the loss is 0.25627501329909147 theta= [[-11.35657179]\n",
      " [  0.09617825]\n",
      " [  0.09003762]]\n",
      "the iterations of  462800 the loss is 0.2562632354217875 theta= [[-11.35765703]\n",
      " [  0.09618684]\n",
      " [  0.09004629]]\n",
      "the iterations of  462900 the loss is 0.25625146189052617 theta= [[-11.35874207]\n",
      " [  0.09619543]\n",
      " [  0.09005497]]\n",
      "the iterations of  463000 the loss is 0.25623969270290625 theta= [[-11.35982691]\n",
      " [  0.09620402]\n",
      " [  0.09006364]]\n",
      "the iterations of  463100 the loss is 0.2562279278565296 theta= [[-11.36091155]\n",
      " [  0.0962126 ]\n",
      " [  0.09007232]]\n",
      "the iterations of  463200 the loss is 0.2562161673489988 theta= [[-11.36199599]\n",
      " [  0.09622118]\n",
      " [  0.09008099]]\n",
      "the iterations of  463300 the loss is 0.25620441117791865 theta= [[-11.36308023]\n",
      " [  0.09622976]\n",
      " [  0.09008965]]\n",
      "the iterations of  463400 the loss is 0.25619265934089586 theta= [[-11.36416427]\n",
      " [  0.09623834]\n",
      " [  0.09009832]]\n",
      "the iterations of  463500 the loss is 0.2561809118355386 theta= [[-11.36524811]\n",
      " [  0.09624692]\n",
      " [  0.09010699]]\n",
      "the iterations of  463600 the loss is 0.2561691686594569 theta= [[-11.36633176]\n",
      " [  0.09625549]\n",
      " [  0.09011565]]\n",
      "the iterations of  463700 the loss is 0.2561574298102627 theta= [[-11.3674152 ]\n",
      " [  0.09626407]\n",
      " [  0.09012431]]\n",
      "the iterations of  463800 the loss is 0.2561456952855696 theta= [[-11.36849844]\n",
      " [  0.09627264]\n",
      " [  0.09013297]]\n",
      "the iterations of  463900 the loss is 0.25613396508299313 theta= [[-11.36958148]\n",
      " [  0.09628121]\n",
      " [  0.09014163]]\n",
      "the iterations of  464000 the loss is 0.2561222392001504 theta= [[-11.37066432]\n",
      " [  0.09628978]\n",
      " [  0.09015029]]\n",
      "the iterations of  464100 the loss is 0.2561105176346604 theta= [[-11.37174697]\n",
      " [  0.09629835]\n",
      " [  0.09015895]]\n",
      "the iterations of  464200 the loss is 0.2560988003841441 theta= [[-11.37282941]\n",
      " [  0.09630692]\n",
      " [  0.0901676 ]]\n",
      "the iterations of  464300 the loss is 0.2560870874462238 theta= [[-11.37391166]\n",
      " [  0.09631548]\n",
      " [  0.09017626]]\n",
      "the iterations of  464400 the loss is 0.25607537881852405 theta= [[-11.3749937 ]\n",
      " [  0.09632404]\n",
      " [  0.09018491]]\n",
      "the iterations of  464500 the loss is 0.2560636744986706 theta= [[-11.37607555]\n",
      " [  0.09633261]\n",
      " [  0.09019356]]\n",
      "the iterations of  464600 the loss is 0.25605197448429157 theta= [[-11.37715719]\n",
      " [  0.09634117]\n",
      " [  0.09020221]]\n",
      "the iterations of  464700 the loss is 0.2560402787730163 theta= [[-11.37823864]\n",
      " [  0.09634973]\n",
      " [  0.09021085]]\n",
      "the iterations of  464800 the loss is 0.25602858736247647 theta= [[-11.37931989]\n",
      " [  0.09635828]\n",
      " [  0.0902195 ]]\n",
      "the iterations of  464900 the loss is 0.25601690025030505 theta= [[-11.38040094]\n",
      " [  0.09636684]\n",
      " [  0.09022814]]\n",
      "the iterations of  465000 the loss is 0.25600521743413696 theta= [[-11.3814818 ]\n",
      " [  0.09637539]\n",
      " [  0.09023678]]\n",
      "the iterations of  465100 the loss is 0.2559935389116088 theta= [[-11.38256245]\n",
      " [  0.09638394]\n",
      " [  0.09024542]]\n",
      "the iterations of  465200 the loss is 0.2559818646803592 theta= [[-11.38364291]\n",
      " [  0.0963925 ]\n",
      " [  0.09025406]]\n",
      "the iterations of  465300 the loss is 0.2559701947380278 theta= [[-11.38472316]\n",
      " [  0.09640105]\n",
      " [  0.0902627 ]]\n",
      "the iterations of  465400 the loss is 0.255958529082257 theta= [[-11.38580322]\n",
      " [  0.09640959]\n",
      " [  0.09027134]]\n",
      "the iterations of  465500 the loss is 0.2559468677106905 theta= [[-11.38688308]\n",
      " [  0.09641814]\n",
      " [  0.09027997]]\n",
      "the iterations of  465600 the loss is 0.2559352106209733 theta= [[-11.38796274]\n",
      " [  0.09642668]\n",
      " [  0.0902886 ]]\n",
      "the iterations of  465700 the loss is 0.2559235578107529 theta= [[-11.3890422 ]\n",
      " [  0.09643523]\n",
      " [  0.09029723]]\n",
      "the iterations of  465800 the loss is 0.2559119092776782 theta= [[-11.39012147]\n",
      " [  0.09644377]\n",
      " [  0.09030586]]\n",
      "the iterations of  465900 the loss is 0.2559002650193998 theta= [[-11.39120054]\n",
      " [  0.09645231]\n",
      " [  0.09031449]]\n",
      "the iterations of  466000 the loss is 0.25588862503357007 theta= [[-11.39227941]\n",
      " [  0.09646085]\n",
      " [  0.09032312]]\n",
      "the iterations of  466100 the loss is 0.2558769893178431 theta= [[-11.39335808]\n",
      " [  0.09646939]\n",
      " [  0.09033174]]\n",
      "the iterations of  466200 the loss is 0.25586535786987497 theta= [[-11.39443655]\n",
      " [  0.09647792]\n",
      " [  0.09034037]]\n",
      "the iterations of  466300 the loss is 0.25585373068732326 theta= [[-11.39551483]\n",
      " [  0.09648646]\n",
      " [  0.09034899]]\n",
      "the iterations of  466400 the loss is 0.25584210776784727 theta= [[-11.3965929 ]\n",
      " [  0.09649499]\n",
      " [  0.09035761]]\n",
      "the iterations of  466500 the loss is 0.2558304891091081 theta= [[-11.39767078]\n",
      " [  0.09650352]\n",
      " [  0.09036623]]\n",
      "the iterations of  466600 the loss is 0.2558188747087687 theta= [[-11.39874847]\n",
      " [  0.09651205]\n",
      " [  0.09037484]]\n",
      "the iterations of  466700 the loss is 0.25580726456449354 theta= [[-11.39982595]\n",
      " [  0.09652058]\n",
      " [  0.09038346]]\n",
      "the iterations of  466800 the loss is 0.25579565867394877 theta= [[-11.40090324]\n",
      " [  0.0965291 ]\n",
      " [  0.09039207]]\n",
      "the iterations of  466900 the loss is 0.25578405703480267 theta= [[-11.40198033]\n",
      " [  0.09653763]\n",
      " [  0.09040069]]\n",
      "the iterations of  467000 the loss is 0.255772459644725 theta= [[-11.40305722]\n",
      " [  0.09654615]\n",
      " [  0.0904093 ]]\n",
      "the iterations of  467100 the loss is 0.25576086650138713 theta= [[-11.40413392]\n",
      " [  0.09655467]\n",
      " [  0.09041791]]\n",
      "the iterations of  467200 the loss is 0.25574927760246224 theta= [[-11.40521042]\n",
      " [  0.09656319]\n",
      " [  0.09042652]]\n",
      "the iterations of  467300 the loss is 0.25573769294562526 theta= [[-11.40628672]\n",
      " [  0.09657171]\n",
      " [  0.09043512]]\n",
      "the iterations of  467400 the loss is 0.2557261125285528 theta= [[-11.40736283]\n",
      " [  0.09658023]\n",
      " [  0.09044373]]\n",
      "the iterations of  467500 the loss is 0.2557145363489235 theta= [[-11.40843873]\n",
      " [  0.09658874]\n",
      " [  0.09045233]]\n",
      "the iterations of  467600 the loss is 0.25570296440441714 theta= [[-11.40951445]\n",
      " [  0.09659726]\n",
      " [  0.09046093]]\n",
      "the iterations of  467700 the loss is 0.25569139669271557 theta= [[-11.41058996]\n",
      " [  0.09660577]\n",
      " [  0.09046953]]\n",
      "the iterations of  467800 the loss is 0.25567983321150245 theta= [[-11.41166528]\n",
      " [  0.09661428]\n",
      " [  0.09047813]]\n",
      "the iterations of  467900 the loss is 0.2556682739584631 theta= [[-11.4127404 ]\n",
      " [  0.09662279]\n",
      " [  0.09048673]]\n",
      "the iterations of  468000 the loss is 0.25565671893128433 theta= [[-11.41381532]\n",
      " [  0.0966313 ]\n",
      " [  0.09049532]]\n",
      "the iterations of  468100 the loss is 0.2556451681276547 theta= [[-11.41489005]\n",
      " [  0.09663981]\n",
      " [  0.09050392]]\n",
      "the iterations of  468200 the loss is 0.25563362154526464 theta= [[-11.41596458]\n",
      " [  0.09664831]\n",
      " [  0.09051251]]\n",
      "the iterations of  468300 the loss is 0.25562207918180624 theta= [[-11.41703892]\n",
      " [  0.09665681]\n",
      " [  0.0905211 ]]\n",
      "the iterations of  468400 the loss is 0.2556105410349733 theta= [[-11.41811306]\n",
      " [  0.09666532]\n",
      " [  0.09052969]]\n",
      "the iterations of  468500 the loss is 0.25559900710246125 theta= [[-11.419187  ]\n",
      " [  0.09667382]\n",
      " [  0.09053828]]\n",
      "the iterations of  468600 the loss is 0.2555874773819673 theta= [[-11.42026075]\n",
      " [  0.09668231]\n",
      " [  0.09054686]]\n",
      "the iterations of  468700 the loss is 0.2555759518711905 theta= [[-11.4213343 ]\n",
      " [  0.09669081]\n",
      " [  0.09055545]]\n",
      "the iterations of  468800 the loss is 0.2555644305678312 theta= [[-11.42240765]\n",
      " [  0.09669931]\n",
      " [  0.09056403]]\n",
      "the iterations of  468900 the loss is 0.25555291346959164 theta= [[-11.42348081]\n",
      " [  0.0967078 ]\n",
      " [  0.09057261]]\n",
      "the iterations of  469000 the loss is 0.2555414005741761 theta= [[-11.42455378]\n",
      " [  0.09671629]\n",
      " [  0.09058119]]\n",
      "the iterations of  469100 the loss is 0.25552989187929 theta= [[-11.42562654]\n",
      " [  0.09672479]\n",
      " [  0.09058977]]\n",
      "the iterations of  469200 the loss is 0.25551838738264054 theta= [[-11.42669911]\n",
      " [  0.09673327]\n",
      " [  0.09059835]]\n",
      "the iterations of  469300 the loss is 0.2555068870819373 theta= [[-11.42777149]\n",
      " [  0.09674176]\n",
      " [  0.09060692]]\n",
      "the iterations of  469400 the loss is 0.2554953909748906 theta= [[-11.42884367]\n",
      " [  0.09675025]\n",
      " [  0.0906155 ]]\n",
      "the iterations of  469500 the loss is 0.2554838990592129 theta= [[-11.42991566]\n",
      " [  0.09675873]\n",
      " [  0.09062407]]\n",
      "the iterations of  469600 the loss is 0.25547241133261855 theta= [[-11.43098745]\n",
      " [  0.09676722]\n",
      " [  0.09063264]]\n",
      "the iterations of  469700 the loss is 0.2554609277928231 theta= [[-11.43205904]\n",
      " [  0.0967757 ]\n",
      " [  0.09064121]]\n",
      "the iterations of  469800 the loss is 0.2554494484375442 theta= [[-11.43313044]\n",
      " [  0.09678418]\n",
      " [  0.09064978]]\n",
      "the iterations of  469900 the loss is 0.2554379732645008 theta= [[-11.43420164]\n",
      " [  0.09679266]\n",
      " [  0.09065834]]\n",
      "the iterations of  470000 the loss is 0.25542650227141417 theta= [[-11.43527265]\n",
      " [  0.09680114]\n",
      " [  0.09066691]]\n",
      "the iterations of  470100 the loss is 0.25541503545600636 theta= [[-11.43634347]\n",
      " [  0.09680961]\n",
      " [  0.09067547]]\n",
      "the iterations of  470200 the loss is 0.2554035728160019 theta= [[-11.43741408]\n",
      " [  0.09681809]\n",
      " [  0.09068403]]\n",
      "the iterations of  470300 the loss is 0.2553921143491265 theta= [[-11.43848451]\n",
      " [  0.09682656]\n",
      " [  0.09069259]]\n",
      "the iterations of  470400 the loss is 0.2553806600531079 theta= [[-11.43955474]\n",
      " [  0.09683503]\n",
      " [  0.09070115]]\n",
      "the iterations of  470500 the loss is 0.2553692099256752 theta= [[-11.44062477]\n",
      " [  0.0968435 ]\n",
      " [  0.09070971]]\n",
      "the iterations of  470600 the loss is 0.2553577639645591 theta= [[-11.44169461]\n",
      " [  0.09685197]\n",
      " [  0.09071826]]\n",
      "the iterations of  470700 the loss is 0.2553463221674924 theta= [[-11.44276426]\n",
      " [  0.09686044]\n",
      " [  0.09072682]]\n",
      "the iterations of  470800 the loss is 0.2553348845322094 theta= [[-11.44383371]\n",
      " [  0.0968689 ]\n",
      " [  0.09073537]]\n",
      "the iterations of  470900 the loss is 0.2553234510564458 theta= [[-11.44490296]\n",
      " [  0.09687737]\n",
      " [  0.09074392]]\n",
      "the iterations of  471000 the loss is 0.2553120217379392 theta= [[-11.44597202]\n",
      " [  0.09688583]\n",
      " [  0.09075247]]\n",
      "the iterations of  471100 the loss is 0.25530059657442883 theta= [[-11.44704089]\n",
      " [  0.09689429]\n",
      " [  0.09076102]]\n",
      "the iterations of  471200 the loss is 0.25528917556365577 theta= [[-11.44810956]\n",
      " [  0.09690275]\n",
      " [  0.09076956]]\n",
      "the iterations of  471300 the loss is 0.25527775870336233 theta= [[-11.44917804]\n",
      " [  0.09691121]\n",
      " [  0.09077811]]\n",
      "the iterations of  471400 the loss is 0.255266345991293 theta= [[-11.45024633]\n",
      " [  0.09691966]\n",
      " [  0.09078665]]\n",
      "the iterations of  471500 the loss is 0.2552549374251933 theta= [[-11.45131442]\n",
      " [  0.09692812]\n",
      " [  0.09079519]]\n",
      "the iterations of  471600 the loss is 0.25524353300281094 theta= [[-11.45238231]\n",
      " [  0.09693657]\n",
      " [  0.09080373]]\n",
      "the iterations of  471700 the loss is 0.25523213272189504 theta= [[-11.45345001]\n",
      " [  0.09694502]\n",
      " [  0.09081227]]\n",
      "the iterations of  471800 the loss is 0.25522073658019656 theta= [[-11.45451752]\n",
      " [  0.09695347]\n",
      " [  0.09082081]]\n",
      "the iterations of  471900 the loss is 0.25520934457546746 theta= [[-11.45558484]\n",
      " [  0.09696192]\n",
      " [  0.09082935]]\n",
      "the iterations of  472000 the loss is 0.2551979567054625 theta= [[-11.45665196]\n",
      " [  0.09697037]\n",
      " [  0.09083788]]\n",
      "the iterations of  472100 the loss is 0.2551865729679373 theta= [[-11.45771889]\n",
      " [  0.09697881]\n",
      " [  0.09084641]]\n",
      "the iterations of  472200 the loss is 0.2551751933606491 theta= [[-11.45878562]\n",
      " [  0.09698726]\n",
      " [  0.09085494]]\n",
      "the iterations of  472300 the loss is 0.25516381788135706 theta= [[-11.45985216]\n",
      " [  0.0969957 ]\n",
      " [  0.09086347]]\n",
      "the iterations of  472400 the loss is 0.2551524465278218 theta= [[-11.46091851]\n",
      " [  0.09700414]\n",
      " [  0.090872  ]]\n",
      "the iterations of  472500 the loss is 0.25514107929780583 theta= [[-11.46198466]\n",
      " [  0.09701258]\n",
      " [  0.09088053]]\n",
      "the iterations of  472600 the loss is 0.25512971618907326 theta= [[-11.46305062]\n",
      " [  0.09702102]\n",
      " [  0.09088905]]\n",
      "the iterations of  472700 the loss is 0.25511835719938925 theta= [[-11.46411639]\n",
      " [  0.09702945]\n",
      " [  0.09089758]]\n",
      "the iterations of  472800 the loss is 0.2551070023265214 theta= [[-11.46518196]\n",
      " [  0.09703789]\n",
      " [  0.0909061 ]]\n",
      "the iterations of  472900 the loss is 0.25509565156823855 theta= [[-11.46624734]\n",
      " [  0.09704632]\n",
      " [  0.09091462]]\n",
      "the iterations of  473000 the loss is 0.2550843049223113 theta= [[-11.46731253]\n",
      " [  0.09705475]\n",
      " [  0.09092314]]\n",
      "the iterations of  473100 the loss is 0.2550729623865115 theta= [[-11.46837753]\n",
      " [  0.09706319]\n",
      " [  0.09093165]]\n",
      "the iterations of  473200 the loss is 0.2550616239586134 theta= [[-11.46944233]\n",
      " [  0.09707161]\n",
      " [  0.09094017]]\n",
      "the iterations of  473300 the loss is 0.2550502896363922 theta= [[-11.47050694]\n",
      " [  0.09708004]\n",
      " [  0.09094868]]\n",
      "the iterations of  473400 the loss is 0.25503895941762506 theta= [[-11.47157135]\n",
      " [  0.09708847]\n",
      " [  0.0909572 ]]\n",
      "the iterations of  473500 the loss is 0.2550276333000904 theta= [[-11.47263558]\n",
      " [  0.09709689]\n",
      " [  0.09096571]]\n",
      "the iterations of  473600 the loss is 0.25501631128156876 theta= [[-11.47369961]\n",
      " [  0.09710532]\n",
      " [  0.09097422]]\n",
      "the iterations of  473700 the loss is 0.25500499335984206 theta= [[-11.47476345]\n",
      " [  0.09711374]\n",
      " [  0.09098273]]\n",
      "the iterations of  473800 the loss is 0.2549936795326937 theta= [[-11.47582709]\n",
      " [  0.09712216]\n",
      " [  0.09099123]]\n",
      "the iterations of  473900 the loss is 0.25498236979790884 theta= [[-11.47689055]\n",
      " [  0.09713058]\n",
      " [  0.09099974]]\n",
      "the iterations of  474000 the loss is 0.2549710641532747 theta= [[-11.47795381]\n",
      " [  0.09713899]\n",
      " [  0.09100824]]\n",
      "the iterations of  474100 the loss is 0.2549597625965792 theta= [[-11.47901688]\n",
      " [  0.09714741]\n",
      " [  0.09101674]]\n",
      "the iterations of  474200 the loss is 0.25494846512561253 theta= [[-11.48007976]\n",
      " [  0.09715582]\n",
      " [  0.09102524]]\n",
      "the iterations of  474300 the loss is 0.25493717173816643 theta= [[-11.48114244]\n",
      " [  0.09716423]\n",
      " [  0.09103374]]\n",
      "the iterations of  474400 the loss is 0.2549258824320341 theta= [[-11.48220493]\n",
      " [  0.09717265]\n",
      " [  0.09104224]]\n",
      "the iterations of  474500 the loss is 0.2549145972050103 theta= [[-11.48326724]\n",
      " [  0.09718106]\n",
      " [  0.09105074]]\n",
      "the iterations of  474600 the loss is 0.25490331605489175 theta= [[-11.48432934]\n",
      " [  0.09718946]\n",
      " [  0.09105923]]\n",
      "the iterations of  474700 the loss is 0.25489203897947627 theta= [[-11.48539126]\n",
      " [  0.09719787]\n",
      " [  0.09106773]]\n",
      "the iterations of  474800 the loss is 0.25488076597656373 theta= [[-11.48645299]\n",
      " [  0.09720628]\n",
      " [  0.09107622]]\n",
      "the iterations of  474900 the loss is 0.25486949704395534 theta= [[-11.48751452]\n",
      " [  0.09721468]\n",
      " [  0.09108471]]\n",
      "the iterations of  475000 the loss is 0.25485823217945386 theta= [[-11.48857586]\n",
      " [  0.09722308]\n",
      " [  0.0910932 ]]\n",
      "the iterations of  475100 the loss is 0.25484697138086404 theta= [[-11.48963701]\n",
      " [  0.09723148]\n",
      " [  0.09110168]]\n",
      "the iterations of  475200 the loss is 0.254835714645992 theta= [[-11.49069797]\n",
      " [  0.09723988]\n",
      " [  0.09111017]]\n",
      "the iterations of  475300 the loss is 0.2548244619726452 theta= [[-11.49175874]\n",
      " [  0.09724828]\n",
      " [  0.09111865]]\n",
      "the iterations of  475400 the loss is 0.2548132133586331 theta= [[-11.49281932]\n",
      " [  0.09725667]\n",
      " [  0.09112714]]\n",
      "the iterations of  475500 the loss is 0.25480196880176664 theta= [[-11.4938797 ]\n",
      " [  0.09726507]\n",
      " [  0.09113562]]\n",
      "the iterations of  475600 the loss is 0.25479072829985827 theta= [[-11.4949399 ]\n",
      " [  0.09727346]\n",
      " [  0.0911441 ]]\n",
      "the iterations of  475700 the loss is 0.25477949185072213 theta= [[-11.4959999 ]\n",
      " [  0.09728185]\n",
      " [  0.09115257]]\n",
      "the iterations of  475800 the loss is 0.2547682594521737 theta= [[-11.49705971]\n",
      " [  0.09729024]\n",
      " [  0.09116105]]\n",
      "the iterations of  475900 the loss is 0.25475703110203035 theta= [[-11.49811933]\n",
      " [  0.09729863]\n",
      " [  0.09116953]]\n",
      "the iterations of  476000 the loss is 0.2547458067981112 theta= [[-11.49917876]\n",
      " [  0.09730702]\n",
      " [  0.091178  ]]\n",
      "the iterations of  476100 the loss is 0.25473458653823633 theta= [[-11.500238  ]\n",
      " [  0.09731541]\n",
      " [  0.09118647]]\n",
      "the iterations of  476200 the loss is 0.25472337032022807 theta= [[-11.50129705]\n",
      " [  0.09732379]\n",
      " [  0.09119494]]\n",
      "the iterations of  476300 the loss is 0.2547121581419098 theta= [[-11.5023559 ]\n",
      " [  0.09733217]\n",
      " [  0.09120341]]\n",
      "the iterations of  476400 the loss is 0.2547009500011068 theta= [[-11.50341457]\n",
      " [  0.09734055]\n",
      " [  0.09121188]]\n",
      "the iterations of  476500 the loss is 0.2546897458956459 theta= [[-11.50447305]\n",
      " [  0.09734893]\n",
      " [  0.09122034]]\n",
      "the iterations of  476600 the loss is 0.25467854582335553 theta= [[-11.50553133]\n",
      " [  0.09735731]\n",
      " [  0.09122881]]\n",
      "the iterations of  476700 the loss is 0.25466734978206573 theta= [[-11.50658943]\n",
      " [  0.09736569]\n",
      " [  0.09123727]]\n",
      "the iterations of  476800 the loss is 0.25465615776960776 theta= [[-11.50764733]\n",
      " [  0.09737406]\n",
      " [  0.09124573]]\n",
      "the iterations of  476900 the loss is 0.25464496978381485 theta= [[-11.50870505]\n",
      " [  0.09738244]\n",
      " [  0.09125419]]\n",
      "the iterations of  477000 the loss is 0.25463378582252166 theta= [[-11.50976257]\n",
      " [  0.09739081]\n",
      " [  0.09126265]]\n",
      "the iterations of  477100 the loss is 0.25462260588356456 theta= [[-11.5108199 ]\n",
      " [  0.09739918]\n",
      " [  0.09127111]]\n",
      "the iterations of  477200 the loss is 0.25461142996478114 theta= [[-11.51187705]\n",
      " [  0.09740755]\n",
      " [  0.09127956]]\n",
      "the iterations of  477300 the loss is 0.254600258064011 theta= [[-11.512934  ]\n",
      " [  0.09741592]\n",
      " [  0.09128802]]\n",
      "the iterations of  477400 the loss is 0.25458909017909515 theta= [[-11.51399076]\n",
      " [  0.09742428]\n",
      " [  0.09129647]]\n",
      "the iterations of  477500 the loss is 0.2545779263078758 theta= [[-11.51504734]\n",
      " [  0.09743265]\n",
      " [  0.09130492]]\n",
      "the iterations of  477600 the loss is 0.2545667664481973 theta= [[-11.51610372]\n",
      " [  0.09744101]\n",
      " [  0.09131337]]\n",
      "the iterations of  477700 the loss is 0.2545556105979055 theta= [[-11.51715992]\n",
      " [  0.09744937]\n",
      " [  0.09132182]]\n",
      "the iterations of  477800 the loss is 0.25454445875484716 theta= [[-11.51821592]\n",
      " [  0.09745773]\n",
      " [  0.09133027]]\n",
      "the iterations of  477900 the loss is 0.2545333109168714 theta= [[-11.51927173]\n",
      " [  0.09746609]\n",
      " [  0.09133871]]\n",
      "the iterations of  478000 the loss is 0.2545221670818285 theta= [[-11.52032736]\n",
      " [  0.09747445]\n",
      " [  0.09134716]]\n",
      "the iterations of  478100 the loss is 0.2545110272475704 theta= [[-11.5213828 ]\n",
      " [  0.09748281]\n",
      " [  0.0913556 ]]\n",
      "the iterations of  478200 the loss is 0.25449989141195045 theta= [[-11.52243804]\n",
      " [  0.09749116]\n",
      " [  0.09136404]]\n",
      "the iterations of  478300 the loss is 0.25448875957282385 theta= [[-11.5234931 ]\n",
      " [  0.09749951]\n",
      " [  0.09137248]]\n",
      "the iterations of  478400 the loss is 0.25447763172804705 theta= [[-11.52454797]\n",
      " [  0.09750787]\n",
      " [  0.09138091]]\n",
      "the iterations of  478500 the loss is 0.2544665078754782 theta= [[-11.52560264]\n",
      " [  0.09751622]\n",
      " [  0.09138935]]\n",
      "the iterations of  478600 the loss is 0.2544553880129769 theta= [[-11.52665713]\n",
      " [  0.09752456]\n",
      " [  0.09139779]]\n",
      "the iterations of  478700 the loss is 0.25444427213840465 theta= [[-11.52771143]\n",
      " [  0.09753291]\n",
      " [  0.09140622]]\n",
      "the iterations of  478800 the loss is 0.25443316024962404 theta= [[-11.52876554]\n",
      " [  0.09754126]\n",
      " [  0.09141465]]\n",
      "the iterations of  478900 the loss is 0.25442205234449944 theta= [[-11.52981947]\n",
      " [  0.0975496 ]\n",
      " [  0.09142308]]\n",
      "the iterations of  479000 the loss is 0.25441094842089657 theta= [[-11.5308732 ]\n",
      " [  0.09755794]\n",
      " [  0.09143151]]\n",
      "the iterations of  479100 the loss is 0.2543998484766831 theta= [[-11.53192674]\n",
      " [  0.09756629]\n",
      " [  0.09143994]]\n",
      "the iterations of  479200 the loss is 0.25438875250972787 theta= [[-11.5329801 ]\n",
      " [  0.09757463]\n",
      " [  0.09144836]]\n",
      "the iterations of  479300 the loss is 0.2543776605179015 theta= [[-11.53403327]\n",
      " [  0.09758296]\n",
      " [  0.09145679]]\n",
      "the iterations of  479400 the loss is 0.25436657249907596 theta= [[-11.53508624]\n",
      " [  0.0975913 ]\n",
      " [  0.09146521]]\n",
      "the iterations of  479500 the loss is 0.2543554884511247 theta= [[-11.53613903]\n",
      " [  0.09759964]\n",
      " [  0.09147363]]\n",
      "the iterations of  479600 the loss is 0.25434440837192324 theta= [[-11.53719163]\n",
      " [  0.09760797]\n",
      " [  0.09148205]]\n",
      "the iterations of  479700 the loss is 0.25433333225934784 theta= [[-11.53824405]\n",
      " [  0.0976163 ]\n",
      " [  0.09149047]]\n",
      "the iterations of  479800 the loss is 0.25432226011127707 theta= [[-11.53929627]\n",
      " [  0.09762463]\n",
      " [  0.09149889]]\n",
      "the iterations of  479900 the loss is 0.2543111919255904 theta= [[-11.54034831]\n",
      " [  0.09763296]\n",
      " [  0.0915073 ]]\n",
      "the iterations of  480000 the loss is 0.25430012770016897 theta= [[-11.54140015]\n",
      " [  0.09764129]\n",
      " [  0.09151572]]\n",
      "the iterations of  480100 the loss is 0.25428906743289587 theta= [[-11.54245181]\n",
      " [  0.09764962]\n",
      " [  0.09152413]]\n",
      "the iterations of  480200 the loss is 0.25427801112165527 theta= [[-11.54350329]\n",
      " [  0.09765794]\n",
      " [  0.09153254]]\n",
      "the iterations of  480300 the loss is 0.2542669587643331 theta= [[-11.54455457]\n",
      " [  0.09766627]\n",
      " [  0.09154095]]\n",
      "the iterations of  480400 the loss is 0.2542559103588166 theta= [[-11.54560566]\n",
      " [  0.09767459]\n",
      " [  0.09154936]]\n",
      "the iterations of  480500 the loss is 0.25424486590299494 theta= [[-11.54665657]\n",
      " [  0.09768291]\n",
      " [  0.09155776]]\n",
      "the iterations of  480600 the loss is 0.25423382539475836 theta= [[-11.54770729]\n",
      " [  0.09769123]\n",
      " [  0.09156617]]\n",
      "the iterations of  480700 the loss is 0.25422278883199895 theta= [[-11.54875782]\n",
      " [  0.09769955]\n",
      " [  0.09157457]]\n",
      "the iterations of  480800 the loss is 0.25421175621261016 theta= [[-11.54980817]\n",
      " [  0.09770786]\n",
      " [  0.09158297]]\n",
      "the iterations of  480900 the loss is 0.25420072753448686 theta= [[-11.55085832]\n",
      " [  0.09771618]\n",
      " [  0.09159137]]\n",
      "the iterations of  481000 the loss is 0.25418970279552566 theta= [[-11.55190829]\n",
      " [  0.09772449]\n",
      " [  0.09159977]]\n",
      "the iterations of  481100 the loss is 0.25417868199362453 theta= [[-11.55295808]\n",
      " [  0.0977328 ]\n",
      " [  0.09160817]]\n",
      "the iterations of  481200 the loss is 0.2541676651266832 theta= [[-11.55400767]\n",
      " [  0.09774111]\n",
      " [  0.09161657]]\n",
      "the iterations of  481300 the loss is 0.25415665219260253 theta= [[-11.55505708]\n",
      " [  0.09774942]\n",
      " [  0.09162496]]\n",
      "the iterations of  481400 the loss is 0.2541456431892852 theta= [[-11.5561063 ]\n",
      " [  0.09775773]\n",
      " [  0.09163335]]\n",
      "the iterations of  481500 the loss is 0.25413463811463527 theta= [[-11.55715533]\n",
      " [  0.09776604]\n",
      " [  0.09164175]]\n",
      "the iterations of  481600 the loss is 0.2541236369665584 theta= [[-11.55820417]\n",
      " [  0.09777434]\n",
      " [  0.09165014]]\n",
      "the iterations of  481700 the loss is 0.2541126397429615 theta= [[-11.55925283]\n",
      " [  0.09778264]\n",
      " [  0.09165852]]\n",
      "the iterations of  481800 the loss is 0.25410164644175326 theta= [[-11.5603013 ]\n",
      " [  0.09779094]\n",
      " [  0.09166691]]\n",
      "the iterations of  481900 the loss is 0.2540906570608438 theta= [[-11.56134959]\n",
      " [  0.09779925]\n",
      " [  0.0916753 ]]\n",
      "the iterations of  482000 the loss is 0.254079671598145 theta= [[-11.56239768]\n",
      " [  0.09780754]\n",
      " [  0.09168368]]\n",
      "the iterations of  482100 the loss is 0.2540686900515696 theta= [[-11.56344559]\n",
      " [  0.09781584]\n",
      " [  0.09169207]]\n",
      "the iterations of  482200 the loss is 0.2540577124190325 theta= [[-11.56449332]\n",
      " [  0.09782414]\n",
      " [  0.09170045]]\n",
      "the iterations of  482300 the loss is 0.2540467386984499 theta= [[-11.56554085]\n",
      " [  0.09783243]\n",
      " [  0.09170883]]\n",
      "the iterations of  482400 the loss is 0.2540357688877391 theta= [[-11.5665882 ]\n",
      " [  0.09784072]\n",
      " [  0.09171721]]\n",
      "the iterations of  482500 the loss is 0.25402480298481955 theta= [[-11.56763537]\n",
      " [  0.09784902]\n",
      " [  0.09172558]]\n",
      "the iterations of  482600 the loss is 0.2540138409876117 theta= [[-11.56868234]\n",
      " [  0.09785731]\n",
      " [  0.09173396]]\n",
      "the iterations of  482700 the loss is 0.2540028828940377 theta= [[-11.56972913]\n",
      " [  0.09786559]\n",
      " [  0.09174233]]\n",
      "the iterations of  482800 the loss is 0.25399192870202125 theta= [[-11.57077574]\n",
      " [  0.09787388]\n",
      " [  0.0917507 ]]\n",
      "the iterations of  482900 the loss is 0.25398097840948725 theta= [[-11.57182216]\n",
      " [  0.09788217]\n",
      " [  0.09175908]]\n",
      "the iterations of  483000 the loss is 0.25397003201436236 theta= [[-11.57286839]\n",
      " [  0.09789045]\n",
      " [  0.09176745]]\n",
      "the iterations of  483100 the loss is 0.25395908951457485 theta= [[-11.57391443]\n",
      " [  0.09789873]\n",
      " [  0.09177581]]\n",
      "the iterations of  483200 the loss is 0.25394815090805417 theta= [[-11.57496029]\n",
      " [  0.09790702]\n",
      " [  0.09178418]]\n",
      "the iterations of  483300 the loss is 0.2539372161927314 theta= [[-11.57600597]\n",
      " [  0.0979153 ]\n",
      " [  0.09179255]]\n",
      "the iterations of  483400 the loss is 0.2539262853665391 theta= [[-11.57705145]\n",
      " [  0.09792357]\n",
      " [  0.09180091]]\n",
      "the iterations of  483500 the loss is 0.25391535842741136 theta= [[-11.57809676]\n",
      " [  0.09793185]\n",
      " [  0.09180927]]\n",
      "the iterations of  483600 the loss is 0.2539044353732835 theta= [[-11.57914187]\n",
      " [  0.09794013]\n",
      " [  0.09181763]]\n",
      "the iterations of  483700 the loss is 0.2538935162020928 theta= [[-11.5801868 ]\n",
      " [  0.0979484 ]\n",
      " [  0.09182599]]\n",
      "the iterations of  483800 the loss is 0.2538826009117775 theta= [[-11.58123155]\n",
      " [  0.09795667]\n",
      " [  0.09183435]]\n",
      "the iterations of  483900 the loss is 0.2538716895002777 theta= [[-11.5822761 ]\n",
      " [  0.09796494]\n",
      " [  0.09184271]]\n",
      "the iterations of  484000 the loss is 0.25386078196553474 theta= [[-11.58332048]\n",
      " [  0.09797321]\n",
      " [  0.09185106]]\n",
      "the iterations of  484100 the loss is 0.25384987830549166 theta= [[-11.58436466]\n",
      " [  0.09798148]\n",
      " [  0.09185941]]\n",
      "the iterations of  484200 the loss is 0.2538389785180928 theta= [[-11.58540867]\n",
      " [  0.09798975]\n",
      " [  0.09186777]]\n",
      "the iterations of  484300 the loss is 0.25382808260128403 theta= [[-11.58645248]\n",
      " [  0.09799801]\n",
      " [  0.09187612]]\n",
      "the iterations of  484400 the loss is 0.2538171905530126 theta= [[-11.58749611]\n",
      " [  0.09800628]\n",
      " [  0.09188447]]\n",
      "the iterations of  484500 the loss is 0.25380630237122737 theta= [[-11.58853956]\n",
      " [  0.09801454]\n",
      " [  0.09189281]]\n",
      "the iterations of  484600 the loss is 0.25379541805387856 theta= [[-11.58958282]\n",
      " [  0.0980228 ]\n",
      " [  0.09190116]]\n",
      "the iterations of  484700 the loss is 0.25378453759891806 theta= [[-11.5906259 ]\n",
      " [  0.09803106]\n",
      " [  0.09190951]]\n",
      "the iterations of  484800 the loss is 0.25377366100429904 theta= [[-11.59166879]\n",
      " [  0.09803932]\n",
      " [  0.09191785]]\n",
      "the iterations of  484900 the loss is 0.2537627882679762 theta= [[-11.59271149]\n",
      " [  0.09804758]\n",
      " [  0.09192619]]\n",
      "the iterations of  485000 the loss is 0.2537519193879055 theta= [[-11.59375401]\n",
      " [  0.09805583]\n",
      " [  0.09193453]]\n",
      "the iterations of  485100 the loss is 0.2537410543620446 theta= [[-11.59479635]\n",
      " [  0.09806409]\n",
      " [  0.09194287]]\n",
      "the iterations of  485200 the loss is 0.25373019318835266 theta= [[-11.5958385 ]\n",
      " [  0.09807234]\n",
      " [  0.09195121]]\n",
      "the iterations of  485300 the loss is 0.25371933586479023 theta= [[-11.59688047]\n",
      " [  0.09808059]\n",
      " [  0.09195954]]\n",
      "the iterations of  485400 the loss is 0.25370848238931926 theta= [[-11.59792225]\n",
      " [  0.09808884]\n",
      " [  0.09196788]]\n",
      "the iterations of  485500 the loss is 0.25369763275990304 theta= [[-11.59896384]\n",
      " [  0.09809709]\n",
      " [  0.09197621]]\n",
      "the iterations of  485600 the loss is 0.2536867869745067 theta= [[-11.60000526]\n",
      " [  0.09810533]\n",
      " [  0.09198454]]\n",
      "the iterations of  485700 the loss is 0.25367594503109653 theta= [[-11.60104649]\n",
      " [  0.09811358]\n",
      " [  0.09199287]]\n",
      "the iterations of  485800 the loss is 0.2536651069276402 theta= [[-11.60208753]\n",
      " [  0.09812182]\n",
      " [  0.0920012 ]]\n",
      "the iterations of  485900 the loss is 0.25365427266210727 theta= [[-11.60312839]\n",
      " [  0.09813006]\n",
      " [  0.09200953]]\n",
      "the iterations of  486000 the loss is 0.25364344223246826 theta= [[-11.60416906]\n",
      " [  0.09813831]\n",
      " [  0.09201786]]\n",
      "the iterations of  486100 the loss is 0.2536326156366953 theta= [[-11.60520955]\n",
      " [  0.09814654]\n",
      " [  0.09202618]]\n",
      "the iterations of  486200 the loss is 0.25362179287276204 theta= [[-11.60624986]\n",
      " [  0.09815478]\n",
      " [  0.0920345 ]]\n",
      "the iterations of  486300 the loss is 0.2536109739386436 theta= [[-11.60728998]\n",
      " [  0.09816302]\n",
      " [  0.09204283]]\n",
      "the iterations of  486400 the loss is 0.25360015883231635 theta= [[-11.60832992]\n",
      " [  0.09817125]\n",
      " [  0.09205115]]\n",
      "the iterations of  486500 the loss is 0.25358934755175844 theta= [[-11.60936968]\n",
      " [  0.09817949]\n",
      " [  0.09205946]]\n",
      "the iterations of  486600 the loss is 0.25357854009494935 theta= [[-11.61040925]\n",
      " [  0.09818772]\n",
      " [  0.09206778]]\n",
      "the iterations of  486700 the loss is 0.25356773645986946 theta= [[-11.61144863]\n",
      " [  0.09819595]\n",
      " [  0.0920761 ]]\n",
      "the iterations of  486800 the loss is 0.25355693664450135 theta= [[-11.61248784]\n",
      " [  0.09820418]\n",
      " [  0.09208441]]\n",
      "the iterations of  486900 the loss is 0.2535461406468287 theta= [[-11.61352686]\n",
      " [  0.09821241]\n",
      " [  0.09209272]]\n",
      "the iterations of  487000 the loss is 0.25353534846483666 theta= [[-11.61456569]\n",
      " [  0.09822063]\n",
      " [  0.09210104]]\n",
      "the iterations of  487100 the loss is 0.2535245600965119 theta= [[-11.61560434]\n",
      " [  0.09822886]\n",
      " [  0.09210935]]\n",
      "the iterations of  487200 the loss is 0.25351377553984233 theta= [[-11.61664281]\n",
      " [  0.09823708]\n",
      " [  0.09211765]]\n",
      "the iterations of  487300 the loss is 0.25350299479281746 theta= [[-11.6176811 ]\n",
      " [  0.09824531]\n",
      " [  0.09212596]]\n",
      "the iterations of  487400 the loss is 0.2534922178534284 theta= [[-11.6187192 ]\n",
      " [  0.09825353]\n",
      " [  0.09213427]]\n",
      "the iterations of  487500 the loss is 0.25348144471966716 theta= [[-11.61975712]\n",
      " [  0.09826175]\n",
      " [  0.09214257]]\n",
      "the iterations of  487600 the loss is 0.2534706753895276 theta= [[-11.62079485]\n",
      " [  0.09826996]\n",
      " [  0.09215087]]\n",
      "the iterations of  487700 the loss is 0.25345990986100486 theta= [[-11.62183241]\n",
      " [  0.09827818]\n",
      " [  0.09215918]]\n",
      "the iterations of  487800 the loss is 0.25344914813209557 theta= [[-11.62286977]\n",
      " [  0.0982864 ]\n",
      " [  0.09216748]]\n",
      "the iterations of  487900 the loss is 0.2534383902007979 theta= [[-11.62390696]\n",
      " [  0.09829461]\n",
      " [  0.09217577]]\n",
      "the iterations of  488000 the loss is 0.2534276360651113 theta= [[-11.62494396]\n",
      " [  0.09830282]\n",
      " [  0.09218407]]\n",
      "the iterations of  488100 the loss is 0.2534168857230365 theta= [[-11.62598078]\n",
      " [  0.09831103]\n",
      " [  0.09219237]]\n",
      "the iterations of  488200 the loss is 0.25340613917257593 theta= [[-11.62701742]\n",
      " [  0.09831924]\n",
      " [  0.09220066]]\n",
      "the iterations of  488300 the loss is 0.2533953964117335 theta= [[-11.62805388]\n",
      " [  0.09832745]\n",
      " [  0.09220895]]\n",
      "the iterations of  488400 the loss is 0.25338465743851396 theta= [[-11.62909015]\n",
      " [  0.09833566]\n",
      " [  0.09221724]]\n",
      "the iterations of  488500 the loss is 0.25337392225092414 theta= [[-11.63012624]\n",
      " [  0.09834386]\n",
      " [  0.09222553]]\n",
      "the iterations of  488600 the loss is 0.2533631908469722 theta= [[-11.63116214]\n",
      " [  0.09835206]\n",
      " [  0.09223382]]\n",
      "the iterations of  488700 the loss is 0.2533524632246672 theta= [[-11.63219787]\n",
      " [  0.09836027]\n",
      " [  0.09224211]]\n",
      "the iterations of  488800 the loss is 0.25334173938202026 theta= [[-11.63323341]\n",
      " [  0.09836847]\n",
      " [  0.09225039]]\n",
      "the iterations of  488900 the loss is 0.2533310193170435 theta= [[-11.63426877]\n",
      " [  0.09837667]\n",
      " [  0.09225868]]\n",
      "the iterations of  489000 the loss is 0.25332030302775055 theta= [[-11.63530394]\n",
      " [  0.09838487]\n",
      " [  0.09226696]]\n",
      "the iterations of  489100 the loss is 0.25330959051215646 theta= [[-11.63633894]\n",
      " [  0.09839306]\n",
      " [  0.09227524]]\n",
      "the iterations of  489200 the loss is 0.25329888176827775 theta= [[-11.63737375]\n",
      " [  0.09840126]\n",
      " [  0.09228352]]\n",
      "the iterations of  489300 the loss is 0.25328817679413224 theta= [[-11.63840838]\n",
      " [  0.09840945]\n",
      " [  0.0922918 ]]\n",
      "the iterations of  489400 the loss is 0.25327747558773944 theta= [[-11.63944283]\n",
      " [  0.09841764]\n",
      " [  0.09230008]]\n",
      "the iterations of  489500 the loss is 0.25326677814711973 theta= [[-11.64047709]\n",
      " [  0.09842583]\n",
      " [  0.09230835]]\n",
      "the iterations of  489600 the loss is 0.25325608447029535 theta= [[-11.64151118]\n",
      " [  0.09843402]\n",
      " [  0.09231663]]\n",
      "the iterations of  489700 the loss is 0.2532453945552899 theta= [[-11.64254508]\n",
      " [  0.09844221]\n",
      " [  0.0923249 ]]\n",
      "the iterations of  489800 the loss is 0.25323470840012813 theta= [[-11.6435788 ]\n",
      " [  0.0984504 ]\n",
      " [  0.09233317]]\n",
      "the iterations of  489900 the loss is 0.2532240260028364 theta= [[-11.64461234]\n",
      " [  0.09845858]\n",
      " [  0.09234144]]\n",
      "the iterations of  490000 the loss is 0.2532133473614425 theta= [[-11.64564569]\n",
      " [  0.09846677]\n",
      " [  0.09234971]]\n",
      "the iterations of  490100 the loss is 0.2532026724739756 theta= [[-11.64667887]\n",
      " [  0.09847495]\n",
      " [  0.09235798]]\n",
      "the iterations of  490200 the loss is 0.253192001338466 theta= [[-11.64771186]\n",
      " [  0.09848313]\n",
      " [  0.09236624]]\n",
      "the iterations of  490300 the loss is 0.25318133395294573 theta= [[-11.64874467]\n",
      " [  0.09849131]\n",
      " [  0.09237451]]\n",
      "the iterations of  490400 the loss is 0.2531706703154481 theta= [[-11.6497773 ]\n",
      " [  0.09849949]\n",
      " [  0.09238277]]\n",
      "the iterations of  490500 the loss is 0.2531600104240077 theta= [[-11.65080975]\n",
      " [  0.09850766]\n",
      " [  0.09239103]]\n",
      "the iterations of  490600 the loss is 0.2531493542766608 theta= [[-11.65184202]\n",
      " [  0.09851584]\n",
      " [  0.09239929]]\n",
      "the iterations of  490700 the loss is 0.25313870187144466 theta= [[-11.65287411]\n",
      " [  0.09852401]\n",
      " [  0.09240755]]\n",
      "the iterations of  490800 the loss is 0.25312805320639825 theta= [[-11.65390601]\n",
      " [  0.09853219]\n",
      " [  0.0924158 ]]\n",
      "the iterations of  490900 the loss is 0.2531174082795619 theta= [[-11.65493774]\n",
      " [  0.09854036]\n",
      " [  0.09242406]]\n",
      "the iterations of  491000 the loss is 0.2531067670889771 theta= [[-11.65596928]\n",
      " [  0.09854853]\n",
      " [  0.09243231]]\n",
      "the iterations of  491100 the loss is 0.2530961296326869 theta= [[-11.65700064]\n",
      " [  0.09855669]\n",
      " [  0.09244057]]\n",
      "the iterations of  491200 the loss is 0.25308549590873586 theta= [[-11.65803182]\n",
      " [  0.09856486]\n",
      " [  0.09244882]]\n",
      "the iterations of  491300 the loss is 0.2530748659151697 theta= [[-11.65906282]\n",
      " [  0.09857303]\n",
      " [  0.09245707]]\n",
      "the iterations of  491400 the loss is 0.25306423965003566 theta= [[-11.66009364]\n",
      " [  0.09858119]\n",
      " [  0.09246532]]\n",
      "the iterations of  491500 the loss is 0.25305361711138213 theta= [[-11.66112428]\n",
      " [  0.09858935]\n",
      " [  0.09247356]]\n",
      "the iterations of  491600 the loss is 0.2530429982972592 theta= [[-11.66215474]\n",
      " [  0.09859751]\n",
      " [  0.09248181]]\n",
      "the iterations of  491700 the loss is 0.253032383205718 theta= [[-11.66318502]\n",
      " [  0.09860567]\n",
      " [  0.09249005]]\n",
      "the iterations of  491800 the loss is 0.25302177183481167 theta= [[-11.66421511]\n",
      " [  0.09861383]\n",
      " [  0.09249829]]\n",
      "the iterations of  491900 the loss is 0.25301116418259395 theta= [[-11.66524503]\n",
      " [  0.09862199]\n",
      " [  0.09250654]]\n",
      "the iterations of  492000 the loss is 0.25300056024712037 theta= [[-11.66627476]\n",
      " [  0.09863014]\n",
      " [  0.09251478]]\n",
      "the iterations of  492100 the loss is 0.25298996002644764 theta= [[-11.66730432]\n",
      " [  0.0986383 ]\n",
      " [  0.09252301]]\n",
      "the iterations of  492200 the loss is 0.252979363518634 theta= [[-11.66833369]\n",
      " [  0.09864645]\n",
      " [  0.09253125]]\n",
      "the iterations of  492300 the loss is 0.2529687707217394 theta= [[-11.66936289]\n",
      " [  0.0986546 ]\n",
      " [  0.09253949]]\n",
      "the iterations of  492400 the loss is 0.25295818163382433 theta= [[-11.6703919 ]\n",
      " [  0.09866275]\n",
      " [  0.09254772]]\n",
      "the iterations of  492500 the loss is 0.25294759625295116 theta= [[-11.67142074]\n",
      " [  0.0986709 ]\n",
      " [  0.09255595]]\n",
      "the iterations of  492600 the loss is 0.2529370145771836 theta= [[-11.67244939]\n",
      " [  0.09867905]\n",
      " [  0.09256418]]\n",
      "the iterations of  492700 the loss is 0.2529264366045869 theta= [[-11.67347787]\n",
      " [  0.09868719]\n",
      " [  0.09257241]]\n",
      "the iterations of  492800 the loss is 0.25291586233322716 theta= [[-11.67450616]\n",
      " [  0.09869534]\n",
      " [  0.09258064]]\n",
      "the iterations of  492900 the loss is 0.25290529176117227 theta= [[-11.67553427]\n",
      " [  0.09870348]\n",
      " [  0.09258887]]\n",
      "the iterations of  493000 the loss is 0.2528947248864915 theta= [[-11.67656221]\n",
      " [  0.09871162]\n",
      " [  0.09259709]]\n",
      "the iterations of  493100 the loss is 0.25288416170725536 theta= [[-11.67758996]\n",
      " [  0.09871976]\n",
      " [  0.09260532]]\n",
      "the iterations of  493200 the loss is 0.25287360222153543 theta= [[-11.67861754]\n",
      " [  0.0987279 ]\n",
      " [  0.09261354]]\n",
      "the iterations of  493300 the loss is 0.2528630464274053 theta= [[-11.67964493]\n",
      " [  0.09873604]\n",
      " [  0.09262176]]\n",
      "the iterations of  493400 the loss is 0.2528524943229393 theta= [[-11.68067215]\n",
      " [  0.09874417]\n",
      " [  0.09262998]]\n",
      "the iterations of  493500 the loss is 0.25284194590621345 theta= [[-11.68169919]\n",
      " [  0.09875231]\n",
      " [  0.0926382 ]]\n",
      "the iterations of  493600 the loss is 0.25283140117530495 theta= [[-11.68272604]\n",
      " [  0.09876044]\n",
      " [  0.09264642]]\n",
      "the iterations of  493700 the loss is 0.2528208601282926 theta= [[-11.68375272]\n",
      " [  0.09876857]\n",
      " [  0.09265463]]\n",
      "the iterations of  493800 the loss is 0.25281032276325627 theta= [[-11.68477922]\n",
      " [  0.0987767 ]\n",
      " [  0.09266285]]\n",
      "the iterations of  493900 the loss is 0.2527997890782774 theta= [[-11.68580554]\n",
      " [  0.09878483]\n",
      " [  0.09267106]]\n",
      "the iterations of  494000 the loss is 0.25278925907143857 theta= [[-11.68683168]\n",
      " [  0.09879296]\n",
      " [  0.09267927]]\n",
      "the iterations of  494100 the loss is 0.25277873274082396 theta= [[-11.68785764]\n",
      " [  0.09880109]\n",
      " [  0.09268748]]\n",
      "the iterations of  494200 the loss is 0.25276821008451894 theta= [[-11.68888342]\n",
      " [  0.09880921]\n",
      " [  0.09269569]]\n",
      "the iterations of  494300 the loss is 0.25275769110061036 theta= [[-11.68990902]\n",
      " [  0.09881733]\n",
      " [  0.0927039 ]]\n",
      "the iterations of  494400 the loss is 0.2527471757871861 theta= [[-11.69093445]\n",
      " [  0.09882546]\n",
      " [  0.0927121 ]]\n",
      "the iterations of  494500 the loss is 0.25273666414233575 theta= [[-11.69195969]\n",
      " [  0.09883358]\n",
      " [  0.09272031]]\n",
      "the iterations of  494600 the loss is 0.2527261561641501 theta= [[-11.69298476]\n",
      " [  0.09884169]\n",
      " [  0.09272851]]\n",
      "the iterations of  494700 the loss is 0.25271565185072115 theta= [[-11.69400964]\n",
      " [  0.09884981]\n",
      " [  0.09273671]]\n",
      "the iterations of  494800 the loss is 0.2527051512001426 theta= [[-11.69503435]\n",
      " [  0.09885793]\n",
      " [  0.09274491]]\n",
      "the iterations of  494900 the loss is 0.252694654210509 theta= [[-11.69605888]\n",
      " [  0.09886604]\n",
      " [  0.09275311]]\n",
      "the iterations of  495000 the loss is 0.25268416087991674 theta= [[-11.69708323]\n",
      " [  0.09887416]\n",
      " [  0.09276131]]\n",
      "the iterations of  495100 the loss is 0.2526736712064631 theta= [[-11.6981074 ]\n",
      " [  0.09888227]\n",
      " [  0.09276951]]\n",
      "the iterations of  495200 the loss is 0.2526631851882471 theta= [[-11.6991314 ]\n",
      " [  0.09889038]\n",
      " [  0.0927777 ]]\n",
      "the iterations of  495300 the loss is 0.25265270282336877 theta= [[-11.70015521]\n",
      " [  0.09889849]\n",
      " [  0.09278589]]\n",
      "the iterations of  495400 the loss is 0.2526422241099295 theta= [[-11.70117885]\n",
      " [  0.0989066 ]\n",
      " [  0.09279409]]\n",
      "the iterations of  495500 the loss is 0.2526317490460325 theta= [[-11.70220231]\n",
      " [  0.0989147 ]\n",
      " [  0.09280228]]\n",
      "the iterations of  495600 the loss is 0.2526212776297816 theta= [[-11.70322559]\n",
      " [  0.09892281]\n",
      " [  0.09281046]]\n",
      "the iterations of  495700 the loss is 0.2526108098592823 theta= [[-11.70424869]\n",
      " [  0.09893091]\n",
      " [  0.09281865]]\n",
      "the iterations of  495800 the loss is 0.25260034573264156 theta= [[-11.70527162]\n",
      " [  0.09893901]\n",
      " [  0.09282684]]\n",
      "the iterations of  495900 the loss is 0.2525898852479672 theta= [[-11.70629437]\n",
      " [  0.09894712]\n",
      " [  0.09283502]]\n",
      "the iterations of  496000 the loss is 0.2525794284033691 theta= [[-11.70731693]\n",
      " [  0.09895521]\n",
      " [  0.09284321]]\n",
      "the iterations of  496100 the loss is 0.25256897519695803 theta= [[-11.70833932]\n",
      " [  0.09896331]\n",
      " [  0.09285139]]\n",
      "the iterations of  496200 the loss is 0.25255852562684566 theta= [[-11.70936154]\n",
      " [  0.09897141]\n",
      " [  0.09285957]]\n",
      "the iterations of  496300 the loss is 0.252548079691146 theta= [[-11.71038357]\n",
      " [  0.0989795 ]\n",
      " [  0.09286775]]\n",
      "the iterations of  496400 the loss is 0.2525376373879736 theta= [[-11.71140543]\n",
      " [  0.0989876 ]\n",
      " [  0.09287593]]\n",
      "the iterations of  496500 the loss is 0.25252719871544466 theta= [[-11.71242711]\n",
      " [  0.09899569]\n",
      " [  0.0928841 ]]\n",
      "the iterations of  496600 the loss is 0.25251676367167647 theta= [[-11.71344861]\n",
      " [  0.09900378]\n",
      " [  0.09289228]]\n",
      "the iterations of  496700 the loss is 0.25250633225478764 theta= [[-11.71446994]\n",
      " [  0.09901187]\n",
      " [  0.09290045]]\n",
      "the iterations of  496800 the loss is 0.2524959044628985 theta= [[-11.71549108]\n",
      " [  0.09901996]\n",
      " [  0.09290862]]\n",
      "the iterations of  496900 the loss is 0.25248548029413037 theta= [[-11.71651205]\n",
      " [  0.09902805]\n",
      " [  0.09291679]]\n",
      "the iterations of  497000 the loss is 0.25247505974660567 theta= [[-11.71753285]\n",
      " [  0.09903613]\n",
      " [  0.09292496]]\n",
      "the iterations of  497100 the loss is 0.25246464281844877 theta= [[-11.71855346]\n",
      " [  0.09904422]\n",
      " [  0.09293313]]\n",
      "the iterations of  497200 the loss is 0.25245422950778473 theta= [[-11.7195739]\n",
      " [  0.0990523]\n",
      " [  0.0929413]]\n",
      "the iterations of  497300 the loss is 0.2524438198127402 theta= [[-11.72059416]\n",
      " [  0.09906038]\n",
      " [  0.09294946]]\n",
      "the iterations of  497400 the loss is 0.2524334137314432 theta= [[-11.72161424]\n",
      " [  0.09906846]\n",
      " [  0.09295763]]\n",
      "the iterations of  497500 the loss is 0.2524230112620229 theta= [[-11.72263415]\n",
      " [  0.09907654]\n",
      " [  0.09296579]]\n",
      "the iterations of  497600 the loss is 0.2524126124026098 theta= [[-11.72365388]\n",
      " [  0.09908462]\n",
      " [  0.09297395]]\n",
      "the iterations of  497700 the loss is 0.2524022171513361 theta= [[-11.72467343]\n",
      " [  0.09909269]\n",
      " [  0.09298211]]\n",
      "the iterations of  497800 the loss is 0.2523918255063346 theta= [[-11.72569281]\n",
      " [  0.09910077]\n",
      " [  0.09299027]]\n",
      "the iterations of  497900 the loss is 0.25238143746574 theta= [[-11.72671201]\n",
      " [  0.09910884]\n",
      " [  0.09299842]]\n",
      "the iterations of  498000 the loss is 0.2523710530276879 theta= [[-11.72773103]\n",
      " [  0.09911691]\n",
      " [  0.09300658]]\n",
      "the iterations of  498100 the loss is 0.2523606721903154 theta= [[-11.72874987]\n",
      " [  0.09912498]\n",
      " [  0.09301473]]\n",
      "the iterations of  498200 the loss is 0.25235029495176087 theta= [[-11.72976854]\n",
      " [  0.09913305]\n",
      " [  0.09302289]]\n",
      "the iterations of  498300 the loss is 0.2523399213101642 theta= [[-11.73078703]\n",
      " [  0.09914112]\n",
      " [  0.09303104]]\n",
      "the iterations of  498400 the loss is 0.25232955126366613 theta= [[-11.73180535]\n",
      " [  0.09914919]\n",
      " [  0.09303919]]\n",
      "the iterations of  498500 the loss is 0.2523191848104091 theta= [[-11.73282349]\n",
      " [  0.09915725]\n",
      " [  0.09304734]]\n",
      "the iterations of  498600 the loss is 0.25230882194853665 theta= [[-11.73384145]\n",
      " [  0.09916532]\n",
      " [  0.09305548]]\n",
      "the iterations of  498700 the loss is 0.25229846267619366 theta= [[-11.73485924]\n",
      " [  0.09917338]\n",
      " [  0.09306363]]\n",
      "the iterations of  498800 the loss is 0.2522881069915262 theta= [[-11.73587685]\n",
      " [  0.09918144]\n",
      " [  0.09307177]]\n",
      "the iterations of  498900 the loss is 0.2522777548926819 theta= [[-11.73689428]\n",
      " [  0.0991895 ]\n",
      " [  0.09307992]]\n",
      "the iterations of  499000 the loss is 0.25226740637780926 theta= [[-11.73791154]\n",
      " [  0.09919756]\n",
      " [  0.09308806]]\n",
      "the iterations of  499100 the loss is 0.2522570614450588 theta= [[-11.73892862]\n",
      " [  0.09920561]\n",
      " [  0.0930962 ]]\n",
      "the iterations of  499200 the loss is 0.2522467200925813 theta= [[-11.73994553]\n",
      " [  0.09921367]\n",
      " [  0.09310434]]\n",
      "the iterations of  499300 the loss is 0.2522363823185297 theta= [[-11.74096226]\n",
      " [  0.09922172]\n",
      " [  0.09311247]]\n",
      "the iterations of  499400 the loss is 0.2522260481210577 theta= [[-11.74197881]\n",
      " [  0.09922978]\n",
      " [  0.09312061]]\n",
      "the iterations of  499500 the loss is 0.252215717498321 theta= [[-11.74299519]\n",
      " [  0.09923783]\n",
      " [  0.09312874]]\n",
      "the iterations of  499600 the loss is 0.2522053904484757 theta= [[-11.74401139]\n",
      " [  0.09924588]\n",
      " [  0.09313688]]\n",
      "the iterations of  499700 the loss is 0.2521950669696793 theta= [[-11.74502742]\n",
      " [  0.09925392]\n",
      " [  0.09314501]]\n",
      "the iterations of  499800 the loss is 0.2521847470600913 theta= [[-11.74604327]\n",
      " [  0.09926197]\n",
      " [  0.09315314]]\n",
      "the iterations of  499900 the loss is 0.252174430717872 theta= [[-11.74705895]\n",
      " [  0.09927002]\n",
      " [  0.09316127]]\n",
      "the iterations of  500000 the loss is 0.2521641179411829 theta= [[-11.74807445]\n",
      " [  0.09927806]\n",
      " [  0.0931694 ]]\n",
      "the iterations of  500100 the loss is 0.2521538087281868 theta= [[-11.74908977]\n",
      " [  0.09928611]\n",
      " [  0.09317752]]\n",
      "the iterations of  500200 the loss is 0.25214350307704814 theta= [[-11.75010492]\n",
      " [  0.09929415]\n",
      " [  0.09318565]]\n",
      "the iterations of  500300 the loss is 0.25213320098593217 theta= [[-11.75111989]\n",
      " [  0.09930219]\n",
      " [  0.09319377]]\n",
      "the iterations of  500400 the loss is 0.25212290245300584 theta= [[-11.75213469]\n",
      " [  0.09931023]\n",
      " [  0.09320189]]\n",
      "the iterations of  500500 the loss is 0.25211260747643693 theta= [[-11.75314932]\n",
      " [  0.09931826]\n",
      " [  0.09321001]]\n",
      "the iterations of  500600 the loss is 0.2521023160543949 theta= [[-11.75416376]\n",
      " [  0.0993263 ]\n",
      " [  0.09321813]]\n",
      "the iterations of  500700 the loss is 0.2520920281850501 theta= [[-11.75517804]\n",
      " [  0.09933433]\n",
      " [  0.09322625]]\n",
      "the iterations of  500800 the loss is 0.25208174386657467 theta= [[-11.75619213]\n",
      " [  0.09934237]\n",
      " [  0.09323437]]\n",
      "the iterations of  500900 the loss is 0.2520714630971413 theta= [[-11.75720606]\n",
      " [  0.0993504 ]\n",
      " [  0.09324248]]\n",
      "the iterations of  501000 the loss is 0.2520611858749246 theta= [[-11.7582198 ]\n",
      " [  0.09935843]\n",
      " [  0.0932506 ]]\n",
      "the iterations of  501100 the loss is 0.2520509121981003 theta= [[-11.75923338]\n",
      " [  0.09936646]\n",
      " [  0.09325871]]\n",
      "the iterations of  501200 the loss is 0.2520406420648451 theta= [[-11.76024678]\n",
      " [  0.09937449]\n",
      " [  0.09326682]]\n",
      "the iterations of  501300 the loss is 0.25203037547333734 theta= [[-11.76126   ]\n",
      " [  0.09938251]\n",
      " [  0.09327493]]\n",
      "the iterations of  501400 the loss is 0.2520201124217565 theta= [[-11.76227305]\n",
      " [  0.09939054]\n",
      " [  0.09328304]]\n",
      "the iterations of  501500 the loss is 0.252009852908283 theta= [[-11.76328592]\n",
      " [  0.09939856]\n",
      " [  0.09329114]]\n",
      "the iterations of  501600 the loss is 0.2519995969310992 theta= [[-11.76429862]\n",
      " [  0.09940659]\n",
      " [  0.09329925]]\n",
      "the iterations of  501700 the loss is 0.251989344488388 theta= [[-11.76531115]\n",
      " [  0.09941461]\n",
      " [  0.09330735]]\n",
      "the iterations of  501800 the loss is 0.2519790955783341 theta= [[-11.7663235 ]\n",
      " [  0.09942263]\n",
      " [  0.09331546]]\n",
      "the iterations of  501900 the loss is 0.2519688501991231 theta= [[-11.76733567]\n",
      " [  0.09943064]\n",
      " [  0.09332356]]\n",
      "the iterations of  502000 the loss is 0.25195860834894207 theta= [[-11.76834767]\n",
      " [  0.09943866]\n",
      " [  0.09333166]]\n",
      "the iterations of  502100 the loss is 0.25194837002597925 theta= [[-11.7693595 ]\n",
      " [  0.09944668]\n",
      " [  0.09333976]]\n",
      "the iterations of  502200 the loss is 0.25193813522842423 theta= [[-11.77037116]\n",
      " [  0.09945469]\n",
      " [  0.09334785]]\n",
      "the iterations of  502300 the loss is 0.2519279039544678 theta= [[-11.77138263]\n",
      " [  0.0994627 ]\n",
      " [  0.09335595]]\n",
      "the iterations of  502400 the loss is 0.25191767620230204 theta= [[-11.77239394]\n",
      " [  0.09947072]\n",
      " [  0.09336405]]\n",
      "the iterations of  502500 the loss is 0.2519074519701201 theta= [[-11.77340507]\n",
      " [  0.09947873]\n",
      " [  0.09337214]]\n",
      "the iterations of  502600 the loss is 0.2518972312561166 theta= [[-11.77441603]\n",
      " [  0.09948674]\n",
      " [  0.09338023]]\n",
      "the iterations of  502700 the loss is 0.2518870140584874 theta= [[-11.77542681]\n",
      " [  0.09949474]\n",
      " [  0.09338832]]\n",
      "the iterations of  502800 the loss is 0.2518768003754294 theta= [[-11.77643742]\n",
      " [  0.09950275]\n",
      " [  0.09339641]]\n",
      "the iterations of  502900 the loss is 0.25186659020514096 theta= [[-11.77744785]\n",
      " [  0.09951075]\n",
      " [  0.0934045 ]]\n",
      "the iterations of  503000 the loss is 0.2518563835458217 theta= [[-11.77845812]\n",
      " [  0.09951876]\n",
      " [  0.09341258]]\n",
      "the iterations of  503100 the loss is 0.2518461803956723 theta= [[-11.7794682 ]\n",
      " [  0.09952676]\n",
      " [  0.09342067]]\n",
      "the iterations of  503200 the loss is 0.2518359807528949 theta= [[-11.78047812]\n",
      " [  0.09953476]\n",
      " [  0.09342875]]\n",
      "the iterations of  503300 the loss is 0.25182578461569266 theta= [[-11.78148786]\n",
      " [  0.09954276]\n",
      " [  0.09343683]]\n",
      "the iterations of  503400 the loss is 0.2518155919822701 theta= [[-11.78249743]\n",
      " [  0.09955076]\n",
      " [  0.09344492]]\n",
      "the iterations of  503500 the loss is 0.2518054028508332 theta= [[-11.78350682]\n",
      " [  0.09955875]\n",
      " [  0.093453  ]]\n",
      "the iterations of  503600 the loss is 0.2517952172195887 theta= [[-11.78451604]\n",
      " [  0.09956675]\n",
      " [  0.09346107]]\n",
      "the iterations of  503700 the loss is 0.25178503508674493 theta= [[-11.78552509]\n",
      " [  0.09957474]\n",
      " [  0.09346915]]\n",
      "the iterations of  503800 the loss is 0.2517748564505114 theta= [[-11.78653396]\n",
      " [  0.09958274]\n",
      " [  0.09347723]]\n",
      "the iterations of  503900 the loss is 0.251764681309099 theta= [[-11.78754266]\n",
      " [  0.09959073]\n",
      " [  0.0934853 ]]\n",
      "the iterations of  504000 the loss is 0.2517545096607194 theta= [[-11.78855119]\n",
      " [  0.09959872]\n",
      " [  0.09349337]]\n",
      "the iterations of  504100 the loss is 0.25174434150358593 theta= [[-11.78955954]\n",
      " [  0.09960671]\n",
      " [  0.09350144]]\n",
      "the iterations of  504200 the loss is 0.2517341768359131 theta= [[-11.79056772]\n",
      " [  0.09961469]\n",
      " [  0.09350951]]\n",
      "the iterations of  504300 the loss is 0.2517240156559165 theta= [[-11.79157573]\n",
      " [  0.09962268]\n",
      " [  0.09351758]]\n",
      "the iterations of  504400 the loss is 0.25171385796181306 theta= [[-11.79258357]\n",
      " [  0.09963066]\n",
      " [  0.09352565]]\n",
      "the iterations of  504500 the loss is 0.25170370375182083 theta= [[-11.79359123]\n",
      " [  0.09963865]\n",
      " [  0.09353372]]\n",
      "the iterations of  504600 the loss is 0.25169355302415924 theta= [[-11.79459872]\n",
      " [  0.09964663]\n",
      " [  0.09354178]]\n",
      "the iterations of  504700 the loss is 0.25168340577704895 theta= [[-11.79560604]\n",
      " [  0.09965461]\n",
      " [  0.09354984]]\n",
      "the iterations of  504800 the loss is 0.25167326200871165 theta= [[-11.79661318]\n",
      " [  0.09966259]\n",
      " [  0.0935579 ]]\n",
      "the iterations of  504900 the loss is 0.2516631217173704 theta= [[-11.79762015]\n",
      " [  0.09967056]\n",
      " [  0.09356597]]\n",
      "the iterations of  505000 the loss is 0.2516529849012496 theta= [[-11.79862695]\n",
      " [  0.09967854]\n",
      " [  0.09357402]]\n",
      "the iterations of  505100 the loss is 0.2516428515585745 theta= [[-11.79963358]\n",
      " [  0.09968652]\n",
      " [  0.09358208]]\n",
      "the iterations of  505200 the loss is 0.2516327216875723 theta= [[-11.80064003]\n",
      " [  0.09969449]\n",
      " [  0.09359014]]\n",
      "the iterations of  505300 the loss is 0.2516225952864705 theta= [[-11.80164631]\n",
      " [  0.09970246]\n",
      " [  0.09359819]]\n",
      "the iterations of  505400 the loss is 0.2516124723534985 theta= [[-11.80265242]\n",
      " [  0.09971043]\n",
      " [  0.09360625]]\n",
      "the iterations of  505500 the loss is 0.2516023528868867 theta= [[-11.80365836]\n",
      " [  0.0997184 ]\n",
      " [  0.0936143 ]]\n",
      "the iterations of  505600 the loss is 0.25159223688486654 theta= [[-11.80466413]\n",
      " [  0.09972637]\n",
      " [  0.09362235]]\n",
      "the iterations of  505700 the loss is 0.25158212434567073 theta= [[-11.80566972]\n",
      " [  0.09973434]\n",
      " [  0.0936304 ]]\n",
      "the iterations of  505800 the loss is 0.25157201526753364 theta= [[-11.80667514]\n",
      " [  0.0997423 ]\n",
      " [  0.09363845]]\n",
      "the iterations of  505900 the loss is 0.25156190964869046 theta= [[-11.80768039]\n",
      " [  0.09975027]\n",
      " [  0.0936465 ]]\n",
      "the iterations of  506000 the loss is 0.25155180748737754 theta= [[-11.80868546]\n",
      " [  0.09975823]\n",
      " [  0.09365454]]\n",
      "the iterations of  506100 the loss is 0.25154170878183285 theta= [[-11.80969037]\n",
      " [  0.09976619]\n",
      " [  0.09366258]]\n",
      "the iterations of  506200 the loss is 0.25153161353029496 theta= [[-11.8106951 ]\n",
      " [  0.09977415]\n",
      " [  0.09367063]]\n",
      "the iterations of  506300 the loss is 0.2515215217310044 theta= [[-11.81169966]\n",
      " [  0.09978211]\n",
      " [  0.09367867]]\n",
      "the iterations of  506400 the loss is 0.2515114333822021 theta= [[-11.81270405]\n",
      " [  0.09979007]\n",
      " [  0.09368671]]\n",
      "the iterations of  506500 the loss is 0.25150134848213085 theta= [[-11.81370827]\n",
      " [  0.09979803]\n",
      " [  0.09369475]]\n",
      "the iterations of  506600 the loss is 0.25149126702903435 theta= [[-11.81471232]\n",
      " [  0.09980598]\n",
      " [  0.09370279]]\n",
      "the iterations of  506700 the loss is 0.2514811890211574 theta= [[-11.81571619]\n",
      " [  0.09981393]\n",
      " [  0.09371082]]\n",
      "the iterations of  506800 the loss is 0.2514711144567466 theta= [[-11.81671989]\n",
      " [  0.09982189]\n",
      " [  0.09371886]]\n",
      "the iterations of  506900 the loss is 0.25146104333404884 theta= [[-11.81772342]\n",
      " [  0.09982984]\n",
      " [  0.09372689]]\n",
      "the iterations of  507000 the loss is 0.251450975651313 theta= [[-11.81872678]\n",
      " [  0.09983779]\n",
      " [  0.09373492]]\n",
      "the iterations of  507100 the loss is 0.25144091140678876 theta= [[-11.81972997]\n",
      " [  0.09984573]\n",
      " [  0.09374295]]\n",
      "the iterations of  507200 the loss is 0.25143085059872716 theta= [[-11.82073299]\n",
      " [  0.09985368]\n",
      " [  0.09375098]]\n",
      "the iterations of  507300 the loss is 0.2514207932253803 theta= [[-11.82173584]\n",
      " [  0.09986163]\n",
      " [  0.09375901]]\n",
      "the iterations of  507400 the loss is 0.2514107392850018 theta= [[-11.82273851]\n",
      " [  0.09986957]\n",
      " [  0.09376704]]\n",
      "the iterations of  507500 the loss is 0.2514006887758459 theta= [[-11.82374101]\n",
      " [  0.09987751]\n",
      " [  0.09377506]]\n",
      "the iterations of  507600 the loss is 0.2513906416961687 theta= [[-11.82474335]\n",
      " [  0.09988545]\n",
      " [  0.09378308]]\n",
      "the iterations of  507700 the loss is 0.25138059804422697 theta= [[-11.82574551]\n",
      " [  0.09989339]\n",
      " [  0.09379111]]\n",
      "the iterations of  507800 the loss is 0.25137055781827905 theta= [[-11.8267475 ]\n",
      " [  0.09990133]\n",
      " [  0.09379913]]\n",
      "the iterations of  507900 the loss is 0.25136052101658424 theta= [[-11.82774932]\n",
      " [  0.09990927]\n",
      " [  0.09380715]]\n",
      "the iterations of  508000 the loss is 0.2513504876374032 theta= [[-11.82875097]\n",
      " [  0.09991721]\n",
      " [  0.09381517]]\n",
      "the iterations of  508100 the loss is 0.25134045767899765 theta= [[-11.82975245]\n",
      " [  0.09992514]\n",
      " [  0.09382318]]\n",
      "the iterations of  508200 the loss is 0.25133043113963066 theta= [[-11.83075375]\n",
      " [  0.09993308]\n",
      " [  0.0938312 ]]\n",
      "the iterations of  508300 the loss is 0.2513204080175662 theta= [[-11.83175489]\n",
      " [  0.09994101]\n",
      " [  0.09383921]]\n",
      "the iterations of  508400 the loss is 0.25131038831106983 theta= [[-11.83275586]\n",
      " [  0.09994894]\n",
      " [  0.09384723]]\n",
      "the iterations of  508500 the loss is 0.251300372018408 theta= [[-11.83375665]\n",
      " [  0.09995687]\n",
      " [  0.09385524]]\n",
      "the iterations of  508600 the loss is 0.25129035913784836 theta= [[-11.83475728]\n",
      " [  0.0999648 ]\n",
      " [  0.09386325]]\n",
      "the iterations of  508700 the loss is 0.25128034966765983 theta= [[-11.83575773]\n",
      " [  0.09997272]\n",
      " [  0.09387126]]\n",
      "the iterations of  508800 the loss is 0.25127034360611267 theta= [[-11.83675802]\n",
      " [  0.09998065]\n",
      " [  0.09387927]]\n",
      "the iterations of  508900 the loss is 0.2512603409514779 theta= [[-11.83775813]\n",
      " [  0.09998857]\n",
      " [  0.09388727]]\n",
      "the iterations of  509000 the loss is 0.25125034170202837 theta= [[-11.83875808]\n",
      " [  0.0999965 ]\n",
      " [  0.09389528]]\n",
      "the iterations of  509100 the loss is 0.25124034585603744 theta= [[-11.83975785]\n",
      " [  0.10000442]\n",
      " [  0.09390328]]\n",
      "the iterations of  509200 the loss is 0.2512303534117801 theta= [[-11.84075745]\n",
      " [  0.10001234]\n",
      " [  0.09391128]]\n",
      "the iterations of  509300 the loss is 0.2512203643675323 theta= [[-11.84175689]\n",
      " [  0.10002026]\n",
      " [  0.09391928]]\n",
      "the iterations of  509400 the loss is 0.25121037872157115 theta= [[-11.84275615]\n",
      " [  0.10002817]\n",
      " [  0.09392728]]\n",
      "the iterations of  509500 the loss is 0.2512003964721751 theta= [[-11.84375525]\n",
      " [  0.10003609]\n",
      " [  0.09393528]]\n",
      "the iterations of  509600 the loss is 0.25119041761762395 theta= [[-11.84475417]\n",
      " [  0.100044  ]\n",
      " [  0.09394328]]\n",
      "the iterations of  509700 the loss is 0.25118044215619795 theta= [[-11.84575292]\n",
      " [  0.10005192]\n",
      " [  0.09395128]]\n",
      "the iterations of  509800 the loss is 0.2511704700861793 theta= [[-11.84675151]\n",
      " [  0.10005983]\n",
      " [  0.09395927]]\n",
      "the iterations of  509900 the loss is 0.25116050140585133 theta= [[-11.84774992]\n",
      " [  0.10006774]\n",
      " [  0.09396726]]\n",
      "the iterations of  510000 the loss is 0.25115053611349797 theta= [[-11.84874817]\n",
      " [  0.10007565]\n",
      " [  0.09397525]]\n",
      "the iterations of  510100 the loss is 0.2511405742074046 theta= [[-11.84974624]\n",
      " [  0.10008356]\n",
      " [  0.09398324]]\n",
      "the iterations of  510200 the loss is 0.25113061568585804 theta= [[-11.85074415]\n",
      " [  0.10009147]\n",
      " [  0.09399123]]\n",
      "the iterations of  510300 the loss is 0.251120660547146 theta= [[-11.85174188]\n",
      " [  0.10009937]\n",
      " [  0.09399922]]\n",
      "the iterations of  510400 the loss is 0.2511107087895574 theta= [[-11.85273945]\n",
      " [  0.10010728]\n",
      " [  0.09400721]]\n",
      "the iterations of  510500 the loss is 0.25110076041138235 theta= [[-11.85373685]\n",
      " [  0.10011518]\n",
      " [  0.09401519]]\n",
      "the iterations of  510600 the loss is 0.25109081541091216 theta= [[-11.85473407]\n",
      " [  0.10012308]\n",
      " [  0.09402318]]\n",
      "the iterations of  510700 the loss is 0.25108087378643934 theta= [[-11.85573113]\n",
      " [  0.10013098]\n",
      " [  0.09403116]]\n",
      "the iterations of  510800 the loss is 0.2510709355362574 theta= [[-11.85672802]\n",
      " [  0.10013888]\n",
      " [  0.09403914]]\n",
      "the iterations of  510900 the loss is 0.2510610006586613 theta= [[-11.85772474]\n",
      " [  0.10014678]\n",
      " [  0.09404712]]\n",
      "the iterations of  511000 the loss is 0.2510510691519468 theta= [[-11.85872129]\n",
      " [  0.10015467]\n",
      " [  0.0940551 ]]\n",
      "the iterations of  511100 the loss is 0.2510411410144111 theta= [[-11.85971767]\n",
      " [  0.10016257]\n",
      " [  0.09406307]]\n",
      "the iterations of  511200 the loss is 0.25103121624435265 theta= [[-11.86071389]\n",
      " [  0.10017046]\n",
      " [  0.09407105]]\n",
      "the iterations of  511300 the loss is 0.25102129484007063 theta= [[-11.86170993]\n",
      " [  0.10017835]\n",
      " [  0.09407902]]\n",
      "the iterations of  511400 the loss is 0.25101137679986585 theta= [[-11.86270581]\n",
      " [  0.10018624]\n",
      " [  0.094087  ]]\n",
      "the iterations of  511500 the loss is 0.2510014621220397 theta= [[-11.86370151]\n",
      " [  0.10019413]\n",
      " [  0.09409497]]\n",
      "the iterations of  511600 the loss is 0.25099155080489555 theta= [[-11.86469705]\n",
      " [  0.10020202]\n",
      " [  0.09410294]]\n",
      "the iterations of  511700 the loss is 0.2509816428467372 theta= [[-11.86569242]\n",
      " [  0.10020991]\n",
      " [  0.09411091]]\n",
      "the iterations of  511800 the loss is 0.25097173824587 theta= [[-11.86668762]\n",
      " [  0.1002178 ]\n",
      " [  0.09411888]]\n",
      "the iterations of  511900 the loss is 0.2509618370006004 theta= [[-11.86768265]\n",
      " [  0.10022568]\n",
      " [  0.09412684]]\n",
      "the iterations of  512000 the loss is 0.2509519391092357 theta= [[-11.86867751]\n",
      " [  0.10023356]\n",
      " [  0.09413481]]\n",
      "the iterations of  512100 the loss is 0.2509420445700848 theta= [[-11.86967221]\n",
      " [  0.10024144]\n",
      " [  0.09414277]]\n",
      "the iterations of  512200 the loss is 0.2509321533814574 theta= [[-11.87066673]\n",
      " [  0.10024933]\n",
      " [  0.09415073]]\n",
      "the iterations of  512300 the loss is 0.2509222655416646 theta= [[-11.87166109]\n",
      " [  0.1002572 ]\n",
      " [  0.09415869]]\n",
      "the iterations of  512400 the loss is 0.2509123810490185 theta= [[-11.87265528]\n",
      " [  0.10026508]\n",
      " [  0.09416665]]\n",
      "the iterations of  512500 the loss is 0.25090249990183244 theta= [[-11.8736493 ]\n",
      " [  0.10027296]\n",
      " [  0.09417461]]\n",
      "the iterations of  512600 the loss is 0.2508926220984209 theta= [[-11.87464315]\n",
      " [  0.10028083]\n",
      " [  0.09418257]]\n",
      "the iterations of  512700 the loss is 0.2508827476370994 theta= [[-11.87563684]\n",
      " [  0.10028871]\n",
      " [  0.09419053]]\n",
      "the iterations of  512800 the loss is 0.2508728765161846 theta= [[-11.87663036]\n",
      " [  0.10029658]\n",
      " [  0.09419848]]\n",
      "the iterations of  512900 the loss is 0.25086300873399453 theta= [[-11.8776237 ]\n",
      " [  0.10030445]\n",
      " [  0.09420643]]\n",
      "the iterations of  513000 the loss is 0.25085314428884836 theta= [[-11.87861689]\n",
      " [  0.10031232]\n",
      " [  0.09421438]]\n",
      "the iterations of  513100 the loss is 0.2508432831790659 theta= [[-11.8796099 ]\n",
      " [  0.10032019]\n",
      " [  0.09422234]]\n",
      "the iterations of  513200 the loss is 0.2508334254029687 theta= [[-11.88060274]\n",
      " [  0.10032806]\n",
      " [  0.09423028]]\n",
      "the iterations of  513300 the loss is 0.2508235709588793 theta= [[-11.88159542]\n",
      " [  0.10033592]\n",
      " [  0.09423823]]\n",
      "the iterations of  513400 the loss is 0.250813719845121 theta= [[-11.88258793]\n",
      " [  0.10034379]\n",
      " [  0.09424618]]\n",
      "the iterations of  513500 the loss is 0.25080387206001886 theta= [[-11.88358027]\n",
      " [  0.10035165]\n",
      " [  0.09425412]]\n",
      "the iterations of  513600 the loss is 0.2507940276018984 theta= [[-11.88457244]\n",
      " [  0.10035951]\n",
      " [  0.09426207]]\n",
      "the iterations of  513700 the loss is 0.250784186469087 theta= [[-11.88556445]\n",
      " [  0.10036737]\n",
      " [  0.09427001]]\n",
      "the iterations of  513800 the loss is 0.25077434865991266 theta= [[-11.88655629]\n",
      " [  0.10037523]\n",
      " [  0.09427795]]\n",
      "the iterations of  513900 the loss is 0.2507645141727047 theta= [[-11.88754796]\n",
      " [  0.10038309]\n",
      " [  0.09428589]]\n",
      "the iterations of  514000 the loss is 0.25075468300579334 theta= [[-11.88853947]\n",
      " [  0.10039095]\n",
      " [  0.09429383]]\n",
      "the iterations of  514100 the loss is 0.2507448551575104 theta= [[-11.8895308 ]\n",
      " [  0.1003988 ]\n",
      " [  0.09430177]]\n",
      "the iterations of  514200 the loss is 0.25073503062618846 theta= [[-11.89052197]\n",
      " [  0.10040666]\n",
      " [  0.0943097 ]]\n",
      "the iterations of  514300 the loss is 0.25072520941016146 theta= [[-11.89151297]\n",
      " [  0.10041451]\n",
      " [  0.09431764]]\n",
      "the iterations of  514400 the loss is 0.250715391507764 theta= [[-11.89250381]\n",
      " [  0.10042236]\n",
      " [  0.09432557]]\n",
      "the iterations of  514500 the loss is 0.2507055769173326 theta= [[-11.89349448]\n",
      " [  0.10043021]\n",
      " [  0.0943335 ]]\n",
      "the iterations of  514600 the loss is 0.2506957656372042 theta= [[-11.89448498]\n",
      " [  0.10043806]\n",
      " [  0.09434143]]\n",
      "the iterations of  514700 the loss is 0.2506859576657171 theta= [[-11.89547531]\n",
      " [  0.10044591]\n",
      " [  0.09434936]]\n",
      "the iterations of  514800 the loss is 0.250676153001211 theta= [[-11.89646548]\n",
      " [  0.10045376]\n",
      " [  0.09435729]]\n",
      "the iterations of  514900 the loss is 0.25066635164202633 theta= [[-11.89745548]\n",
      " [  0.1004616 ]\n",
      " [  0.09436522]]\n",
      "the iterations of  515000 the loss is 0.2506565535865048 theta= [[-11.89844531]\n",
      " [  0.10046945]\n",
      " [  0.09437314]]\n",
      "the iterations of  515100 the loss is 0.2506467588329892 theta= [[-11.89943497]\n",
      " [  0.10047729]\n",
      " [  0.09438107]]\n",
      "the iterations of  515200 the loss is 0.25063696737982377 theta= [[-11.90042447]\n",
      " [  0.10048513]\n",
      " [  0.09438899]]\n",
      "the iterations of  515300 the loss is 0.25062717922535344 theta= [[-11.90141381]\n",
      " [  0.10049297]\n",
      " [  0.09439691]]\n",
      "the iterations of  515400 the loss is 0.2506173943679242 theta= [[-11.90240297]\n",
      " [  0.10050081]\n",
      " [  0.09440483]]\n",
      "the iterations of  515500 the loss is 0.2506076128058836 theta= [[-11.90339197]\n",
      " [  0.10050864]\n",
      " [  0.09441275]]\n",
      "the iterations of  515600 the loss is 0.25059783453758017 theta= [[-11.9043808 ]\n",
      " [  0.10051648]\n",
      " [  0.09442067]]\n",
      "the iterations of  515700 the loss is 0.2505880595613632 theta= [[-11.90536947]\n",
      " [  0.10052432]\n",
      " [  0.09442858]]\n",
      "the iterations of  515800 the loss is 0.2505782878755836 theta= [[-11.90635797]\n",
      " [  0.10053215]\n",
      " [  0.0944365 ]]\n",
      "the iterations of  515900 the loss is 0.25056851947859327 theta= [[-11.9073463 ]\n",
      " [  0.10053998]\n",
      " [  0.09444441]]\n",
      "the iterations of  516000 the loss is 0.25055875436874486 theta= [[-11.90833447]\n",
      " [  0.10054781]\n",
      " [  0.09445232]]\n",
      "the iterations of  516100 the loss is 0.25054899254439245 theta= [[-11.90932247]\n",
      " [  0.10055564]\n",
      " [  0.09446023]]\n",
      "the iterations of  516200 the loss is 0.2505392340038911 theta= [[-11.91031031]\n",
      " [  0.10056347]\n",
      " [  0.09446814]]\n",
      "the iterations of  516300 the loss is 0.25052947874559733 theta= [[-11.91129798]\n",
      " [  0.1005713 ]\n",
      " [  0.09447605]]\n",
      "the iterations of  516400 the loss is 0.25051972676786827 theta= [[-11.91228548]\n",
      " [  0.10057912]\n",
      " [  0.09448396]]\n",
      "the iterations of  516500 the loss is 0.25050997806906256 theta= [[-11.91327281]\n",
      " [  0.10058695]\n",
      " [  0.09449186]]\n",
      "the iterations of  516600 the loss is 0.25050023264753973 theta= [[-11.91425999]\n",
      " [  0.10059477]\n",
      " [  0.09449977]]\n",
      "the iterations of  516700 the loss is 0.25049049050166045 theta= [[-11.91524699]\n",
      " [  0.10060259]\n",
      " [  0.09450767]]\n",
      "the iterations of  516800 the loss is 0.25048075162978656 theta= [[-11.91623383]\n",
      " [  0.10061041]\n",
      " [  0.09451557]]\n",
      "the iterations of  516900 the loss is 0.25047101603028094 theta= [[-11.9172205 ]\n",
      " [  0.10061823]\n",
      " [  0.09452347]]\n",
      "the iterations of  517000 the loss is 0.2504612837015075 theta= [[-11.91820701]\n",
      " [  0.10062605]\n",
      " [  0.09453137]]\n",
      "the iterations of  517100 the loss is 0.25045155464183166 theta= [[-11.91919335]\n",
      " [  0.10063386]\n",
      " [  0.09453927]]\n",
      "the iterations of  517200 the loss is 0.25044182884961935 theta= [[-11.92017953]\n",
      " [  0.10064168]\n",
      " [  0.09454717]]\n",
      "the iterations of  517300 the loss is 0.25043210632323826 theta= [[-11.92116554]\n",
      " [  0.10064949]\n",
      " [  0.09455506]]\n",
      "the iterations of  517400 the loss is 0.2504223870610563 theta= [[-11.92215138]\n",
      " [  0.10065731]\n",
      " [  0.09456296]]\n",
      "the iterations of  517500 the loss is 0.2504126710614434 theta= [[-11.92313706]\n",
      " [  0.10066512]\n",
      " [  0.09457085]]\n",
      "the iterations of  517600 the loss is 0.25040295832277004 theta= [[-11.92412257]\n",
      " [  0.10067293]\n",
      " [  0.09457874]]\n",
      "the iterations of  517700 the loss is 0.250393248843408 theta= [[-11.92510792]\n",
      " [  0.10068074]\n",
      " [  0.09458663]]\n",
      "the iterations of  517800 the loss is 0.2503835426217301 theta= [[-11.92609311]\n",
      " [  0.10068854]\n",
      " [  0.09459452]]\n",
      "the iterations of  517900 the loss is 0.2503738396561104 theta= [[-11.92707812]\n",
      " [  0.10069635]\n",
      " [  0.09460241]]\n",
      "the iterations of  518000 the loss is 0.25036413994492357 theta= [[-11.92806298]\n",
      " [  0.10070415]\n",
      " [  0.09461029]]\n",
      "the iterations of  518100 the loss is 0.25035444348654606 theta= [[-11.92904766]\n",
      " [  0.10071196]\n",
      " [  0.09461818]]\n",
      "the iterations of  518200 the loss is 0.250344750279355 theta= [[-11.93003219]\n",
      " [  0.10071976]\n",
      " [  0.09462606]]\n",
      "the iterations of  518300 the loss is 0.25033506032172864 theta= [[-11.93101654]\n",
      " [  0.10072756]\n",
      " [  0.09463394]]\n",
      "the iterations of  518400 the loss is 0.2503253736120464 theta= [[-11.93200074]\n",
      " [  0.10073536]\n",
      " [  0.09464182]]\n",
      "the iterations of  518500 the loss is 0.2503156901486889 theta= [[-11.93298476]\n",
      " [  0.10074316]\n",
      " [  0.0946497 ]]\n",
      "the iterations of  518600 the loss is 0.25030600993003754 theta= [[-11.93396863]\n",
      " [  0.10075096]\n",
      " [  0.09465758]]\n",
      "the iterations of  518700 the loss is 0.25029633295447495 theta= [[-11.93495232]\n",
      " [  0.10075875]\n",
      " [  0.09466546]]\n",
      "the iterations of  518800 the loss is 0.2502866592203852 theta= [[-11.93593586]\n",
      " [  0.10076655]\n",
      " [  0.09467333]]\n",
      "the iterations of  518900 the loss is 0.250276988726153 theta= [[-11.93691922]\n",
      " [  0.10077434]\n",
      " [  0.09468121]]\n",
      "the iterations of  519000 the loss is 0.2502673214701641 theta= [[-11.93790243]\n",
      " [  0.10078213]\n",
      " [  0.09468908]]\n",
      "the iterations of  519100 the loss is 0.25025765745080575 theta= [[-11.93888547]\n",
      " [  0.10078992]\n",
      " [  0.09469695]]\n",
      "the iterations of  519200 the loss is 0.250247996666466 theta= [[-11.93986834]\n",
      " [  0.10079771]\n",
      " [  0.09470482]]\n",
      "the iterations of  519300 the loss is 0.2502383391155341 theta= [[-11.94085105]\n",
      " [  0.1008055 ]\n",
      " [  0.09471269]]\n",
      "the iterations of  519400 the loss is 0.2502286847964002 theta= [[-11.9418336 ]\n",
      " [  0.10081329]\n",
      " [  0.09472056]]\n",
      "the iterations of  519500 the loss is 0.2502190337074558 theta= [[-11.94281598]\n",
      " [  0.10082107]\n",
      " [  0.09472843]]\n",
      "the iterations of  519600 the loss is 0.2502093858470933 theta= [[-11.94379819]\n",
      " [  0.10082886]\n",
      " [  0.09473629]]\n",
      "the iterations of  519700 the loss is 0.25019974121370625 theta= [[-11.94478025]\n",
      " [  0.10083664]\n",
      " [  0.09474416]]\n",
      "the iterations of  519800 the loss is 0.2501900998056891 theta= [[-11.94576214]\n",
      " [  0.10084442]\n",
      " [  0.09475202]]\n",
      "the iterations of  519900 the loss is 0.2501804616214377 theta= [[-11.94674386]\n",
      " [  0.1008522 ]\n",
      " [  0.09475988]]\n",
      "the iterations of  520000 the loss is 0.25017082665934876 theta= [[-11.94772542]\n",
      " [  0.10085998]\n",
      " [  0.09476774]]\n",
      "the iterations of  520100 the loss is 0.25016119491782013 theta= [[-11.94870682]\n",
      " [  0.10086776]\n",
      " [  0.0947756 ]]\n",
      "the iterations of  520200 the loss is 0.25015156639525066 theta= [[-11.94968805]\n",
      " [  0.10087553]\n",
      " [  0.09478346]]\n",
      "the iterations of  520300 the loss is 0.25014194109004056 theta= [[-11.95066912]\n",
      " [  0.10088331]\n",
      " [  0.09479131]]\n",
      "the iterations of  520400 the loss is 0.2501323190005907 theta= [[-11.95165002]\n",
      " [  0.10089108]\n",
      " [  0.09479917]]\n",
      "the iterations of  520500 the loss is 0.25012270012530324 theta= [[-11.95263076]\n",
      " [  0.10089886]\n",
      " [  0.09480702]]\n",
      "the iterations of  520600 the loss is 0.25011308446258146 theta= [[-11.95361134]\n",
      " [  0.10090663]\n",
      " [  0.09481488]]\n",
      "the iterations of  520700 the loss is 0.25010347201082955 theta= [[-11.95459175]\n",
      " [  0.1009144 ]\n",
      " [  0.09482273]]\n",
      "the iterations of  520800 the loss is 0.25009386276845297 theta= [[-11.955572  ]\n",
      " [  0.10092217]\n",
      " [  0.09483058]]\n",
      "the iterations of  520900 the loss is 0.250084256733858 theta= [[-11.95655208]\n",
      " [  0.10092993]\n",
      " [  0.09483842]]\n",
      "the iterations of  521000 the loss is 0.25007465390545236 theta= [[-11.957532  ]\n",
      " [  0.1009377 ]\n",
      " [  0.09484627]]\n",
      "the iterations of  521100 the loss is 0.25006505428164444 theta= [[-11.95851176]\n",
      " [  0.10094547]\n",
      " [  0.09485412]]\n",
      "the iterations of  521200 the loss is 0.2500554578608439 theta= [[-11.95949136]\n",
      " [  0.10095323]\n",
      " [  0.09486196]]\n",
      "the iterations of  521300 the loss is 0.25004586464146156 theta= [[-11.96047079]\n",
      " [  0.10096099]\n",
      " [  0.09486981]]\n",
      "the iterations of  521400 the loss is 0.2500362746219089 theta= [[-11.96145006]\n",
      " [  0.10096875]\n",
      " [  0.09487765]]\n",
      "the iterations of  521500 the loss is 0.25002668780059906 theta= [[-11.96242916]\n",
      " [  0.10097651]\n",
      " [  0.09488549]]\n",
      "the iterations of  521600 the loss is 0.25001710417594575 theta= [[-11.9634081 ]\n",
      " [  0.10098427]\n",
      " [  0.09489333]]\n",
      "the iterations of  521700 the loss is 0.25000752374636415 theta= [[-11.96438688]\n",
      " [  0.10099203]\n",
      " [  0.09490117]]\n",
      "the iterations of  521800 the loss is 0.24999794651026983 theta= [[-11.9653655 ]\n",
      " [  0.10099978]\n",
      " [  0.094909  ]]\n",
      "the iterations of  521900 the loss is 0.24998837246608027 theta= [[-11.96634395]\n",
      " [  0.10100754]\n",
      " [  0.09491684]]\n",
      "the iterations of  522000 the loss is 0.2499788016122134 theta= [[-11.96732224]\n",
      " [  0.10101529]\n",
      " [  0.09492467]]\n",
      "the iterations of  522100 the loss is 0.24996923394708848 theta= [[-11.96830036]\n",
      " [  0.10102304]\n",
      " [  0.09493251]]\n",
      "the iterations of  522200 the loss is 0.2499596694691257 theta= [[-11.96927833]\n",
      " [  0.1010308 ]\n",
      " [  0.09494034]]\n",
      "the iterations of  522300 the loss is 0.24995010817674654 theta= [[-11.97025613]\n",
      " [  0.10103855]\n",
      " [  0.09494817]]\n",
      "the iterations of  522400 the loss is 0.24994055006837324 theta= [[-11.97123376]\n",
      " [  0.10104629]\n",
      " [  0.094956  ]]\n",
      "the iterations of  522500 the loss is 0.24993099514242928 theta= [[-11.97221124]\n",
      " [  0.10105404]\n",
      " [  0.09496383]]\n",
      "the iterations of  522600 the loss is 0.24992144339733893 theta= [[-11.97318855]\n",
      " [  0.10106179]\n",
      " [  0.09497165]]\n",
      "the iterations of  522700 the loss is 0.2499118948315281 theta= [[-11.9741657 ]\n",
      " [  0.10106953]\n",
      " [  0.09497948]]\n",
      "the iterations of  522800 the loss is 0.24990234944342304 theta= [[-11.97514269]\n",
      " [  0.10107727]\n",
      " [  0.0949873 ]]\n",
      "the iterations of  522900 the loss is 0.24989280723145163 theta= [[-11.97611951]\n",
      " [  0.10108502]\n",
      " [  0.09499512]]\n",
      "the iterations of  523000 the loss is 0.24988326819404236 theta= [[-11.97709617]\n",
      " [  0.10109276]\n",
      " [  0.09500295]]\n",
      "the iterations of  523100 the loss is 0.24987373232962518 theta= [[-11.97807267]\n",
      " [  0.1011005 ]\n",
      " [  0.09501077]]\n",
      "the iterations of  523200 the loss is 0.2498641996366307 theta= [[-11.97904901]\n",
      " [  0.10110824]\n",
      " [  0.09501858]]\n",
      "the iterations of  523300 the loss is 0.24985467011349072 theta= [[-11.98002518]\n",
      " [  0.10111597]\n",
      " [  0.0950264 ]]\n",
      "the iterations of  523400 the loss is 0.24984514375863817 theta= [[-11.98100119]\n",
      " [  0.10112371]\n",
      " [  0.09503422]]\n",
      "the iterations of  523500 the loss is 0.2498356205705071 theta= [[-11.98197704]\n",
      " [  0.10113144]\n",
      " [  0.09504203]]\n",
      "the iterations of  523600 the loss is 0.24982610054753238 theta= [[-11.98295273]\n",
      " [  0.10113918]\n",
      " [  0.09504985]]\n",
      "the iterations of  523700 the loss is 0.24981658368815007 theta= [[-11.98392826]\n",
      " [  0.10114691]\n",
      " [  0.09505766]]\n",
      "the iterations of  523800 the loss is 0.24980706999079716 theta= [[-11.98490362]\n",
      " [  0.10115464]\n",
      " [  0.09506547]]\n",
      "the iterations of  523900 the loss is 0.2497975594539118 theta= [[-11.98587882]\n",
      " [  0.10116237]\n",
      " [  0.09507328]]\n",
      "the iterations of  524000 the loss is 0.24978805207593321 theta= [[-11.98685386]\n",
      " [  0.1011701 ]\n",
      " [  0.09508109]]\n",
      "the iterations of  524100 the loss is 0.2497785478553015 theta= [[-11.98782874]\n",
      " [  0.10117782]\n",
      " [  0.0950889 ]]\n",
      "the iterations of  524200 the loss is 0.24976904679045805 theta= [[-11.98880345]\n",
      " [  0.10118555]\n",
      " [  0.0950967 ]]\n",
      "the iterations of  524300 the loss is 0.2497595488798449 theta= [[-11.98977801]\n",
      " [  0.10119327]\n",
      " [  0.09510451]]\n",
      "the iterations of  524400 the loss is 0.24975005412190548 theta= [[-11.9907524 ]\n",
      " [  0.101201  ]\n",
      " [  0.09511231]]\n",
      "the iterations of  524500 the loss is 0.2497405625150842 theta= [[-11.99172663]\n",
      " [  0.10120872]\n",
      " [  0.09512011]]\n",
      "the iterations of  524600 the loss is 0.2497310740578264 theta= [[-11.9927007 ]\n",
      " [  0.10121644]\n",
      " [  0.09512792]]\n",
      "the iterations of  524700 the loss is 0.2497215887485785 theta= [[-11.99367461]\n",
      " [  0.10122416]\n",
      " [  0.09513572]]\n",
      "the iterations of  524800 the loss is 0.2497121065857878 theta= [[-11.99464835]\n",
      " [  0.10123187]\n",
      " [  0.09514351]]\n",
      "the iterations of  524900 the loss is 0.24970262756790298 theta= [[-11.99562193]\n",
      " [  0.10123959]\n",
      " [  0.09515131]]\n",
      "the iterations of  525000 the loss is 0.24969315169337353 theta= [[-11.99659536]\n",
      " [  0.10124731]\n",
      " [  0.09515911]]\n",
      "the iterations of  525100 the loss is 0.24968367896065005 theta= [[-11.99756862]\n",
      " [  0.10125502]\n",
      " [  0.0951669 ]]\n",
      "the iterations of  525200 the loss is 0.24967420936818396 theta= [[-11.99854172]\n",
      " [  0.10126273]\n",
      " [  0.09517469]]\n",
      "the iterations of  525300 the loss is 0.24966474291442803 theta= [[-11.99951466]\n",
      " [  0.10127045]\n",
      " [  0.09518249]]\n",
      "the iterations of  525400 the loss is 0.24965527959783596 theta= [[-12.00048743]\n",
      " [  0.10127816]\n",
      " [  0.09519028]]\n",
      "the iterations of  525500 the loss is 0.24964581941686237 theta= [[-12.00146005]\n",
      " [  0.10128586]\n",
      " [  0.09519807]]\n",
      "the iterations of  525600 the loss is 0.24963636236996287 theta= [[-12.00243251]\n",
      " [  0.10129357]\n",
      " [  0.09520586]]\n",
      "the iterations of  525700 the loss is 0.24962690845559427 theta= [[-12.0034048 ]\n",
      " [  0.10130128]\n",
      " [  0.09521364]]\n",
      "the iterations of  525800 the loss is 0.24961745767221438 theta= [[-12.00437693]\n",
      " [  0.10130898]\n",
      " [  0.09522143]]\n",
      "the iterations of  525900 the loss is 0.24960801001828187 theta= [[-12.0053489 ]\n",
      " [  0.10131669]\n",
      " [  0.09522921]]\n",
      "the iterations of  526000 the loss is 0.24959856549225673 theta= [[-12.00632072]\n",
      " [  0.10132439]\n",
      " [  0.095237  ]]\n",
      "the iterations of  526100 the loss is 0.2495891240925997 theta= [[-12.00729237]\n",
      " [  0.10133209]\n",
      " [  0.09524478]]\n",
      "the iterations of  526200 the loss is 0.24957968581777265 theta= [[-12.00826386]\n",
      " [  0.10133979]\n",
      " [  0.09525256]]\n",
      "the iterations of  526300 the loss is 0.24957025066623845 theta= [[-12.00923518]\n",
      " [  0.10134749]\n",
      " [  0.09526034]]\n",
      "the iterations of  526400 the loss is 0.24956081863646087 theta= [[-12.01020635]\n",
      " [  0.10135519]\n",
      " [  0.09526812]]\n",
      "the iterations of  526500 the loss is 0.2495513897269051 theta= [[-12.01117736]\n",
      " [  0.10136289]\n",
      " [  0.09527589]]\n",
      "the iterations of  526600 the loss is 0.24954196393603695 theta= [[-12.01214821]\n",
      " [  0.10137058]\n",
      " [  0.09528367]]\n",
      "the iterations of  526700 the loss is 0.24953254126232335 theta= [[-12.01311889]\n",
      " [  0.10137828]\n",
      " [  0.09529144]]\n",
      "the iterations of  526800 the loss is 0.2495231217042323 theta= [[-12.01408942]\n",
      " [  0.10138597]\n",
      " [  0.09529922]]\n",
      "the iterations of  526900 the loss is 0.2495137052602329 theta= [[-12.01505978]\n",
      " [  0.10139366]\n",
      " [  0.09530699]]\n",
      "the iterations of  527000 the loss is 0.24950429192879503 theta= [[-12.01602999]\n",
      " [  0.10140135]\n",
      " [  0.09531476]]\n",
      "the iterations of  527100 the loss is 0.2494948817083898 theta= [[-12.01700003]\n",
      " [  0.10140904]\n",
      " [  0.09532253]]\n",
      "the iterations of  527200 the loss is 0.24948547459748927 theta= [[-12.01796992]\n",
      " [  0.10141673]\n",
      " [  0.0953303 ]]\n",
      "the iterations of  527300 the loss is 0.2494760705945665 theta= [[-12.01893964]\n",
      " [  0.10142441]\n",
      " [  0.09533806]]\n",
      "the iterations of  527400 the loss is 0.24946666969809553 theta= [[-12.0199092 ]\n",
      " [  0.1014321 ]\n",
      " [  0.09534583]]\n",
      "the iterations of  527500 the loss is 0.24945727190655134 theta= [[-12.02087861]\n",
      " [  0.10143978]\n",
      " [  0.09535359]]\n",
      "the iterations of  527600 the loss is 0.24944787721841016 theta= [[-12.02184785]\n",
      " [  0.10144747]\n",
      " [  0.09536136]]\n",
      "the iterations of  527700 the loss is 0.2494384856321492 theta= [[-12.02281693]\n",
      " [  0.10145515]\n",
      " [  0.09536912]]\n",
      "the iterations of  527800 the loss is 0.2494290971462463 theta= [[-12.02378586]\n",
      " [  0.10146283]\n",
      " [  0.09537688]]\n",
      "the iterations of  527900 the loss is 0.24941971175918085 theta= [[-12.02475462]\n",
      " [  0.10147051]\n",
      " [  0.09538464]]\n",
      "the iterations of  528000 the loss is 0.24941032946943287 theta= [[-12.02572323]\n",
      " [  0.10147818]\n",
      " [  0.09539239]]\n",
      "the iterations of  528100 the loss is 0.24940095027548342 theta= [[-12.02669167]\n",
      " [  0.10148586]\n",
      " [  0.09540015]]\n",
      "the iterations of  528200 the loss is 0.24939157417581484 theta= [[-12.02765995]\n",
      " [  0.10149354]\n",
      " [  0.09540791]]\n",
      "the iterations of  528300 the loss is 0.24938220116891002 theta= [[-12.02862808]\n",
      " [  0.10150121]\n",
      " [  0.09541566]]\n",
      "the iterations of  528400 the loss is 0.2493728312532533 theta= [[-12.02959604]\n",
      " [  0.10150888]\n",
      " [  0.09542341]]\n",
      "the iterations of  528500 the loss is 0.24936346442732982 theta= [[-12.03056385]\n",
      " [  0.10151655]\n",
      " [  0.09543117]]\n",
      "the iterations of  528600 the loss is 0.24935410068962574 theta= [[-12.03153149]\n",
      " [  0.10152422]\n",
      " [  0.09543892]]\n",
      "the iterations of  528700 the loss is 0.24934474003862828 theta= [[-12.03249898]\n",
      " [  0.10153189]\n",
      " [  0.09544667]]\n",
      "the iterations of  528800 the loss is 0.2493353824728254 theta= [[-12.03346631]\n",
      " [  0.10153956]\n",
      " [  0.09545441]]\n",
      "the iterations of  528900 the loss is 0.24932602799070636 theta= [[-12.03443347]\n",
      " [  0.10154723]\n",
      " [  0.09546216]]\n",
      "the iterations of  529000 the loss is 0.24931667659076154 theta= [[-12.03540048]\n",
      " [  0.10155489]\n",
      " [  0.0954699 ]]\n",
      "the iterations of  529100 the loss is 0.24930732827148183 theta= [[-12.03636733]\n",
      " [  0.10156256]\n",
      " [  0.09547765]]\n",
      "the iterations of  529200 the loss is 0.2492979830313595 theta= [[-12.03733402]\n",
      " [  0.10157022]\n",
      " [  0.09548539]]\n",
      "the iterations of  529300 the loss is 0.2492886408688878 theta= [[-12.03830055]\n",
      " [  0.10157788]\n",
      " [  0.09549313]]\n",
      "the iterations of  529400 the loss is 0.24927930178256058 theta= [[-12.03926692]\n",
      " [  0.10158554]\n",
      " [  0.09550087]]\n",
      "the iterations of  529500 the loss is 0.2492699657708733 theta= [[-12.04023313]\n",
      " [  0.1015932 ]\n",
      " [  0.09550861]]\n",
      "the iterations of  529600 the loss is 0.24926063283232214 theta= [[-12.04119919]\n",
      " [  0.10160086]\n",
      " [  0.09551635]]\n",
      "the iterations of  529700 the loss is 0.24925130296540407 theta= [[-12.04216508]\n",
      " [  0.10160852]\n",
      " [  0.09552409]]\n",
      "the iterations of  529800 the loss is 0.24924197616861712 theta= [[-12.04313081]\n",
      " [  0.10161617]\n",
      " [  0.09553182]]\n",
      "the iterations of  529900 the loss is 0.2492326524404606 theta= [[-12.04409639]\n",
      " [  0.10162382]\n",
      " [  0.09553956]]\n",
      "the iterations of  530000 the loss is 0.24922333177943468 theta= [[-12.04506181]\n",
      " [  0.10163148]\n",
      " [  0.09554729]]\n",
      "the iterations of  530100 the loss is 0.24921401418404027 theta= [[-12.04602707]\n",
      " [  0.10163913]\n",
      " [  0.09555502]]\n",
      "the iterations of  530200 the loss is 0.2492046996527795 theta= [[-12.04699217]\n",
      " [  0.10164678]\n",
      " [  0.09556275]]\n",
      "the iterations of  530300 the loss is 0.2491953881841555 theta= [[-12.04795711]\n",
      " [  0.10165443]\n",
      " [  0.09557048]]\n",
      "the iterations of  530400 the loss is 0.24918607977667243 theta= [[-12.04892189]\n",
      " [  0.10166208]\n",
      " [  0.09557821]]\n",
      "the iterations of  530500 the loss is 0.24917677442883535 theta= [[-12.04988651]\n",
      " [  0.10166972]\n",
      " [  0.09558593]]\n",
      "the iterations of  530600 the loss is 0.24916747213915005 theta= [[-12.05085098]\n",
      " [  0.10167737]\n",
      " [  0.09559366]]\n",
      "the iterations of  530700 the loss is 0.249158172906124 theta= [[-12.05181529]\n",
      " [  0.10168501]\n",
      " [  0.09560138]]\n",
      "the iterations of  530800 the loss is 0.2491488767282648 theta= [[-12.05277943]\n",
      " [  0.10169266]\n",
      " [  0.09560911]]\n",
      "the iterations of  530900 the loss is 0.24913958360408156 theta= [[-12.05374342]\n",
      " [  0.1017003 ]\n",
      " [  0.09561683]]\n",
      "the iterations of  531000 the loss is 0.24913029353208446 theta= [[-12.05470726]\n",
      " [  0.10170794]\n",
      " [  0.09562455]]\n",
      "the iterations of  531100 the loss is 0.24912100651078437 theta= [[-12.05567093]\n",
      " [  0.10171558]\n",
      " [  0.09563227]]\n",
      "the iterations of  531200 the loss is 0.249111722538693 theta= [[-12.05663445]\n",
      " [  0.10172321]\n",
      " [  0.09563999]]\n",
      "the iterations of  531300 the loss is 0.24910244161432352 theta= [[-12.0575978 ]\n",
      " [  0.10173085]\n",
      " [  0.0956477 ]]\n",
      "the iterations of  531400 the loss is 0.24909316373618978 theta= [[-12.058561  ]\n",
      " [  0.10173849]\n",
      " [  0.09565542]]\n",
      "the iterations of  531500 the loss is 0.24908388890280672 theta= [[-12.05952404]\n",
      " [  0.10174612]\n",
      " [  0.09566313]]\n",
      "the iterations of  531600 the loss is 0.24907461711269005 theta= [[-12.06048692]\n",
      " [  0.10175375]\n",
      " [  0.09567084]]\n",
      "the iterations of  531700 the loss is 0.24906534836435687 theta= [[-12.06144965]\n",
      " [  0.10176139]\n",
      " [  0.09567856]]\n",
      "the iterations of  531800 the loss is 0.2490560826563246 theta= [[-12.06241222]\n",
      " [  0.10176902]\n",
      " [  0.09568627]]\n",
      "the iterations of  531900 the loss is 0.24904681998711228 theta= [[-12.06337463]\n",
      " [  0.10177664]\n",
      " [  0.09569397]]\n",
      "the iterations of  532000 the loss is 0.24903756035523972 theta= [[-12.06433688]\n",
      " [  0.10178427]\n",
      " [  0.09570168]]\n",
      "the iterations of  532100 the loss is 0.24902830375922766 theta= [[-12.06529897]\n",
      " [  0.1017919 ]\n",
      " [  0.09570939]]\n",
      "the iterations of  532200 the loss is 0.24901905019759765 theta= [[-12.06626091]\n",
      " [  0.10179953]\n",
      " [  0.09571709]]\n",
      "the iterations of  532300 the loss is 0.24900979966887257 theta= [[-12.06722269]\n",
      " [  0.10180715]\n",
      " [  0.0957248 ]]\n",
      "the iterations of  532400 the loss is 0.24900055217157596 theta= [[-12.06818431]\n",
      " [  0.10181477]\n",
      " [  0.0957325 ]]\n",
      "the iterations of  532500 the loss is 0.24899130770423247 theta= [[-12.06914577]\n",
      " [  0.10182239]\n",
      " [  0.0957402 ]]\n",
      "the iterations of  532600 the loss is 0.24898206626536776 theta= [[-12.07010707]\n",
      " [  0.10183002]\n",
      " [  0.0957479 ]]\n",
      "the iterations of  532700 the loss is 0.24897282785350835 theta= [[-12.07106822]\n",
      " [  0.10183763]\n",
      " [  0.0957556 ]]\n",
      "the iterations of  532800 the loss is 0.24896359246718164 theta= [[-12.07202921]\n",
      " [  0.10184525]\n",
      " [  0.0957633 ]]\n",
      "the iterations of  532900 the loss is 0.24895436010491634 theta= [[-12.07299005]\n",
      " [  0.10185287]\n",
      " [  0.095771  ]]\n",
      "the iterations of  533000 the loss is 0.24894513076524188 theta= [[-12.07395072]\n",
      " [  0.10186049]\n",
      " [  0.09577869]]\n",
      "the iterations of  533100 the loss is 0.2489359044466887 theta= [[-12.07491124]\n",
      " [  0.1018681 ]\n",
      " [  0.09578639]]\n",
      "the iterations of  533200 the loss is 0.24892668114778815 theta= [[-12.0758716 ]\n",
      " [  0.10187571]\n",
      " [  0.09579408]]\n",
      "the iterations of  533300 the loss is 0.24891746086707262 theta= [[-12.07683181]\n",
      " [  0.10188333]\n",
      " [  0.09580177]]\n",
      "the iterations of  533400 the loss is 0.2489082436030754 theta= [[-12.07779186]\n",
      " [  0.10189094]\n",
      " [  0.09580946]]\n",
      "the iterations of  533500 the loss is 0.24889902935433075 theta= [[-12.07875175]\n",
      " [  0.10189855]\n",
      " [  0.09581715]]\n",
      "the iterations of  533600 the loss is 0.24888981811937416 theta= [[-12.07971148]\n",
      " [  0.10190615]\n",
      " [  0.09582484]]\n",
      "the iterations of  533700 the loss is 0.2488806098967416 theta= [[-12.08067106]\n",
      " [  0.10191376]\n",
      " [  0.09583253]]\n",
      "the iterations of  533800 the loss is 0.2488714046849703 theta= [[-12.08163048]\n",
      " [  0.10192137]\n",
      " [  0.09584021]]\n",
      "the iterations of  533900 the loss is 0.2488622024825985 theta= [[-12.08258974]\n",
      " [  0.10192897]\n",
      " [  0.09584789]]\n",
      "the iterations of  534000 the loss is 0.2488530032881652 theta= [[-12.08354884]\n",
      " [  0.10193657]\n",
      " [  0.09585558]]\n",
      "the iterations of  534100 the loss is 0.2488438071002105 theta= [[-12.08450779]\n",
      " [  0.10194418]\n",
      " [  0.09586326]]\n",
      "the iterations of  534200 the loss is 0.24883461391727552 theta= [[-12.08546659]\n",
      " [  0.10195178]\n",
      " [  0.09587094]]\n",
      "the iterations of  534300 the loss is 0.2488254237379019 theta= [[-12.08642522]\n",
      " [  0.10195938]\n",
      " [  0.09587862]]\n",
      "the iterations of  534400 the loss is 0.24881623656063284 theta= [[-12.0873837 ]\n",
      " [  0.10196698]\n",
      " [  0.0958863 ]]\n",
      "the iterations of  534500 the loss is 0.2488070523840121 theta= [[-12.08834202]\n",
      " [  0.10197457]\n",
      " [  0.09589397]]\n",
      "the iterations of  534600 the loss is 0.24879787120658453 theta= [[-12.08930019]\n",
      " [  0.10198217]\n",
      " [  0.09590165]]\n",
      "the iterations of  534700 the loss is 0.24878869302689602 theta= [[-12.0902582 ]\n",
      " [  0.10198976]\n",
      " [  0.09590932]]\n",
      "the iterations of  534800 the loss is 0.24877951784349317 theta= [[-12.09121605]\n",
      " [  0.10199736]\n",
      " [  0.095917  ]]\n",
      "the iterations of  534900 the loss is 0.24877034565492373 theta= [[-12.09217375]\n",
      " [  0.10200495]\n",
      " [  0.09592467]]\n",
      "the iterations of  535000 the loss is 0.24876117645973628 theta= [[-12.09313129]\n",
      " [  0.10201254]\n",
      " [  0.09593234]]\n",
      "the iterations of  535100 the loss is 0.24875201025648055 theta= [[-12.09408867]\n",
      " [  0.10202013]\n",
      " [  0.09594001]]\n",
      "the iterations of  535200 the loss is 0.24874284704370703 theta= [[-12.0950459 ]\n",
      " [  0.10202772]\n",
      " [  0.09594768]]\n",
      "the iterations of  535300 the loss is 0.2487336868199671 theta= [[-12.09600298]\n",
      " [  0.10203531]\n",
      " [  0.09595534]]\n",
      "the iterations of  535400 the loss is 0.2487245295838132 theta= [[-12.09695989]\n",
      " [  0.10204289]\n",
      " [  0.09596301]]\n",
      "the iterations of  535500 the loss is 0.24871537533379887 theta= [[-12.09791665]\n",
      " [  0.10205048]\n",
      " [  0.09597067]]\n",
      "the iterations of  535600 the loss is 0.24870622406847834 theta= [[-12.09887326]\n",
      " [  0.10205806]\n",
      " [  0.09597834]]\n",
      "the iterations of  535700 the loss is 0.2486970757864068 theta= [[-12.0998297 ]\n",
      " [  0.10206565]\n",
      " [  0.095986  ]]\n",
      "the iterations of  535800 the loss is 0.24868793048614055 theta= [[-12.100786  ]\n",
      " [  0.10207323]\n",
      " [  0.09599366]]\n",
      "the iterations of  535900 the loss is 0.24867878816623687 theta= [[-12.10174213]\n",
      " [  0.10208081]\n",
      " [  0.09600132]]\n",
      "the iterations of  536000 the loss is 0.24866964882525383 theta= [[-12.10269811]\n",
      " [  0.10208839]\n",
      " [  0.09600898]]\n",
      "the iterations of  536100 the loss is 0.2486605124617502 theta= [[-12.10365394]\n",
      " [  0.10209596]\n",
      " [  0.09601664]]\n",
      "the iterations of  536200 the loss is 0.24865137907428628 theta= [[-12.10460961]\n",
      " [  0.10210354]\n",
      " [  0.09602429]]\n",
      "the iterations of  536300 the loss is 0.24864224866142284 theta= [[-12.10556512]\n",
      " [  0.10211112]\n",
      " [  0.09603195]]\n",
      "the iterations of  536400 the loss is 0.248633121221722 theta= [[-12.10652048]\n",
      " [  0.10211869]\n",
      " [  0.0960396 ]]\n",
      "the iterations of  536500 the loss is 0.2486239967537463 theta= [[-12.10747568]\n",
      " [  0.10212626]\n",
      " [  0.09604725]]\n",
      "the iterations of  536600 the loss is 0.24861487525605952 theta= [[-12.10843073]\n",
      " [  0.10213383]\n",
      " [  0.0960549 ]]\n",
      "the iterations of  536700 the loss is 0.24860575672722646 theta= [[-12.10938562]\n",
      " [  0.1021414 ]\n",
      " [  0.09606255]]\n",
      "the iterations of  536800 the loss is 0.2485966411658126 theta= [[-12.11034035]\n",
      " [  0.10214897]\n",
      " [  0.0960702 ]]\n",
      "the iterations of  536900 the loss is 0.24858752857038469 theta= [[-12.11129494]\n",
      " [  0.10215654]\n",
      " [  0.09607785]]\n",
      "the iterations of  537000 the loss is 0.24857841893951016 theta= [[-12.11224936]\n",
      " [  0.10216411]\n",
      " [  0.09608549]]\n",
      "the iterations of  537100 the loss is 0.24856931227175724 theta= [[-12.11320363]\n",
      " [  0.10217167]\n",
      " [  0.09609314]]\n",
      "the iterations of  537200 the loss is 0.24856020856569555 theta= [[-12.11415775]\n",
      " [  0.10217924]\n",
      " [  0.09610078]]\n",
      "the iterations of  537300 the loss is 0.24855110781989528 theta= [[-12.11511171]\n",
      " [  0.1021868 ]\n",
      " [  0.09610843]]\n",
      "the iterations of  537400 the loss is 0.2485420100329276 theta= [[-12.11606551]\n",
      " [  0.10219436]\n",
      " [  0.09611607]]\n",
      "the iterations of  537500 the loss is 0.24853291520336485 theta= [[-12.11701916]\n",
      " [  0.10220192]\n",
      " [  0.09612371]]\n",
      "the iterations of  537600 the loss is 0.24852382332977985 theta= [[-12.11797266]\n",
      " [  0.10220948]\n",
      " [  0.09613134]]\n",
      "the iterations of  537700 the loss is 0.24851473441074684 theta= [[-12.118926  ]\n",
      " [  0.10221704]\n",
      " [  0.09613898]]\n",
      "the iterations of  537800 the loss is 0.24850564844484072 theta= [[-12.11987918]\n",
      " [  0.1022246 ]\n",
      " [  0.09614662]]\n",
      "the iterations of  537900 the loss is 0.24849656543063736 theta= [[-12.12083221]\n",
      " [  0.10223215]\n",
      " [  0.09615425]]\n",
      "the iterations of  538000 the loss is 0.2484874853667135 theta= [[-12.12178509]\n",
      " [  0.10223971]\n",
      " [  0.09616189]]\n",
      "the iterations of  538100 the loss is 0.2484784082516469 theta= [[-12.12273781]\n",
      " [  0.10224726]\n",
      " [  0.09616952]]\n",
      "the iterations of  538200 the loss is 0.2484693340840164 theta= [[-12.12369037]\n",
      " [  0.10225482]\n",
      " [  0.09617715]]\n",
      "the iterations of  538300 the loss is 0.24846026286240142 theta= [[-12.12464278]\n",
      " [  0.10226237]\n",
      " [  0.09618478]]\n",
      "the iterations of  538400 the loss is 0.2484511945853824 theta= [[-12.12559504]\n",
      " [  0.10226992]\n",
      " [  0.09619241]]\n",
      "the iterations of  538500 the loss is 0.24844212925154094 theta= [[-12.12654714]\n",
      " [  0.10227746]\n",
      " [  0.09620004]]\n",
      "the iterations of  538600 the loss is 0.24843306685945923 theta= [[-12.12749909]\n",
      " [  0.10228501]\n",
      " [  0.09620766]]\n",
      "the iterations of  538700 the loss is 0.2484240074077206 theta= [[-12.12845088]\n",
      " [  0.10229256]\n",
      " [  0.09621529]]\n",
      "the iterations of  538800 the loss is 0.24841495089490936 theta= [[-12.12940252]\n",
      " [  0.1023001 ]\n",
      " [  0.09622291]]\n",
      "the iterations of  538900 the loss is 0.2484058973196106 theta= [[-12.13035401]\n",
      " [  0.10230765]\n",
      " [  0.09623054]]\n",
      "the iterations of  539000 the loss is 0.24839684668041032 theta= [[-12.13130534]\n",
      " [  0.10231519]\n",
      " [  0.09623816]]\n",
      "the iterations of  539100 the loss is 0.24838779897589536 theta= [[-12.13225651]\n",
      " [  0.10232273]\n",
      " [  0.09624578]]\n",
      "the iterations of  539200 the loss is 0.2483787542046538 theta= [[-12.13320753]\n",
      " [  0.10233027]\n",
      " [  0.0962534 ]]\n",
      "the iterations of  539300 the loss is 0.24836971236527414 theta= [[-12.1341584 ]\n",
      " [  0.10233781]\n",
      " [  0.09626102]]\n",
      "the iterations of  539400 the loss is 0.24836067345634646 theta= [[-12.13510911]\n",
      " [  0.10234535]\n",
      " [  0.09626863]]\n",
      "the iterations of  539500 the loss is 0.24835163747646125 theta= [[-12.13605967]\n",
      " [  0.10235288]\n",
      " [  0.09627625]]\n",
      "the iterations of  539600 the loss is 0.24834260442420997 theta= [[-12.13701008]\n",
      " [  0.10236042]\n",
      " [  0.09628386]]\n",
      "the iterations of  539700 the loss is 0.24833357429818534 theta= [[-12.13796033]\n",
      " [  0.10236795]\n",
      " [  0.09629148]]\n",
      "the iterations of  539800 the loss is 0.24832454709698035 theta= [[-12.13891043]\n",
      " [  0.10237549]\n",
      " [  0.09629909]]\n",
      "the iterations of  539900 the loss is 0.2483155228191896 theta= [[-12.13986037]\n",
      " [  0.10238302]\n",
      " [  0.0963067 ]]\n",
      "the iterations of  540000 the loss is 0.24830650146340807 theta= [[-12.14081016]\n",
      " [  0.10239055]\n",
      " [  0.09631431]]\n",
      "the iterations of  540100 the loss is 0.24829748302823199 theta= [[-12.1417598 ]\n",
      " [  0.10239808]\n",
      " [  0.09632192]]\n",
      "the iterations of  540200 the loss is 0.24828846751225847 theta= [[-12.14270928]\n",
      " [  0.10240561]\n",
      " [  0.09632952]]\n",
      "the iterations of  540300 the loss is 0.2482794549140853 theta= [[-12.14365861]\n",
      " [  0.10241313]\n",
      " [  0.09633713]]\n",
      "the iterations of  540400 the loss is 0.24827044523231137 theta= [[-12.14460778]\n",
      " [  0.10242066]\n",
      " [  0.09634473]]\n",
      "the iterations of  540500 the loss is 0.24826143846553644 theta= [[-12.1455568 ]\n",
      " [  0.10242818]\n",
      " [  0.09635234]]\n",
      "the iterations of  540600 the loss is 0.24825243461236116 theta= [[-12.14650567]\n",
      " [  0.10243571]\n",
      " [  0.09635994]]\n",
      "the iterations of  540700 the loss is 0.24824343367138713 theta= [[-12.14745439]\n",
      " [  0.10244323]\n",
      " [  0.09636754]]\n",
      "the iterations of  540800 the loss is 0.24823443564121672 theta= [[-12.14840295]\n",
      " [  0.10245075]\n",
      " [  0.09637514]]\n",
      "the iterations of  540900 the loss is 0.24822544052045334 theta= [[-12.14935136]\n",
      " [  0.10245827]\n",
      " [  0.09638274]]\n",
      "the iterations of  541000 the loss is 0.2482164483077015 theta= [[-12.15029961]\n",
      " [  0.10246579]\n",
      " [  0.09639034]]\n",
      "the iterations of  541100 the loss is 0.24820745900156616 theta= [[-12.15124771]\n",
      " [  0.1024733 ]\n",
      " [  0.09639793]]\n",
      "the iterations of  541200 the loss is 0.24819847260065347 theta= [[-12.15219566]\n",
      " [  0.10248082]\n",
      " [  0.09640553]]\n",
      "the iterations of  541300 the loss is 0.24818948910357036 theta= [[-12.15314345]\n",
      " [  0.10248834]\n",
      " [  0.09641312]]\n",
      "the iterations of  541400 the loss is 0.24818050850892484 theta= [[-12.1540911 ]\n",
      " [  0.10249585]\n",
      " [  0.09642071]]\n",
      "the iterations of  541500 the loss is 0.24817153081532559 theta= [[-12.15503858]\n",
      " [  0.10250336]\n",
      " [  0.0964283 ]]\n",
      "the iterations of  541600 the loss is 0.24816255602138249 theta= [[-12.15598592]\n",
      " [  0.10251087]\n",
      " [  0.09643589]]\n",
      "the iterations of  541700 the loss is 0.24815358412570598 theta= [[-12.1569331 ]\n",
      " [  0.10251838]\n",
      " [  0.09644348]]\n",
      "the iterations of  541800 the loss is 0.24814461512690766 theta= [[-12.15788013]\n",
      " [  0.10252589]\n",
      " [  0.09645107]]\n",
      "the iterations of  541900 the loss is 0.24813564902359972 theta= [[-12.15882701]\n",
      " [  0.1025334 ]\n",
      " [  0.09645866]]\n",
      "the iterations of  542000 the loss is 0.24812668581439568 theta= [[-12.15977373]\n",
      " [  0.10254091]\n",
      " [  0.09646624]]\n",
      "the iterations of  542100 the loss is 0.2481177254979097 theta= [[-12.1607203 ]\n",
      " [  0.10254841]\n",
      " [  0.09647383]]\n",
      "the iterations of  542200 the loss is 0.2481087680727569 theta= [[-12.16166672]\n",
      " [  0.10255592]\n",
      " [  0.09648141]]\n",
      "the iterations of  542300 the loss is 0.24809981353755314 theta= [[-12.16261299]\n",
      " [  0.10256342]\n",
      " [  0.09648899]]\n",
      "the iterations of  542400 the loss is 0.24809086189091545 theta= [[-12.1635591 ]\n",
      " [  0.10257092]\n",
      " [  0.09649657]]\n",
      "the iterations of  542500 the loss is 0.24808191313146144 theta= [[-12.16450506]\n",
      " [  0.10257842]\n",
      " [  0.09650415]]\n",
      "the iterations of  542600 the loss is 0.24807296725780995 theta= [[-12.16545087]\n",
      " [  0.10258592]\n",
      " [  0.09651173]]\n",
      "the iterations of  542700 the loss is 0.24806402426858032 theta= [[-12.16639652]\n",
      " [  0.10259342]\n",
      " [  0.09651931]]\n",
      "the iterations of  542800 the loss is 0.2480550841623933 theta= [[-12.16734203]\n",
      " [  0.10260091]\n",
      " [  0.09652688]]\n",
      "the iterations of  542900 the loss is 0.2480461469378699 theta= [[-12.16828738]\n",
      " [  0.10260841]\n",
      " [  0.09653446]]\n",
      "the iterations of  543000 the loss is 0.24803721259363257 theta= [[-12.16923258]\n",
      " [  0.1026159 ]\n",
      " [  0.09654203]]\n",
      "the iterations of  543100 the loss is 0.2480282811283043 theta= [[-12.17017762]\n",
      " [  0.1026234 ]\n",
      " [  0.0965496 ]]\n",
      "the iterations of  543200 the loss is 0.24801935254050925 theta= [[-12.17112252]\n",
      " [  0.10263089]\n",
      " [  0.09655717]]\n",
      "the iterations of  543300 the loss is 0.24801042682887242 theta= [[-12.17206726]\n",
      " [  0.10263838]\n",
      " [  0.09656474]]\n",
      "the iterations of  543400 the loss is 0.24800150399201923 theta= [[-12.17301185]\n",
      " [  0.10264587]\n",
      " [  0.09657231]]\n",
      "the iterations of  543500 the loss is 0.24799258402857652 theta= [[-12.17395628]\n",
      " [  0.10265336]\n",
      " [  0.09657988]]\n",
      "the iterations of  543600 the loss is 0.24798366693717203 theta= [[-12.17490057]\n",
      " [  0.10266085]\n",
      " [  0.09658744]]\n",
      "the iterations of  543700 the loss is 0.24797475271643404 theta= [[-12.1758447 ]\n",
      " [  0.10266833]\n",
      " [  0.09659501]]\n",
      "the iterations of  543800 the loss is 0.24796584136499183 theta= [[-12.17678868]\n",
      " [  0.10267582]\n",
      " [  0.09660257]]\n",
      "the iterations of  543900 the loss is 0.2479569328814757 theta= [[-12.17773251]\n",
      " [  0.1026833 ]\n",
      " [  0.09661013]]\n",
      "the iterations of  544000 the loss is 0.24794802726451678 theta= [[-12.17867619]\n",
      " [  0.10269078]\n",
      " [  0.0966177 ]]\n",
      "the iterations of  544100 the loss is 0.24793912451274694 theta= [[-12.17961971]\n",
      " [  0.10269827]\n",
      " [  0.09662526]]\n",
      "the iterations of  544200 the loss is 0.24793022462479913 theta= [[-12.18056309]\n",
      " [  0.10270575]\n",
      " [  0.09663281]]\n",
      "the iterations of  544300 the loss is 0.24792132759930724 theta= [[-12.18150631]\n",
      " [  0.10271323]\n",
      " [  0.09664037]]\n",
      "the iterations of  544400 the loss is 0.2479124334349056 theta= [[-12.18244938]\n",
      " [  0.1027207 ]\n",
      " [  0.09664793]]\n",
      "the iterations of  544500 the loss is 0.24790354213022986 theta= [[-12.1833923 ]\n",
      " [  0.10272818]\n",
      " [  0.09665548]]\n",
      "the iterations of  544600 the loss is 0.24789465368391642 theta= [[-12.18433507]\n",
      " [  0.10273566]\n",
      " [  0.09666304]]\n",
      "the iterations of  544700 the loss is 0.24788576809460242 theta= [[-12.18527768]\n",
      " [  0.10274313]\n",
      " [  0.09667059]]\n",
      "the iterations of  544800 the loss is 0.24787688536092609 theta= [[-12.18622015]\n",
      " [  0.1027506 ]\n",
      " [  0.09667814]]\n",
      "the iterations of  544900 the loss is 0.2478680054815266 theta= [[-12.18716246]\n",
      " [  0.10275807]\n",
      " [  0.09668569]]\n",
      "the iterations of  545000 the loss is 0.2478591284550435 theta= [[-12.18810462]\n",
      " [  0.10276555]\n",
      " [  0.09669324]]\n",
      "the iterations of  545100 the loss is 0.2478502542801179 theta= [[-12.18904663]\n",
      " [  0.10277301]\n",
      " [  0.09670079]]\n",
      "the iterations of  545200 the loss is 0.24784138295539124 theta= [[-12.18998849]\n",
      " [  0.10278048]\n",
      " [  0.09670834]]\n",
      "the iterations of  545300 the loss is 0.2478325144795062 theta= [[-12.1909302 ]\n",
      " [  0.10278795]\n",
      " [  0.09671588]]\n",
      "the iterations of  545400 the loss is 0.2478236488511058 theta= [[-12.19187175]\n",
      " [  0.10279542]\n",
      " [  0.09672343]]\n",
      "the iterations of  545500 the loss is 0.24781478606883475 theta= [[-12.19281316]\n",
      " [  0.10280288]\n",
      " [  0.09673097]]\n",
      "the iterations of  545600 the loss is 0.247805926131338 theta= [[-12.19375441]\n",
      " [  0.10281035]\n",
      " [  0.09673851]]\n",
      "the iterations of  545700 the loss is 0.24779706903726142 theta= [[-12.19469552]\n",
      " [  0.10281781]\n",
      " [  0.09674605]]\n",
      "the iterations of  545800 the loss is 0.2477882147852522 theta= [[-12.19563647]\n",
      " [  0.10282527]\n",
      " [  0.09675359]]\n",
      "the iterations of  545900 the loss is 0.24777936337395767 theta= [[-12.19657727]\n",
      " [  0.10283273]\n",
      " [  0.09676113]]\n",
      "the iterations of  546000 the loss is 0.2477705148020267 theta= [[-12.19751792]\n",
      " [  0.10284019]\n",
      " [  0.09676867]]\n",
      "the iterations of  546100 the loss is 0.24776166906810881 theta= [[-12.19845842]\n",
      " [  0.10284765]\n",
      " [  0.09677621]]\n",
      "the iterations of  546200 the loss is 0.24775282617085448 theta= [[-12.19939877]\n",
      " [  0.1028551 ]\n",
      " [  0.09678374]]\n",
      "the iterations of  546300 the loss is 0.24774398610891463 theta= [[-12.20033896]\n",
      " [  0.10286256]\n",
      " [  0.09679127]]\n",
      "the iterations of  546400 the loss is 0.2477351488809413 theta= [[-12.20127901]\n",
      " [  0.10287001]\n",
      " [  0.09679881]]\n",
      "the iterations of  546500 the loss is 0.24772631448558768 theta= [[-12.20221891]\n",
      " [  0.10287746]\n",
      " [  0.09680634]]\n",
      "the iterations of  546600 the loss is 0.2477174829215074 theta= [[-12.20315865]\n",
      " [  0.10288492]\n",
      " [  0.09681387]]\n",
      "the iterations of  546700 the loss is 0.24770865418735533 theta= [[-12.20409825]\n",
      " [  0.10289237]\n",
      " [  0.0968214 ]]\n",
      "the iterations of  546800 the loss is 0.24769982828178677 theta= [[-12.20503769]\n",
      " [  0.10289982]\n",
      " [  0.09682892]]\n",
      "the iterations of  546900 the loss is 0.24769100520345833 theta= [[-12.20597699]\n",
      " [  0.10290726]\n",
      " [  0.09683645]]\n",
      "the iterations of  547000 the loss is 0.24768218495102715 theta= [[-12.20691613]\n",
      " [  0.10291471]\n",
      " [  0.09684398]]\n",
      "the iterations of  547100 the loss is 0.24767336752315144 theta= [[-12.20785512]\n",
      " [  0.10292216]\n",
      " [  0.0968515 ]]\n",
      "the iterations of  547200 the loss is 0.24766455291849013 theta= [[-12.20879397]\n",
      " [  0.1029296 ]\n",
      " [  0.09685902]]\n",
      "the iterations of  547300 the loss is 0.2476557411357032 theta= [[-12.20973266]\n",
      " [  0.10293705]\n",
      " [  0.09686655]]\n",
      "the iterations of  547400 the loss is 0.24764693217345118 theta= [[-12.2106712 ]\n",
      " [  0.10294449]\n",
      " [  0.09687407]]\n",
      "the iterations of  547500 the loss is 0.24763812603039562 theta= [[-12.21160959]\n",
      " [  0.10295193]\n",
      " [  0.09688159]]\n",
      "the iterations of  547600 the loss is 0.247629322705199 theta= [[-12.21254784]\n",
      " [  0.10295937]\n",
      " [  0.0968891 ]]\n",
      "the iterations of  547700 the loss is 0.24762052219652464 theta= [[-12.21348593]\n",
      " [  0.10296681]\n",
      " [  0.09689662]]\n",
      "the iterations of  547800 the loss is 0.24761172450303667 theta= [[-12.21442387]\n",
      " [  0.10297425]\n",
      " [  0.09690414]]\n",
      "the iterations of  547900 the loss is 0.24760292962340016 theta= [[-12.21536166]\n",
      " [  0.10298168]\n",
      " [  0.09691165]]\n",
      "the iterations of  548000 the loss is 0.24759413755628087 theta= [[-12.2162993 ]\n",
      " [  0.10298912]\n",
      " [  0.09691917]]\n",
      "the iterations of  548100 the loss is 0.2475853483003455 theta= [[-12.21723679]\n",
      " [  0.10299655]\n",
      " [  0.09692668]]\n",
      "the iterations of  548200 the loss is 0.2475765618542617 theta= [[-12.21817414]\n",
      " [  0.10300399]\n",
      " [  0.09693419]]\n",
      "the iterations of  548300 the loss is 0.24756777821669762 theta= [[-12.21911133]\n",
      " [  0.10301142]\n",
      " [  0.0969417 ]]\n",
      "the iterations of  548400 the loss is 0.24755899738632284 theta= [[-12.22004837]\n",
      " [  0.10301885]\n",
      " [  0.09694921]]\n",
      "the iterations of  548500 the loss is 0.24755021936180718 theta= [[-12.22098526]\n",
      " [  0.10302628]\n",
      " [  0.09695672]]\n",
      "the iterations of  548600 the loss is 0.24754144414182183 theta= [[-12.22192201]\n",
      " [  0.10303371]\n",
      " [  0.09696422]]\n",
      "the iterations of  548700 the loss is 0.24753267172503843 theta= [[-12.2228586 ]\n",
      " [  0.10304113]\n",
      " [  0.09697173]]\n",
      "the iterations of  548800 the loss is 0.24752390211012976 theta= [[-12.22379504]\n",
      " [  0.10304856]\n",
      " [  0.09697923]]\n",
      "the iterations of  548900 the loss is 0.2475151352957694 theta= [[-12.22473134]\n",
      " [  0.10305598]\n",
      " [  0.09698673]]\n",
      "the iterations of  549000 the loss is 0.24750637128063144 theta= [[-12.22566748]\n",
      " [  0.10306341]\n",
      " [  0.09699424]]\n",
      "the iterations of  549100 the loss is 0.24749761006339127 theta= [[-12.22660348]\n",
      " [  0.10307083]\n",
      " [  0.09700174]]\n",
      "the iterations of  549200 the loss is 0.24748885164272497 theta= [[-12.22753933]\n",
      " [  0.10307825]\n",
      " [  0.09700924]]\n",
      "the iterations of  549300 the loss is 0.24748009601730941 theta= [[-12.22847502]\n",
      " [  0.10308567]\n",
      " [  0.09701673]]\n",
      "the iterations of  549400 the loss is 0.2474713431858222 theta= [[-12.22941057]\n",
      " [  0.10309309]\n",
      " [  0.09702423]]\n",
      "the iterations of  549500 the loss is 0.24746259314694213 theta= [[-12.23034597]\n",
      " [  0.10310051]\n",
      " [  0.09703173]]\n",
      "the iterations of  549600 the loss is 0.24745384589934843 theta= [[-12.23128122]\n",
      " [  0.10310792]\n",
      " [  0.09703922]]\n",
      "the iterations of  549700 the loss is 0.24744510144172158 theta= [[-12.23221632]\n",
      " [  0.10311534]\n",
      " [  0.09704672]]\n",
      "the iterations of  549800 the loss is 0.24743635977274256 theta= [[-12.23315127]\n",
      " [  0.10312275]\n",
      " [  0.09705421]]\n",
      "the iterations of  549900 the loss is 0.24742762089109344 theta= [[-12.23408607]\n",
      " [  0.10313017]\n",
      " [  0.0970617 ]]\n",
      "the iterations of  550000 the loss is 0.24741888479545696 theta= [[-12.23502072]\n",
      " [  0.10313758]\n",
      " [  0.09706919]]\n",
      "the iterations of  550100 the loss is 0.2474101514845168 theta= [[-12.23595522]\n",
      " [  0.10314499]\n",
      " [  0.09707668]]\n",
      "the iterations of  550200 the loss is 0.2474014209569575 theta= [[-12.23688958]\n",
      " [  0.1031524 ]\n",
      " [  0.09708417]]\n",
      "the iterations of  550300 the loss is 0.2473926932114642 theta= [[-12.23782378]\n",
      " [  0.10315981]\n",
      " [  0.09709165]]\n",
      "the iterations of  550400 the loss is 0.2473839682467231 theta= [[-12.23875784]\n",
      " [  0.10316722]\n",
      " [  0.09709914]]\n",
      "the iterations of  550500 the loss is 0.24737524606142142 theta= [[-12.23969175]\n",
      " [  0.10317462]\n",
      " [  0.09710662]]\n",
      "the iterations of  550600 the loss is 0.24736652665424677 theta= [[-12.24062551]\n",
      " [  0.10318203]\n",
      " [  0.0971141 ]]\n",
      "the iterations of  550700 the loss is 0.24735781002388793 theta= [[-12.24155912]\n",
      " [  0.10318943]\n",
      " [  0.09712159]]\n",
      "the iterations of  550800 the loss is 0.24734909616903442 theta= [[-12.24249258]\n",
      " [  0.10319683]\n",
      " [  0.09712907]]\n",
      "the iterations of  550900 the loss is 0.24734038508837652 theta= [[-12.24342589]\n",
      " [  0.10320423]\n",
      " [  0.09713655]]\n",
      "the iterations of  551000 the loss is 0.24733167678060547 theta= [[-12.24435906]\n",
      " [  0.10321163]\n",
      " [  0.09714402]]\n",
      "the iterations of  551100 the loss is 0.24732297124441321 theta= [[-12.24529208]\n",
      " [  0.10321903]\n",
      " [  0.0971515 ]]\n",
      "the iterations of  551200 the loss is 0.24731426847849264 theta= [[-12.24622494]\n",
      " [  0.10322643]\n",
      " [  0.09715898]]\n",
      "the iterations of  551300 the loss is 0.24730556848153742 theta= [[-12.24715766]\n",
      " [  0.10323383]\n",
      " [  0.09716645]]\n",
      "the iterations of  551400 the loss is 0.2472968712522421 theta= [[-12.24809023]\n",
      " [  0.10324122]\n",
      " [  0.09717393]]\n",
      "the iterations of  551500 the loss is 0.24728817678930198 theta= [[-12.24902266]\n",
      " [  0.10324862]\n",
      " [  0.0971814 ]]\n",
      "the iterations of  551600 the loss is 0.24727948509141334 theta= [[-12.24995493]\n",
      " [  0.10325601]\n",
      " [  0.09718887]]\n",
      "the iterations of  551700 the loss is 0.2472707961572731 theta= [[-12.25088706]\n",
      " [  0.1032634 ]\n",
      " [  0.09719634]]\n",
      "the iterations of  551800 the loss is 0.24726210998557888 theta= [[-12.25181903]\n",
      " [  0.10327079]\n",
      " [  0.09720381]]\n",
      "the iterations of  551900 the loss is 0.24725342657502974 theta= [[-12.25275086]\n",
      " [  0.10327818]\n",
      " [  0.09721128]]\n",
      "the iterations of  552000 the loss is 0.24724474592432483 theta= [[-12.25368255]\n",
      " [  0.10328557]\n",
      " [  0.09721874]]\n",
      "the iterations of  552100 the loss is 0.2472360680321647 theta= [[-12.25461408]\n",
      " [  0.10329296]\n",
      " [  0.09722621]]\n",
      "the iterations of  552200 the loss is 0.24722739289725038 theta= [[-12.25554547]\n",
      " [  0.10330035]\n",
      " [  0.09723367]]\n",
      "the iterations of  552300 the loss is 0.24721872051828378 theta= [[-12.2564767 ]\n",
      " [  0.10330773]\n",
      " [  0.09724113]]\n",
      "the iterations of  552400 the loss is 0.2472100508939677 theta= [[-12.25740779]\n",
      " [  0.10331512]\n",
      " [  0.0972486 ]]\n",
      "the iterations of  552500 the loss is 0.24720138402300598 theta= [[-12.25833874]\n",
      " [  0.1033225 ]\n",
      " [  0.09725606]]\n",
      "the iterations of  552600 the loss is 0.24719271990410296 theta= [[-12.25926953]\n",
      " [  0.10332988]\n",
      " [  0.09726352]]\n",
      "the iterations of  552700 the loss is 0.24718405853596387 theta= [[-12.26020018]\n",
      " [  0.10333726]\n",
      " [  0.09727097]]\n",
      "the iterations of  552800 the loss is 0.2471753999172946 theta= [[-12.26113067]\n",
      " [  0.10334464]\n",
      " [  0.09727843]]\n",
      "the iterations of  552900 the loss is 0.24716674404680242 theta= [[-12.26206103]\n",
      " [  0.10335202]\n",
      " [  0.09728589]]\n",
      "the iterations of  553000 the loss is 0.24715809092319482 theta= [[-12.26299123]\n",
      " [  0.1033594 ]\n",
      " [  0.09729334]]\n",
      "the iterations of  553100 the loss is 0.2471494405451805 theta= [[-12.26392128]\n",
      " [  0.10336677]\n",
      " [  0.0973008 ]]\n",
      "the iterations of  553200 the loss is 0.24714079291146873 theta= [[-12.26485119]\n",
      " [  0.10337415]\n",
      " [  0.09730825]]\n",
      "the iterations of  553300 the loss is 0.24713214802076966 theta= [[-12.26578095]\n",
      " [  0.10338152]\n",
      " [  0.0973157 ]]\n",
      "the iterations of  553400 the loss is 0.24712350587179446 theta= [[-12.26671057]\n",
      " [  0.10338889]\n",
      " [  0.09732315]]\n",
      "the iterations of  553500 the loss is 0.24711486646325487 theta= [[-12.26764003]\n",
      " [  0.10339626]\n",
      " [  0.0973306 ]]\n",
      "the iterations of  553600 the loss is 0.24710622979386357 theta= [[-12.26856935]\n",
      " [  0.10340363]\n",
      " [  0.09733805]]\n",
      "the iterations of  553700 the loss is 0.24709759586233399 theta= [[-12.26949852]\n",
      " [  0.103411  ]\n",
      " [  0.09734549]]\n",
      "the iterations of  553800 the loss is 0.2470889646673803 theta= [[-12.27042755]\n",
      " [  0.10341837]\n",
      " [  0.09735294]]\n",
      "the iterations of  553900 the loss is 0.24708033620771785 theta= [[-12.27135642]\n",
      " [  0.10342574]\n",
      " [  0.09736038]]\n",
      "the iterations of  554000 the loss is 0.2470717104820624 theta= [[-12.27228515]\n",
      " [  0.1034331 ]\n",
      " [  0.09736783]]\n",
      "the iterations of  554100 the loss is 0.24706308748913053 theta= [[-12.27321373]\n",
      " [  0.10344047]\n",
      " [  0.09737527]]\n",
      "the iterations of  554200 the loss is 0.24705446722764013 theta= [[-12.27414217]\n",
      " [  0.10344783]\n",
      " [  0.09738271]]\n",
      "the iterations of  554300 the loss is 0.2470458496963091 theta= [[-12.27507046]\n",
      " [  0.10345519]\n",
      " [  0.09739015]]\n",
      "the iterations of  554400 the loss is 0.247037234893857 theta= [[-12.2759986 ]\n",
      " [  0.10346255]\n",
      " [  0.09739759]]\n",
      "the iterations of  554500 the loss is 0.24702862281900356 theta= [[-12.27692659]\n",
      " [  0.10346991]\n",
      " [  0.09740503]]\n",
      "the iterations of  554600 the loss is 0.24702001347046973 theta= [[-12.27785444]\n",
      " [  0.10347727]\n",
      " [  0.09741246]]\n",
      "the iterations of  554700 the loss is 0.24701140684697687 theta= [[-12.27878214]\n",
      " [  0.10348463]\n",
      " [  0.0974199 ]]\n",
      "the iterations of  554800 the loss is 0.24700280294724777 theta= [[-12.2797097 ]\n",
      " [  0.10349199]\n",
      " [  0.09742733]]\n",
      "the iterations of  554900 the loss is 0.2469942017700052 theta= [[-12.2806371 ]\n",
      " [  0.10349934]\n",
      " [  0.09743476]]\n",
      "the iterations of  555000 the loss is 0.24698560331397348 theta= [[-12.28156436]\n",
      " [  0.1035067 ]\n",
      " [  0.0974422 ]]\n",
      "the iterations of  555100 the loss is 0.24697700757787744 theta= [[-12.28249148]\n",
      " [  0.10351405]\n",
      " [  0.09744963]]\n",
      "the iterations of  555200 the loss is 0.24696841456044244 theta= [[-12.28341844]\n",
      " [  0.1035214 ]\n",
      " [  0.09745705]]\n",
      "the iterations of  555300 the loss is 0.24695982426039517 theta= [[-12.28434526]\n",
      " [  0.10352875]\n",
      " [  0.09746448]]\n",
      "the iterations of  555400 the loss is 0.24695123667646274 theta= [[-12.28527194]\n",
      " [  0.1035361 ]\n",
      " [  0.09747191]]\n",
      "the iterations of  555500 the loss is 0.2469426518073734 theta= [[-12.28619846]\n",
      " [  0.10354345]\n",
      " [  0.09747934]]\n",
      "the iterations of  555600 the loss is 0.24693406965185566 theta= [[-12.28712485]\n",
      " [  0.1035508 ]\n",
      " [  0.09748676]]\n",
      "the iterations of  555700 the loss is 0.24692549020863958 theta= [[-12.28805108]\n",
      " [  0.10355814]\n",
      " [  0.09749418]]\n",
      "the iterations of  555800 the loss is 0.2469169134764554 theta= [[-12.28897717]\n",
      " [  0.10356549]\n",
      " [  0.09750161]]\n",
      "the iterations of  555900 the loss is 0.24690833945403445 theta= [[-12.28990311]\n",
      " [  0.10357283]\n",
      " [  0.09750903]]\n",
      "the iterations of  556000 the loss is 0.2468997681401088 theta= [[-12.29082891]\n",
      " [  0.10358017]\n",
      " [  0.09751645]]\n",
      "the iterations of  556100 the loss is 0.2468911995334112 theta= [[-12.29175456]\n",
      " [  0.10358751]\n",
      " [  0.09752387]]\n",
      "the iterations of  556200 the loss is 0.24688263363267546 theta= [[-12.29268006]\n",
      " [  0.10359485]\n",
      " [  0.09753128]]\n",
      "the iterations of  556300 the loss is 0.24687407043663612 theta= [[-12.29360542]\n",
      " [  0.10360219]\n",
      " [  0.0975387 ]]\n",
      "the iterations of  556400 the loss is 0.24686550994402828 theta= [[-12.29453063]\n",
      " [  0.10360953]\n",
      " [  0.09754612]]\n",
      "the iterations of  556500 the loss is 0.24685695215358802 theta= [[-12.29545569]\n",
      " [  0.10361687]\n",
      " [  0.09755353]]\n",
      "the iterations of  556600 the loss is 0.2468483970640521 theta= [[-12.29638061]\n",
      " [  0.1036242 ]\n",
      " [  0.09756094]]\n",
      "the iterations of  556700 the loss is 0.2468398446741585 theta= [[-12.29730538]\n",
      " [  0.10363154]\n",
      " [  0.09756836]]\n",
      "the iterations of  556800 the loss is 0.2468312949826455 theta= [[-12.29823001]\n",
      " [  0.10363887]\n",
      " [  0.09757577]]\n",
      "the iterations of  556900 the loss is 0.2468227479882524 theta= [[-12.29915449]\n",
      " [  0.1036462 ]\n",
      " [  0.09758318]]\n",
      "the iterations of  557000 the loss is 0.24681420368971918 theta= [[-12.30007883]\n",
      " [  0.10365353]\n",
      " [  0.09759058]]\n",
      "the iterations of  557100 the loss is 0.2468056620857867 theta= [[-12.30100302]\n",
      " [  0.10366086]\n",
      " [  0.09759799]]\n",
      "the iterations of  557200 the loss is 0.2467971231751966 theta= [[-12.30192706]\n",
      " [  0.10366819]\n",
      " [  0.0976054 ]]\n",
      "the iterations of  557300 the loss is 0.2467885869566913 theta= [[-12.30285096]\n",
      " [  0.10367552]\n",
      " [  0.0976128 ]]\n",
      "the iterations of  557400 the loss is 0.24678005342901416 theta= [[-12.30377471]\n",
      " [  0.10368285]\n",
      " [  0.09762021]]\n",
      "the iterations of  557500 the loss is 0.24677152259090904 theta= [[-12.30469832]\n",
      " [  0.10369017]\n",
      " [  0.09762761]]\n",
      "the iterations of  557600 the loss is 0.24676299444112065 theta= [[-12.30562178]\n",
      " [  0.1036975 ]\n",
      " [  0.09763501]]\n",
      "the iterations of  557700 the loss is 0.24675446897839484 theta= [[-12.3065451 ]\n",
      " [  0.10370482]\n",
      " [  0.09764241]]\n",
      "the iterations of  557800 the loss is 0.24674594620147805 theta= [[-12.30746827]\n",
      " [  0.10371214]\n",
      " [  0.09764981]]\n",
      "the iterations of  557900 the loss is 0.2467374261091173 theta= [[-12.30839129]\n",
      " [  0.10371946]\n",
      " [  0.09765721]]\n",
      "the iterations of  558000 the loss is 0.24672890870006053 theta= [[-12.30931417]\n",
      " [  0.10372678]\n",
      " [  0.09766461]]\n",
      "the iterations of  558100 the loss is 0.24672039397305665 theta= [[-12.31023691]\n",
      " [  0.1037341 ]\n",
      " [  0.097672  ]]\n",
      "the iterations of  558200 the loss is 0.24671188192685495 theta= [[-12.3111595 ]\n",
      " [  0.10374142]\n",
      " [  0.0976794 ]]\n",
      "the iterations of  558300 the loss is 0.24670337256020602 theta= [[-12.31208194]\n",
      " [  0.10374874]\n",
      " [  0.09768679]]\n",
      "the iterations of  558400 the loss is 0.24669486587186085 theta= [[-12.31300424]\n",
      " [  0.10375605]\n",
      " [  0.09769418]]\n",
      "the iterations of  558500 the loss is 0.2466863618605714 theta= [[-12.31392639]\n",
      " [  0.10376336]\n",
      " [  0.09770157]]\n",
      "the iterations of  558600 the loss is 0.24667786052509036 theta= [[-12.3148484 ]\n",
      " [  0.10377068]\n",
      " [  0.09770896]]\n",
      "the iterations of  558700 the loss is 0.24666936186417124 theta= [[-12.31577026]\n",
      " [  0.10377799]\n",
      " [  0.09771635]]\n",
      "the iterations of  558800 the loss is 0.24666086587656819 theta= [[-12.31669198]\n",
      " [  0.1037853 ]\n",
      " [  0.09772374]]\n",
      "the iterations of  558900 the loss is 0.24665237256103625 theta= [[-12.31761356]\n",
      " [  0.10379261]\n",
      " [  0.09773113]]\n",
      "the iterations of  559000 the loss is 0.24664388191633144 theta= [[-12.31853498]\n",
      " [  0.10379992]\n",
      " [  0.09773851]]\n",
      "the iterations of  559100 the loss is 0.24663539394121023 theta= [[-12.31945627]\n",
      " [  0.10380722]\n",
      " [  0.0977459 ]]\n",
      "the iterations of  559200 the loss is 0.24662690863442996 theta= [[-12.32037741]\n",
      " [  0.10381453]\n",
      " [  0.09775328]]\n",
      "the iterations of  559300 the loss is 0.24661842599474887 theta= [[-12.3212984 ]\n",
      " [  0.10382184]\n",
      " [  0.09776066]]\n",
      "the iterations of  559400 the loss is 0.24660994602092612 theta= [[-12.32221925]\n",
      " [  0.10382914]\n",
      " [  0.09776804]]\n",
      "the iterations of  559500 the loss is 0.24660146871172103 theta= [[-12.32313996]\n",
      " [  0.10383644]\n",
      " [  0.09777542]]\n",
      "the iterations of  559600 the loss is 0.24659299406589444 theta= [[-12.32406052]\n",
      " [  0.10384374]\n",
      " [  0.0977828 ]]\n",
      "the iterations of  559700 the loss is 0.24658452208220752 theta= [[-12.32498093]\n",
      " [  0.10385104]\n",
      " [  0.09779018]]\n",
      "the iterations of  559800 the loss is 0.24657605275942213 theta= [[-12.3259012 ]\n",
      " [  0.10385834]\n",
      " [  0.09779756]]\n",
      "the iterations of  559900 the loss is 0.24656758609630158 theta= [[-12.32682133]\n",
      " [  0.10386564]\n",
      " [  0.09780493]]\n",
      "the iterations of  560000 the loss is 0.24655912209160913 theta= [[-12.32774131]\n",
      " [  0.10387294]\n",
      " [  0.09781231]]\n",
      "the iterations of  560100 the loss is 0.24655066074410925 theta= [[-12.32866115]\n",
      " [  0.10388023]\n",
      " [  0.09781968]]\n",
      "the iterations of  560200 the loss is 0.2465422020525671 theta= [[-12.32958084]\n",
      " [  0.10388753]\n",
      " [  0.09782705]]\n",
      "the iterations of  560300 the loss is 0.24653374601574882 theta= [[-12.33050039]\n",
      " [  0.10389482]\n",
      " [  0.09783442]]\n",
      "the iterations of  560400 the loss is 0.246525292632421 theta= [[-12.33141979]\n",
      " [  0.10390211]\n",
      " [  0.09784179]]\n",
      "the iterations of  560500 the loss is 0.2465168419013512 theta= [[-12.33233905]\n",
      " [  0.10390941]\n",
      " [  0.09784916]]\n",
      "the iterations of  560600 the loss is 0.2465083938213075 theta= [[-12.33325817]\n",
      " [  0.1039167 ]\n",
      " [  0.09785653]]\n",
      "the iterations of  560700 the loss is 0.24649994839105907 theta= [[-12.33417714]\n",
      " [  0.10392399]\n",
      " [  0.09786389]]\n",
      "the iterations of  560800 the loss is 0.24649150560937574 theta= [[-12.33509597]\n",
      " [  0.10393127]\n",
      " [  0.09787126]]\n",
      "the iterations of  560900 the loss is 0.2464830654750281 theta= [[-12.33601465]\n",
      " [  0.10393856]\n",
      " [  0.09787862]]\n",
      "the iterations of  561000 the loss is 0.2464746279867875 theta= [[-12.33693319]\n",
      " [  0.10394585]\n",
      " [  0.09788598]]\n",
      "the iterations of  561100 the loss is 0.2464661931434259 theta= [[-12.33785159]\n",
      " [  0.10395313]\n",
      " [  0.09789335]]\n",
      "the iterations of  561200 the loss is 0.2464577609437165 theta= [[-12.33876984]\n",
      " [  0.10396041]\n",
      " [  0.09790071]]\n",
      "the iterations of  561300 the loss is 0.2464493313864327 theta= [[-12.33968795]\n",
      " [  0.1039677 ]\n",
      " [  0.09790806]]\n",
      "the iterations of  561400 the loss is 0.24644090447034914 theta= [[-12.34060591]\n",
      " [  0.10397498]\n",
      " [  0.09791542]]\n",
      "the iterations of  561500 the loss is 0.24643248019424088 theta= [[-12.34152373]\n",
      " [  0.10398226]\n",
      " [  0.09792278]]\n",
      "the iterations of  561600 the loss is 0.24642405855688387 theta= [[-12.34244141]\n",
      " [  0.10398954]\n",
      " [  0.09793014]]\n",
      "the iterations of  561700 the loss is 0.24641563955705492 theta= [[-12.34335894]\n",
      " [  0.10399681]\n",
      " [  0.09793749]]\n",
      "the iterations of  561800 the loss is 0.24640722319353145 theta= [[-12.34427633]\n",
      " [  0.10400409]\n",
      " [  0.09794484]]\n",
      "the iterations of  561900 the loss is 0.246398809465092 theta= [[-12.34519358]\n",
      " [  0.10401137]\n",
      " [  0.0979522 ]]\n",
      "the iterations of  562000 the loss is 0.24639039837051535 theta= [[-12.34611068]\n",
      " [  0.10401864]\n",
      " [  0.09795955]]\n",
      "the iterations of  562100 the loss is 0.24638198990858118 theta= [[-12.34702764]\n",
      " [  0.10402591]\n",
      " [  0.0979669 ]]\n",
      "the iterations of  562200 the loss is 0.24637358407807025 theta= [[-12.34794445]\n",
      " [  0.10403319]\n",
      " [  0.09797425]]\n",
      "the iterations of  562300 the loss is 0.24636518087776385 theta= [[-12.34886112]\n",
      " [  0.10404046]\n",
      " [  0.0979816 ]]\n",
      "the iterations of  562400 the loss is 0.24635678030644392 theta= [[-12.34977765]\n",
      " [  0.10404773]\n",
      " [  0.09798894]]\n",
      "the iterations of  562500 the loss is 0.24634838236289355 theta= [[-12.35069404]\n",
      " [  0.104055  ]\n",
      " [  0.09799629]]\n",
      "the iterations of  562600 the loss is 0.24633998704589619 theta= [[-12.35161028]\n",
      " [  0.10406226]\n",
      " [  0.09800363]]\n",
      "the iterations of  562700 the loss is 0.2463315943542363 theta= [[-12.35252638]\n",
      " [  0.10406953]\n",
      " [  0.09801098]]\n",
      "the iterations of  562800 the loss is 0.2463232042866988 theta= [[-12.35344233]\n",
      " [  0.1040768 ]\n",
      " [  0.09801832]]\n",
      "the iterations of  562900 the loss is 0.24631481684206974 theta= [[-12.35435814]\n",
      " [  0.10408406]\n",
      " [  0.09802566]]\n",
      "the iterations of  563000 the loss is 0.2463064320191358 theta= [[-12.35527381]\n",
      " [  0.10409132]\n",
      " [  0.098033  ]]\n",
      "the iterations of  563100 the loss is 0.24629804981668424 theta= [[-12.35618934]\n",
      " [  0.10409858]\n",
      " [  0.09804034]]\n",
      "the iterations of  563200 the loss is 0.24628967023350332 theta= [[-12.35710472]\n",
      " [  0.10410585]\n",
      " [  0.09804768]]\n",
      "the iterations of  563300 the loss is 0.2462812932683818 theta= [[-12.35801996]\n",
      " [  0.10411311]\n",
      " [  0.09805501]]\n",
      "the iterations of  563400 the loss is 0.2462729189201095 theta= [[-12.35893505]\n",
      " [  0.10412036]\n",
      " [  0.09806235]]\n",
      "the iterations of  563500 the loss is 0.24626454718747678 theta= [[-12.35985001]\n",
      " [  0.10412762]\n",
      " [  0.09806968]]\n",
      "the iterations of  563600 the loss is 0.24625617806927486 theta= [[-12.36076482]\n",
      " [  0.10413488]\n",
      " [  0.09807702]]\n",
      "the iterations of  563700 the loss is 0.2462478115642957 theta= [[-12.36167949]\n",
      " [  0.10414213]\n",
      " [  0.09808435]]\n",
      "the iterations of  563800 the loss is 0.24623944767133193 theta= [[-12.36259401]\n",
      " [  0.10414939]\n",
      " [  0.09809168]]\n",
      "the iterations of  563900 the loss is 0.24623108638917693 theta= [[-12.36350839]\n",
      " [  0.10415664]\n",
      " [  0.09809901]]\n",
      "the iterations of  564000 the loss is 0.24622272771662504 theta= [[-12.36442263]\n",
      " [  0.10416389]\n",
      " [  0.09810634]]\n",
      "the iterations of  564100 the loss is 0.24621437165247123 theta= [[-12.36533673]\n",
      " [  0.10417114]\n",
      " [  0.09811366]]\n",
      "the iterations of  564200 the loss is 0.2462060181955109 theta= [[-12.36625068]\n",
      " [  0.10417839]\n",
      " [  0.09812099]]\n",
      "the iterations of  564300 the loss is 0.24619766734454082 theta= [[-12.3671645 ]\n",
      " [  0.10418564]\n",
      " [  0.09812832]]\n",
      "the iterations of  564400 the loss is 0.24618931909835798 theta= [[-12.36807816]\n",
      " [  0.10419289]\n",
      " [  0.09813564]]\n",
      "the iterations of  564500 the loss is 0.2461809734557604 theta= [[-12.36899169]\n",
      " [  0.10420014]\n",
      " [  0.09814296]]\n",
      "the iterations of  564600 the loss is 0.2461726304155468 theta= [[-12.36990507]\n",
      " [  0.10420738]\n",
      " [  0.09815029]]\n",
      "the iterations of  564700 the loss is 0.24616428997651668 theta= [[-12.37081832]\n",
      " [  0.10421463]\n",
      " [  0.09815761]]\n",
      "the iterations of  564800 the loss is 0.2461559521374701 theta= [[-12.37173142]\n",
      " [  0.10422187]\n",
      " [  0.09816493]]\n",
      "the iterations of  564900 the loss is 0.24614761689720815 theta= [[-12.37264437]\n",
      " [  0.10422911]\n",
      " [  0.09817225]]\n",
      "the iterations of  565000 the loss is 0.24613928425453227 theta= [[-12.37355719]\n",
      " [  0.10423635]\n",
      " [  0.09817956]]\n",
      "the iterations of  565100 the loss is 0.2461309542082452 theta= [[-12.37446986]\n",
      " [  0.10424359]\n",
      " [  0.09818688]]\n",
      "the iterations of  565200 the loss is 0.24612262675714988 theta= [[-12.37538239]\n",
      " [  0.10425083]\n",
      " [  0.09819419]]\n",
      "the iterations of  565300 the loss is 0.24611430190005035 theta= [[-12.37629478]\n",
      " [  0.10425807]\n",
      " [  0.09820151]]\n",
      "the iterations of  565400 the loss is 0.24610597963575123 theta= [[-12.37720703]\n",
      " [  0.1042653 ]\n",
      " [  0.09820882]]\n",
      "the iterations of  565500 the loss is 0.24609765996305788 theta= [[-12.37811913]\n",
      " [  0.10427254]\n",
      " [  0.09821613]]\n",
      "the iterations of  565600 the loss is 0.24608934288077663 theta= [[-12.37903109]\n",
      " [  0.10427977]\n",
      " [  0.09822344]]\n",
      "the iterations of  565700 the loss is 0.24608102838771423 theta= [[-12.37994291]\n",
      " [  0.10428701]\n",
      " [  0.09823075]]\n",
      "the iterations of  565800 the loss is 0.24607271648267828 theta= [[-12.38085459]\n",
      " [  0.10429424]\n",
      " [  0.09823806]]\n",
      "the iterations of  565900 the loss is 0.24606440716447728 theta= [[-12.38176613]\n",
      " [  0.10430147]\n",
      " [  0.09824537]]\n",
      "the iterations of  566000 the loss is 0.24605610043192047 theta= [[-12.38267752]\n",
      " [  0.1043087 ]\n",
      " [  0.09825268]]\n",
      "the iterations of  566100 the loss is 0.24604779628381734 theta= [[-12.38358877]\n",
      " [  0.10431593]\n",
      " [  0.09825998]]\n",
      "the iterations of  566200 the loss is 0.24603949471897874 theta= [[-12.38449988]\n",
      " [  0.10432315]\n",
      " [  0.09826728]]\n",
      "the iterations of  566300 the loss is 0.24603119573621599 theta= [[-12.38541085]\n",
      " [  0.10433038]\n",
      " [  0.09827459]]\n",
      "the iterations of  566400 the loss is 0.24602289933434118 theta= [[-12.38632168]\n",
      " [  0.10433761]\n",
      " [  0.09828189]]\n",
      "the iterations of  566500 the loss is 0.24601460551216708 theta= [[-12.38723237]\n",
      " [  0.10434483]\n",
      " [  0.09828919]]\n",
      "the iterations of  566600 the loss is 0.24600631426850728 theta= [[-12.38814291]\n",
      " [  0.10435205]\n",
      " [  0.09829649]]\n",
      "the iterations of  566700 the loss is 0.24599802560217604 theta= [[-12.38905331]\n",
      " [  0.10435927]\n",
      " [  0.09830379]]\n",
      "the iterations of  566800 the loss is 0.24598973951198833 theta= [[-12.38996358]\n",
      " [  0.1043665 ]\n",
      " [  0.09831109]]\n",
      "the iterations of  566900 the loss is 0.24598145599676005 theta= [[-12.3908737 ]\n",
      " [  0.10437371]\n",
      " [  0.09831838]]\n",
      "the iterations of  567000 the loss is 0.2459731750553077 theta= [[-12.39178367]\n",
      " [  0.10438093]\n",
      " [  0.09832568]]\n",
      "the iterations of  567100 the loss is 0.24596489668644825 theta= [[-12.39269351]\n",
      " [  0.10438815]\n",
      " [  0.09833297]]\n",
      "the iterations of  567200 the loss is 0.24595662088899978 theta= [[-12.39360321]\n",
      " [  0.10439537]\n",
      " [  0.09834026]]\n",
      "the iterations of  567300 the loss is 0.24594834766178117 theta= [[-12.39451276]\n",
      " [  0.10440258]\n",
      " [  0.09834756]]\n",
      "the iterations of  567400 the loss is 0.2459400770036117 theta= [[-12.39542218]\n",
      " [  0.1044098 ]\n",
      " [  0.09835485]]\n",
      "the iterations of  567500 the loss is 0.2459318089133116 theta= [[-12.39633145]\n",
      " [  0.10441701]\n",
      " [  0.09836214]]\n",
      "the iterations of  567600 the loss is 0.2459235433897018 theta= [[-12.39724058]\n",
      " [  0.10442422]\n",
      " [  0.09836942]]\n",
      "the iterations of  567700 the loss is 0.2459152804316038 theta= [[-12.39814957]\n",
      " [  0.10443143]\n",
      " [  0.09837671]]\n",
      "the iterations of  567800 the loss is 0.24590702003783999 theta= [[-12.39905842]\n",
      " [  0.10443864]\n",
      " [  0.098384  ]]\n",
      "the iterations of  567900 the loss is 0.2458987622072335 theta= [[-12.39996712]\n",
      " [  0.10444585]\n",
      " [  0.09839128]]\n",
      "the iterations of  568000 the loss is 0.24589050693860812 theta= [[-12.40087569]\n",
      " [  0.10445306]\n",
      " [  0.09839857]]\n",
      "the iterations of  568100 the loss is 0.24588225423078847 theta= [[-12.40178412]\n",
      " [  0.10446026]\n",
      " [  0.09840585]]\n",
      "the iterations of  568200 the loss is 0.24587400408259974 theta= [[-12.4026924 ]\n",
      " [  0.10446747]\n",
      " [  0.09841313]]\n",
      "the iterations of  568300 the loss is 0.24586575649286796 theta= [[-12.40360055]\n",
      " [  0.10447467]\n",
      " [  0.09842041]]\n",
      "the iterations of  568400 the loss is 0.24585751146041987 theta= [[-12.40450855]\n",
      " [  0.10448188]\n",
      " [  0.09842769]]\n",
      "the iterations of  568500 the loss is 0.2458492689840829 theta= [[-12.40541641]\n",
      " [  0.10448908]\n",
      " [  0.09843497]]\n",
      "the iterations of  568600 the loss is 0.24584102906268515 theta= [[-12.40632413]\n",
      " [  0.10449628]\n",
      " [  0.09844225]]\n",
      "the iterations of  568700 the loss is 0.24583279169505567 theta= [[-12.40723172]\n",
      " [  0.10450348]\n",
      " [  0.09844952]]\n",
      "the iterations of  568800 the loss is 0.24582455688002391 theta= [[-12.40813916]\n",
      " [  0.10451068]\n",
      " [  0.0984568 ]]\n",
      "the iterations of  568900 the loss is 0.24581632461642033 theta= [[-12.40904646]\n",
      " [  0.10451788]\n",
      " [  0.09846407]]\n",
      "the iterations of  569000 the loss is 0.245808094903076 theta= [[-12.40995362]\n",
      " [  0.10452507]\n",
      " [  0.09847134]]\n",
      "the iterations of  569100 the loss is 0.24579986773882279 theta= [[-12.41086063]\n",
      " [  0.10453227]\n",
      " [  0.09847862]]\n",
      "the iterations of  569200 the loss is 0.24579164312249321 theta= [[-12.41176751]\n",
      " [  0.10453946]\n",
      " [  0.09848589]]\n",
      "the iterations of  569300 the loss is 0.24578342105292036 theta= [[-12.41267425]\n",
      " [  0.10454665]\n",
      " [  0.09849316]]\n",
      "the iterations of  569400 the loss is 0.24577520152893834 theta= [[-12.41358085]\n",
      " [  0.10455385]\n",
      " [  0.09850042]]\n",
      "the iterations of  569500 the loss is 0.24576698454938176 theta= [[-12.41448731]\n",
      " [  0.10456104]\n",
      " [  0.09850769]]\n",
      "the iterations of  569600 the loss is 0.24575877011308614 theta= [[-12.41539362]\n",
      " [  0.10456823]\n",
      " [  0.09851496]]\n",
      "the iterations of  569700 the loss is 0.24575055821888753 theta= [[-12.4162998 ]\n",
      " [  0.10457542]\n",
      " [  0.09852222]]\n",
      "the iterations of  569800 the loss is 0.24574234886562277 theta= [[-12.41720584]\n",
      " [  0.1045826 ]\n",
      " [  0.09852949]]\n",
      "the iterations of  569900 the loss is 0.24573414205212946 theta= [[-12.41811173]\n",
      " [  0.10458979]\n",
      " [  0.09853675]]\n",
      "the iterations of  570000 the loss is 0.2457259377772458 theta= [[-12.41901749]\n",
      " [  0.10459698]\n",
      " [  0.09854401]]\n",
      "the iterations of  570100 the loss is 0.2457177360398108 theta= [[-12.41992311]\n",
      " [  0.10460416]\n",
      " [  0.09855127]]\n",
      "the iterations of  570200 the loss is 0.24570953683866434 theta= [[-12.42082858]\n",
      " [  0.10461134]\n",
      " [  0.09855853]]\n",
      "the iterations of  570300 the loss is 0.24570134017264678 theta= [[-12.42173392]\n",
      " [  0.10461853]\n",
      " [  0.09856579]]\n",
      "the iterations of  570400 the loss is 0.24569314604059908 theta= [[-12.42263911]\n",
      " [  0.10462571]\n",
      " [  0.09857305]]\n",
      "the iterations of  570500 the loss is 0.24568495444136323 theta= [[-12.42354417]\n",
      " [  0.10463289]\n",
      " [  0.0985803 ]]\n",
      "the iterations of  570600 the loss is 0.2456767653737819 theta= [[-12.42444909]\n",
      " [  0.10464007]\n",
      " [  0.09858756]]\n",
      "the iterations of  570700 the loss is 0.24566857883669815 theta= [[-12.42535386]\n",
      " [  0.10464724]\n",
      " [  0.09859481]]\n",
      "the iterations of  570800 the loss is 0.2456603948289563 theta= [[-12.4262585 ]\n",
      " [  0.10465442]\n",
      " [  0.09860206]]\n",
      "the iterations of  570900 the loss is 0.2456522133494008 theta= [[-12.427163  ]\n",
      " [  0.1046616 ]\n",
      " [  0.09860932]]\n",
      "the iterations of  571000 the loss is 0.24564403439687724 theta= [[-12.42806735]\n",
      " [  0.10466877]\n",
      " [  0.09861657]]\n",
      "the iterations of  571100 the loss is 0.24563585797023177 theta= [[-12.42897157]\n",
      " [  0.10467594]\n",
      " [  0.09862382]]\n",
      "the iterations of  571200 the loss is 0.24562768406831106 theta= [[-12.42987565]\n",
      " [  0.10468312]\n",
      " [  0.09863107]]\n",
      "the iterations of  571300 the loss is 0.24561951268996288 theta= [[-12.43077959]\n",
      " [  0.10469029]\n",
      " [  0.09863831]]\n",
      "the iterations of  571400 the loss is 0.24561134383403543 theta= [[-12.43168339]\n",
      " [  0.10469746]\n",
      " [  0.09864556]]\n",
      "the iterations of  571500 the loss is 0.24560317749937774 theta= [[-12.43258705]\n",
      " [  0.10470463]\n",
      " [  0.0986528 ]]\n",
      "the iterations of  571600 the loss is 0.2455950136848395 theta= [[-12.43349057]\n",
      " [  0.10471179]\n",
      " [  0.09866005]]\n",
      "the iterations of  571700 the loss is 0.24558685238927105 theta= [[-12.43439395]\n",
      " [  0.10471896]\n",
      " [  0.09866729]]\n",
      "the iterations of  571800 the loss is 0.24557869361152349 theta= [[-12.43529719]\n",
      " [  0.10472613]\n",
      " [  0.09867453]]\n",
      "the iterations of  571900 the loss is 0.24557053735044868 theta= [[-12.43620029]\n",
      " [  0.10473329]\n",
      " [  0.09868177]]\n",
      "the iterations of  572000 the loss is 0.24556238360489907 theta= [[-12.43710325]\n",
      " [  0.10474045]\n",
      " [  0.09868901]]\n",
      "the iterations of  572100 the loss is 0.24555423237372792 theta= [[-12.43800608]\n",
      " [  0.10474762]\n",
      " [  0.09869625]]\n",
      "the iterations of  572200 the loss is 0.24554608365578934 theta= [[-12.43890876]\n",
      " [  0.10475478]\n",
      " [  0.09870349]]\n",
      "the iterations of  572300 the loss is 0.24553793744993777 theta= [[-12.43981131]\n",
      " [  0.10476194]\n",
      " [  0.09871073]]\n",
      "the iterations of  572400 the loss is 0.24552979375502867 theta= [[-12.44071371]\n",
      " [  0.1047691 ]\n",
      " [  0.09871796]]\n",
      "the iterations of  572500 the loss is 0.24552165256991806 theta= [[-12.44161598]\n",
      " [  0.10477626]\n",
      " [  0.09872519]]\n",
      "the iterations of  572600 the loss is 0.24551351389346263 theta= [[-12.44251811]\n",
      " [  0.10478341]\n",
      " [  0.09873243]]\n",
      "the iterations of  572700 the loss is 0.24550537772451986 theta= [[-12.4434201 ]\n",
      " [  0.10479057]\n",
      " [  0.09873966]]\n",
      "the iterations of  572800 the loss is 0.24549724406194795 theta= [[-12.44432195]\n",
      " [  0.10479772]\n",
      " [  0.09874689]]\n",
      "the iterations of  572900 the loss is 0.2454891129046057 theta= [[-12.44522366]\n",
      " [  0.10480488]\n",
      " [  0.09875412]]\n",
      "the iterations of  573000 the loss is 0.24548098425135287 theta= [[-12.44612523]\n",
      " [  0.10481203]\n",
      " [  0.09876135]]\n",
      "the iterations of  573100 the loss is 0.24547285810104938 theta= [[-12.44702666]\n",
      " [  0.10481918]\n",
      " [  0.09876858]]\n",
      "the iterations of  573200 the loss is 0.2454647344525566 theta= [[-12.44792796]\n",
      " [  0.10482633]\n",
      " [  0.0987758 ]]\n",
      "the iterations of  573300 the loss is 0.24545661330473587 theta= [[-12.44882911]\n",
      " [  0.10483348]\n",
      " [  0.09878303]]\n",
      "the iterations of  573400 the loss is 0.2454484946564496 theta= [[-12.44973013]\n",
      " [  0.10484063]\n",
      " [  0.09879025]]\n",
      "the iterations of  573500 the loss is 0.24544037850656106 theta= [[-12.45063101]\n",
      " [  0.10484778]\n",
      " [  0.09879748]]\n",
      "the iterations of  573600 the loss is 0.2454322648539338 theta= [[-12.45153175]\n",
      " [  0.10485492]\n",
      " [  0.0988047 ]]\n",
      "the iterations of  573700 the loss is 0.24542415369743234 theta= [[-12.45243235]\n",
      " [  0.10486207]\n",
      " [  0.09881192]]\n",
      "the iterations of  573800 the loss is 0.24541604503592193 theta= [[-12.45333281]\n",
      " [  0.10486921]\n",
      " [  0.09881914]]\n",
      "the iterations of  573900 the loss is 0.24540793886826825 theta= [[-12.45423314]\n",
      " [  0.10487636]\n",
      " [  0.09882636]]\n",
      "the iterations of  574000 the loss is 0.24539983519333805 theta= [[-12.45513332]\n",
      " [  0.1048835 ]\n",
      " [  0.09883357]]\n",
      "the iterations of  574100 the loss is 0.2453917340099984 theta= [[-12.45603337]\n",
      " [  0.10489064]\n",
      " [  0.09884079]]\n",
      "the iterations of  574200 the loss is 0.24538363531711732 theta= [[-12.45693328]\n",
      " [  0.10489778]\n",
      " [  0.09884801]]\n",
      "the iterations of  574300 the loss is 0.2453755391135634 theta= [[-12.45783305]\n",
      " [  0.10490492]\n",
      " [  0.09885522]]\n",
      "the iterations of  574400 the loss is 0.24536744539820593 theta= [[-12.45873268]\n",
      " [  0.10491205]\n",
      " [  0.09886243]]\n",
      "the iterations of  574500 the loss is 0.245359354169915 theta= [[-12.45963218]\n",
      " [  0.10491919]\n",
      " [  0.09886965]]\n",
      "the iterations of  574600 the loss is 0.24535126542756147 theta= [[-12.46053153]\n",
      " [  0.10492632]\n",
      " [  0.09887686]]\n",
      "the iterations of  574700 the loss is 0.24534317917001652 theta= [[-12.46143075]\n",
      " [  0.10493346]\n",
      " [  0.09888407]]\n",
      "the iterations of  574800 the loss is 0.24533509539615228 theta= [[-12.46232983]\n",
      " [  0.10494059]\n",
      " [  0.09889128]]\n",
      "the iterations of  574900 the loss is 0.2453270141048416 theta= [[-12.46322877]\n",
      " [  0.10494772]\n",
      " [  0.09889848]]\n",
      "the iterations of  575000 the loss is 0.24531893529495785 theta= [[-12.46412758]\n",
      " [  0.10495485]\n",
      " [  0.09890569]]\n",
      "the iterations of  575100 the loss is 0.24531085896537533 theta= [[-12.46502624]\n",
      " [  0.10496198]\n",
      " [  0.0989129 ]]\n",
      "the iterations of  575200 the loss is 0.24530278511496892 theta= [[-12.46592477]\n",
      " [  0.10496911]\n",
      " [  0.0989201 ]]\n",
      "the iterations of  575300 the loss is 0.24529471374261413 theta= [[-12.46682316]\n",
      " [  0.10497624]\n",
      " [  0.0989273 ]]\n",
      "the iterations of  575400 the loss is 0.24528664484718699 theta= [[-12.46772141]\n",
      " [  0.10498337]\n",
      " [  0.09893451]]\n",
      "the iterations of  575500 the loss is 0.24527857842756473 theta= [[-12.46861953]\n",
      " [  0.10499049]\n",
      " [  0.09894171]]\n",
      "the iterations of  575600 the loss is 0.24527051448262477 theta= [[-12.4695175 ]\n",
      " [  0.10499762]\n",
      " [  0.09894891]]\n",
      "the iterations of  575700 the loss is 0.24526245301124536 theta= [[-12.47041534]\n",
      " [  0.10500474]\n",
      " [  0.09895611]]\n",
      "the iterations of  575800 the loss is 0.24525439401230575 theta= [[-12.47131304]\n",
      " [  0.10501186]\n",
      " [  0.09896331]]\n",
      "the iterations of  575900 the loss is 0.24524633748468536 theta= [[-12.47221061]\n",
      " [  0.10501898]\n",
      " [  0.0989705 ]]\n",
      "the iterations of  576000 the loss is 0.24523828342726456 theta= [[-12.47310803]\n",
      " [  0.1050261 ]\n",
      " [  0.0989777 ]]\n",
      "the iterations of  576100 the loss is 0.2452302318389245 theta= [[-12.47400532]\n",
      " [  0.10503322]\n",
      " [  0.09898489]]\n",
      "the iterations of  576200 the loss is 0.245222182718547 theta= [[-12.47490247]\n",
      " [  0.10504034]\n",
      " [  0.09899209]]\n",
      "the iterations of  576300 the loss is 0.24521413606501416 theta= [[-12.47579948]\n",
      " [  0.10504746]\n",
      " [  0.09899928]]\n",
      "the iterations of  576400 the loss is 0.2452060918772094 theta= [[-12.47669636]\n",
      " [  0.10505457]\n",
      " [  0.09900647]]\n",
      "the iterations of  576500 the loss is 0.24519805015401638 theta= [[-12.4775931 ]\n",
      " [  0.10506169]\n",
      " [  0.09901366]]\n",
      "the iterations of  576600 the loss is 0.2451900108943194 theta= [[-12.4784897 ]\n",
      " [  0.1050688 ]\n",
      " [  0.09902085]]\n",
      "the iterations of  576700 the loss is 0.2451819740970037 theta= [[-12.47938616]\n",
      " [  0.10507592]\n",
      " [  0.09902804]]\n",
      "the iterations of  576800 the loss is 0.24517393976095522 theta= [[-12.48028249]\n",
      " [  0.10508303]\n",
      " [  0.09903523]]\n",
      "the iterations of  576900 the loss is 0.24516590788506032 theta= [[-12.48117868]\n",
      " [  0.10509014]\n",
      " [  0.09904241]]\n",
      "the iterations of  577000 the loss is 0.24515787846820614 theta= [[-12.48207473]\n",
      " [  0.10509725]\n",
      " [  0.0990496 ]]\n",
      "the iterations of  577100 the loss is 0.24514985150928076 theta= [[-12.48297065]\n",
      " [  0.10510435]\n",
      " [  0.09905678]]\n",
      "the iterations of  577200 the loss is 0.24514182700717252 theta= [[-12.48386642]\n",
      " [  0.10511146]\n",
      " [  0.09906396]]\n",
      "the iterations of  577300 the loss is 0.24513380496077064 theta= [[-12.48476206]\n",
      " [  0.10511857]\n",
      " [  0.09907115]]\n",
      "the iterations of  577400 the loss is 0.24512578536896515 theta= [[-12.48565757]\n",
      " [  0.10512567]\n",
      " [  0.09907833]]\n",
      "the iterations of  577500 the loss is 0.24511776823064646 theta= [[-12.48655293]\n",
      " [  0.10513278]\n",
      " [  0.09908551]]\n",
      "the iterations of  577600 the loss is 0.24510975354470577 theta= [[-12.48744816]\n",
      " [  0.10513988]\n",
      " [  0.09909268]]\n",
      "the iterations of  577700 the loss is 0.24510174131003534 theta= [[-12.48834326]\n",
      " [  0.10514698]\n",
      " [  0.09909986]]\n",
      "the iterations of  577800 the loss is 0.24509373152552733 theta= [[-12.48923821]\n",
      " [  0.10515408]\n",
      " [  0.09910704]]\n",
      "the iterations of  577900 the loss is 0.2450857241900755 theta= [[-12.49013303]\n",
      " [  0.10516118]\n",
      " [  0.09911421]]\n",
      "the iterations of  578000 the loss is 0.24507771930257335 theta= [[-12.49102771]\n",
      " [  0.10516828]\n",
      " [  0.09912139]]\n",
      "the iterations of  578100 the loss is 0.2450697168619158 theta= [[-12.49192226]\n",
      " [  0.10517538]\n",
      " [  0.09912856]]\n",
      "the iterations of  578200 the loss is 0.24506171686699793 theta= [[-12.49281667]\n",
      " [  0.10518247]\n",
      " [  0.09913573]]\n",
      "the iterations of  578300 the loss is 0.24505371931671593 theta= [[-12.49371094]\n",
      " [  0.10518957]\n",
      " [  0.0991429 ]]\n",
      "the iterations of  578400 the loss is 0.24504572420996634 theta= [[-12.49460507]\n",
      " [  0.10519666]\n",
      " [  0.09915007]]\n",
      "the iterations of  578500 the loss is 0.2450377315456465 theta= [[-12.49549907]\n",
      " [  0.10520376]\n",
      " [  0.09915724]]\n",
      "the iterations of  578600 the loss is 0.24502974132265432 theta= [[-12.49639294]\n",
      " [  0.10521085]\n",
      " [  0.09916441]]\n",
      "the iterations of  578700 the loss is 0.24502175353988853 theta= [[-12.49728666]\n",
      " [  0.10521794]\n",
      " [  0.09917158]]\n",
      "the iterations of  578800 the loss is 0.2450137681962485 theta= [[-12.49818025]\n",
      " [  0.10522503]\n",
      " [  0.09917874]]\n",
      "the iterations of  578900 the loss is 0.2450057852906343 theta= [[-12.4990737 ]\n",
      " [  0.10523212]\n",
      " [  0.09918591]]\n",
      "the iterations of  579000 the loss is 0.2449978048219463 theta= [[-12.49996702]\n",
      " [  0.10523921]\n",
      " [  0.09919307]]\n",
      "the iterations of  579100 the loss is 0.24498982678908607 theta= [[-12.5008602 ]\n",
      " [  0.10524629]\n",
      " [  0.09920023]]\n",
      "the iterations of  579200 the loss is 0.24498185119095567 theta= [[-12.50175324]\n",
      " [  0.10525338]\n",
      " [  0.09920739]]\n",
      "the iterations of  579300 the loss is 0.24497387802645756 theta= [[-12.50264615]\n",
      " [  0.10526047]\n",
      " [  0.09921455]]\n",
      "the iterations of  579400 the loss is 0.24496590729449516 theta= [[-12.50353892]\n",
      " [  0.10526755]\n",
      " [  0.09922171]]\n",
      "the iterations of  579500 the loss is 0.24495793899397267 theta= [[-12.50443156]\n",
      " [  0.10527463]\n",
      " [  0.09922887]]\n",
      "the iterations of  579600 the loss is 0.24494997312379455 theta= [[-12.50532406]\n",
      " [  0.10528171]\n",
      " [  0.09923603]]\n",
      "the iterations of  579700 the loss is 0.2449420096828664 theta= [[-12.50621642]\n",
      " [  0.10528879]\n",
      " [  0.09924318]]\n",
      "the iterations of  579800 the loss is 0.2449340486700939 theta= [[-12.50710865]\n",
      " [  0.10529587]\n",
      " [  0.09925034]]\n",
      "the iterations of  579900 the loss is 0.24492609008438382 theta= [[-12.50800074]\n",
      " [  0.10530295]\n",
      " [  0.09925749]]\n",
      "the iterations of  580000 the loss is 0.24491813392464362 theta= [[-12.50889269]\n",
      " [  0.10531003]\n",
      " [  0.09926464]]\n",
      "the iterations of  580100 the loss is 0.24491018018978125 theta= [[-12.50978451]\n",
      " [  0.1053171 ]\n",
      " [  0.0992718 ]]\n",
      "the iterations of  580200 the loss is 0.24490222887870527 theta= [[-12.51067619]\n",
      " [  0.10532418]\n",
      " [  0.09927895]]\n",
      "the iterations of  580300 the loss is 0.24489427999032504 theta= [[-12.51156774]\n",
      " [  0.10533125]\n",
      " [  0.0992861 ]]\n",
      "the iterations of  580400 the loss is 0.24488633352355077 theta= [[-12.51245915]\n",
      " [  0.10533833]\n",
      " [  0.09929324]]\n",
      "the iterations of  580500 the loss is 0.24487838947729298 theta= [[-12.51335043]\n",
      " [  0.1053454 ]\n",
      " [  0.09930039]]\n",
      "the iterations of  580600 the loss is 0.2448704478504629 theta= [[-12.51424157]\n",
      " [  0.10535247]\n",
      " [  0.09930754]]\n",
      "the iterations of  580700 the loss is 0.24486250864197262 theta= [[-12.51513257]\n",
      " [  0.10535954]\n",
      " [  0.09931468]]\n",
      "the iterations of  580800 the loss is 0.24485457185073475 theta= [[-12.51602344]\n",
      " [  0.10536661]\n",
      " [  0.09932183]]\n",
      "the iterations of  580900 the loss is 0.2448466374756624 theta= [[-12.51691417]\n",
      " [  0.10537367]\n",
      " [  0.09932897]]\n",
      "the iterations of  581000 the loss is 0.24483870551566983 theta= [[-12.51780477]\n",
      " [  0.10538074]\n",
      " [  0.09933611]]\n",
      "the iterations of  581100 the loss is 0.24483077596967148 theta= [[-12.51869523]\n",
      " [  0.10538781]\n",
      " [  0.09934325]]\n",
      "the iterations of  581200 the loss is 0.2448228488365827 theta= [[-12.51958556]\n",
      " [  0.10539487]\n",
      " [  0.09935039]]\n",
      "the iterations of  581300 the loss is 0.2448149241153194 theta= [[-12.52047575]\n",
      " [  0.10540193]\n",
      " [  0.09935753]]\n",
      "the iterations of  581400 the loss is 0.24480700180479803 theta= [[-12.52136581]\n",
      " [  0.105409  ]\n",
      " [  0.09936467]]\n",
      "the iterations of  581500 the loss is 0.24479908190393612 theta= [[-12.52225573]\n",
      " [  0.10541606]\n",
      " [  0.09937181]]\n",
      "the iterations of  581600 the loss is 0.24479116441165125 theta= [[-12.52314551]\n",
      " [  0.10542312]\n",
      " [  0.09937894]]\n",
      "the iterations of  581700 the loss is 0.24478324932686224 theta= [[-12.52403516]\n",
      " [  0.10543018]\n",
      " [  0.09938607]]\n",
      "the iterations of  581800 the loss is 0.24477533664848816 theta= [[-12.52492467]\n",
      " [  0.10543724]\n",
      " [  0.09939321]]\n",
      "the iterations of  581900 the loss is 0.244767426375449 theta= [[-12.52581405]\n",
      " [  0.10544429]\n",
      " [  0.09940034]]\n",
      "the iterations of  582000 the loss is 0.24475951850666505 theta= [[-12.5267033 ]\n",
      " [  0.10545135]\n",
      " [  0.09940747]]\n",
      "the iterations of  582100 the loss is 0.24475161304105778 theta= [[-12.52759241]\n",
      " [  0.1054584 ]\n",
      " [  0.0994146 ]]\n",
      "the iterations of  582200 the loss is 0.24474370997754888 theta= [[-12.52848138]\n",
      " [  0.10546546]\n",
      " [  0.09942173]]\n",
      "the iterations of  582300 the loss is 0.2447358093150609 theta= [[-12.52937022]\n",
      " [  0.10547251]\n",
      " [  0.09942886]]\n",
      "the iterations of  582400 the loss is 0.24472791105251684 theta= [[-12.53025892]\n",
      " [  0.10547956]\n",
      " [  0.09943599]]\n",
      "the iterations of  582500 the loss is 0.24472001518884048 theta= [[-12.53114749]\n",
      " [  0.10548661]\n",
      " [  0.09944311]]\n",
      "the iterations of  582600 the loss is 0.24471212172295645 theta= [[-12.53203592]\n",
      " [  0.10549366]\n",
      " [  0.09945024]]\n",
      "the iterations of  582700 the loss is 0.24470423065378966 theta= [[-12.53292422]\n",
      " [  0.10550071]\n",
      " [  0.09945736]]\n",
      "the iterations of  582800 the loss is 0.24469634198026585 theta= [[-12.53381239]\n",
      " [  0.10550776]\n",
      " [  0.09946448]]\n",
      "the iterations of  582900 the loss is 0.24468845570131154 theta= [[-12.53470041]\n",
      " [  0.1055148 ]\n",
      " [  0.0994716 ]]\n",
      "the iterations of  583000 the loss is 0.2446805718158538 theta= [[-12.53558831]\n",
      " [  0.10552185]\n",
      " [  0.09947872]]\n",
      "the iterations of  583100 the loss is 0.24467269032282013 theta= [[-12.53647607]\n",
      " [  0.10552889]\n",
      " [  0.09948584]]\n",
      "the iterations of  583200 the loss is 0.2446648112211391 theta= [[-12.53736369]\n",
      " [  0.10553594]\n",
      " [  0.09949296]]\n",
      "the iterations of  583300 the loss is 0.2446569345097394 theta= [[-12.53825118]\n",
      " [  0.10554298]\n",
      " [  0.09950008]]\n",
      "the iterations of  583400 the loss is 0.2446490601875507 theta= [[-12.53913854]\n",
      " [  0.10555002]\n",
      " [  0.0995072 ]]\n",
      "the iterations of  583500 the loss is 0.24464118825350362 theta= [[-12.54002576]\n",
      " [  0.10555706]\n",
      " [  0.09951431]]\n",
      "the iterations of  583600 the loss is 0.24463331870652877 theta= [[-12.54091285]\n",
      " [  0.1055641 ]\n",
      " [  0.09952143]]\n",
      "the iterations of  583700 the loss is 0.24462545154555776 theta= [[-12.5417998 ]\n",
      " [  0.10557114]\n",
      " [  0.09952854]]\n",
      "the iterations of  583800 the loss is 0.24461758676952303 theta= [[-12.54268662]\n",
      " [  0.10557817]\n",
      " [  0.09953565]]\n",
      "the iterations of  583900 the loss is 0.24460972437735712 theta= [[-12.5435733 ]\n",
      " [  0.10558521]\n",
      " [  0.09954276]]\n",
      "the iterations of  584000 the loss is 0.2446018643679938 theta= [[-12.54445985]\n",
      " [  0.10559224]\n",
      " [  0.09954987]]\n",
      "the iterations of  584100 the loss is 0.24459400674036696 theta= [[-12.54534626]\n",
      " [  0.10559928]\n",
      " [  0.09955698]]\n",
      "the iterations of  584200 the loss is 0.24458615149341167 theta= [[-12.54623254]\n",
      " [  0.10560631]\n",
      " [  0.09956409]]\n",
      "the iterations of  584300 the loss is 0.2445782986260632 theta= [[-12.54711869]\n",
      " [  0.10561334]\n",
      " [  0.09957119]]\n",
      "the iterations of  584400 the loss is 0.2445704481372578 theta= [[-12.5480047 ]\n",
      " [  0.10562037]\n",
      " [  0.0995783 ]]\n",
      "the iterations of  584500 the loss is 0.2445626000259319 theta= [[-12.54889058]\n",
      " [  0.1056274 ]\n",
      " [  0.0995854 ]]\n",
      "the iterations of  584600 the loss is 0.24455475429102308 theta= [[-12.54977632]\n",
      " [  0.10563443]\n",
      " [  0.09959251]]\n",
      "the iterations of  584700 the loss is 0.24454691093146938 theta= [[-12.55066193]\n",
      " [  0.10564146]\n",
      " [  0.09959961]]\n",
      "the iterations of  584800 the loss is 0.24453906994620933 theta= [[-12.55154741]\n",
      " [  0.10564848]\n",
      " [  0.09960671]]\n",
      "the iterations of  584900 the loss is 0.2445312313341822 theta= [[-12.55243275]\n",
      " [  0.10565551]\n",
      " [  0.09961381]]\n",
      "the iterations of  585000 the loss is 0.24452339509432805 theta= [[-12.55331795]\n",
      " [  0.10566253]\n",
      " [  0.09962091]]\n",
      "the iterations of  585100 the loss is 0.24451556122558726 theta= [[-12.55420303]\n",
      " [  0.10566956]\n",
      " [  0.09962801]]\n",
      "the iterations of  585200 the loss is 0.24450772972690107 theta= [[-12.55508797]\n",
      " [  0.10567658]\n",
      " [  0.09963511]]\n",
      "the iterations of  585300 the loss is 0.2444999005972114 theta= [[-12.55597277]\n",
      " [  0.1056836 ]\n",
      " [  0.0996422 ]]\n",
      "the iterations of  585400 the loss is 0.2444920738354608 theta= [[-12.55685744]\n",
      " [  0.10569062]\n",
      " [  0.0996493 ]]\n",
      "the iterations of  585500 the loss is 0.24448424944059216 theta= [[-12.55774198]\n",
      " [  0.10569764]\n",
      " [  0.09965639]]\n",
      "the iterations of  585600 the loss is 0.24447642741154954 theta= [[-12.55862639]\n",
      " [  0.10570466]\n",
      " [  0.09966348]]\n",
      "the iterations of  585700 the loss is 0.2444686077472771 theta= [[-12.55951066]\n",
      " [  0.10571167]\n",
      " [  0.09967058]]\n",
      "the iterations of  585800 the loss is 0.24446079044671984 theta= [[-12.56039479]\n",
      " [  0.10571869]\n",
      " [  0.09967767]]\n",
      "the iterations of  585900 the loss is 0.24445297550882347 theta= [[-12.5612788 ]\n",
      " [  0.10572571]\n",
      " [  0.09968476]]\n",
      "the iterations of  586000 the loss is 0.24444516293253443 theta= [[-12.56216267]\n",
      " [  0.10573272]\n",
      " [  0.09969185]]\n",
      "the iterations of  586100 the loss is 0.24443735271679948 theta= [[-12.5630464 ]\n",
      " [  0.10573973]\n",
      " [  0.09969893]]\n",
      "the iterations of  586200 the loss is 0.24442954486056628 theta= [[-12.56393   ]\n",
      " [  0.10574674]\n",
      " [  0.09970602]]\n",
      "the iterations of  586300 the loss is 0.24442173936278297 theta= [[-12.56481347]\n",
      " [  0.10575375]\n",
      " [  0.09971311]]\n",
      "the iterations of  586400 the loss is 0.24441393622239843 theta= [[-12.56569681]\n",
      " [  0.10576076]\n",
      " [  0.09972019]]\n",
      "the iterations of  586500 the loss is 0.24440613543836204 theta= [[-12.56658001]\n",
      " [  0.10576777]\n",
      " [  0.09972727]]\n",
      "the iterations of  586600 the loss is 0.24439833700962377 theta= [[-12.56746308]\n",
      " [  0.10577478]\n",
      " [  0.09973436]]\n",
      "the iterations of  586700 the loss is 0.24439054093513465 theta= [[-12.56834602]\n",
      " [  0.10578179]\n",
      " [  0.09974144]]\n",
      "the iterations of  586800 the loss is 0.24438274721384584 theta= [[-12.56922882]\n",
      " [  0.10578879]\n",
      " [  0.09974852]]\n",
      "the iterations of  586900 the loss is 0.24437495584470945 theta= [[-12.57011149]\n",
      " [  0.1057958 ]\n",
      " [  0.0997556 ]]\n",
      "the iterations of  587000 the loss is 0.24436716682667803 theta= [[-12.57099402]\n",
      " [  0.1058028 ]\n",
      " [  0.09976267]]\n",
      "the iterations of  587100 the loss is 0.24435938015870473 theta= [[-12.57187643]\n",
      " [  0.1058098 ]\n",
      " [  0.09976975]]\n",
      "the iterations of  587200 the loss is 0.24435159583974358 theta= [[-12.5727587 ]\n",
      " [  0.1058168 ]\n",
      " [  0.09977683]]\n",
      "the iterations of  587300 the loss is 0.244343813868749 theta= [[-12.57364083]\n",
      " [  0.1058238 ]\n",
      " [  0.0997839 ]]\n",
      "the iterations of  587400 the loss is 0.24433603424467612 theta= [[-12.57452284]\n",
      " [  0.1058308 ]\n",
      " [  0.09979098]]\n",
      "the iterations of  587500 the loss is 0.24432825696648078 theta= [[-12.57540471]\n",
      " [  0.1058378 ]\n",
      " [  0.09979805]]\n",
      "the iterations of  587600 the loss is 0.24432048203311926 theta= [[-12.57628645]\n",
      " [  0.1058448 ]\n",
      " [  0.09980512]]\n",
      "the iterations of  587700 the loss is 0.24431270944354858 theta= [[-12.57716805]\n",
      " [  0.10585179]\n",
      " [  0.09981219]]\n",
      "the iterations of  587800 the loss is 0.24430493919672633 theta= [[-12.57804952]\n",
      " [  0.10585879]\n",
      " [  0.09981926]]\n",
      "the iterations of  587900 the loss is 0.2442971712916108 theta= [[-12.57893086]\n",
      " [  0.10586578]\n",
      " [  0.09982633]]\n",
      "the iterations of  588000 the loss is 0.24428940572716107 theta= [[-12.57981207]\n",
      " [  0.10587277]\n",
      " [  0.0998334 ]]\n",
      "the iterations of  588100 the loss is 0.24428164250233642 theta= [[-12.58069314]\n",
      " [  0.10587977]\n",
      " [  0.09984046]]\n",
      "the iterations of  588200 the loss is 0.2442738816160971 theta= [[-12.58157408]\n",
      " [  0.10588676]\n",
      " [  0.09984753]]\n",
      "the iterations of  588300 the loss is 0.2442661230674038 theta= [[-12.58245489]\n",
      " [  0.10589375]\n",
      " [  0.09985459]]\n",
      "the iterations of  588400 the loss is 0.24425836685521796 theta= [[-12.58333557]\n",
      " [  0.10590074]\n",
      " [  0.09986166]]\n",
      "the iterations of  588500 the loss is 0.2442506129785016 theta= [[-12.58421611]\n",
      " [  0.10590772]\n",
      " [  0.09986872]]\n",
      "the iterations of  588600 the loss is 0.24424286143621732 theta= [[-12.58509652]\n",
      " [  0.10591471]\n",
      " [  0.09987578]]\n",
      "the iterations of  588700 the loss is 0.2442351122273283 theta= [[-12.5859768 ]\n",
      " [  0.1059217 ]\n",
      " [  0.09988284]]\n",
      "the iterations of  588800 the loss is 0.24422736535079856 theta= [[-12.58685694]\n",
      " [  0.10592868]\n",
      " [  0.0998899 ]]\n",
      "the iterations of  588900 the loss is 0.24421962080559256 theta= [[-12.58773695]\n",
      " [  0.10593566]\n",
      " [  0.09989696]]\n",
      "the iterations of  589000 the loss is 0.2442118785906753 theta= [[-12.58861683]\n",
      " [  0.10594265]\n",
      " [  0.09990402]]\n",
      "the iterations of  589100 the loss is 0.24420413870501267 theta= [[-12.58949658]\n",
      " [  0.10594963]\n",
      " [  0.09991107]]\n",
      "the iterations of  589200 the loss is 0.24419640114757096 theta= [[-12.5903762 ]\n",
      " [  0.10595661]\n",
      " [  0.09991813]]\n",
      "the iterations of  589300 the loss is 0.244188665917317 theta= [[-12.59125568]\n",
      " [  0.10596359]\n",
      " [  0.09992518]]\n",
      "the iterations of  589400 the loss is 0.24418093301321858 theta= [[-12.59213503]\n",
      " [  0.10597057]\n",
      " [  0.09993223]]\n",
      "the iterations of  589500 the loss is 0.24417320243424392 theta= [[-12.59301425]\n",
      " [  0.10597754]\n",
      " [  0.09993929]]\n",
      "the iterations of  589600 the loss is 0.24416547417936163 theta= [[-12.59389334]\n",
      " [  0.10598452]\n",
      " [  0.09994634]]\n",
      "the iterations of  589700 the loss is 0.24415774824754133 theta= [[-12.59477229]\n",
      " [  0.10599149]\n",
      " [  0.09995339]]\n",
      "the iterations of  589800 the loss is 0.244150024637753 theta= [[-12.59565112]\n",
      " [  0.10599847]\n",
      " [  0.09996044]]\n",
      "the iterations of  589900 the loss is 0.24414230334896736 theta= [[-12.59652981]\n",
      " [  0.10600544]\n",
      " [  0.09996748]]\n",
      "the iterations of  590000 the loss is 0.24413458438015567 theta= [[-12.59740836]\n",
      " [  0.10601241]\n",
      " [  0.09997453]]\n",
      "the iterations of  590100 the loss is 0.24412686773029 theta= [[-12.59828679]\n",
      " [  0.10601938]\n",
      " [  0.09998158]]\n",
      "the iterations of  590200 the loss is 0.24411915339834267 theta= [[-12.59916509]\n",
      " [  0.10602635]\n",
      " [  0.09998862]]\n",
      "the iterations of  590300 the loss is 0.24411144138328697 theta= [[-12.60004325]\n",
      " [  0.10603332]\n",
      " [  0.09999566]]\n",
      "the iterations of  590400 the loss is 0.24410373168409638 theta= [[-12.60092128]\n",
      " [  0.10604029]\n",
      " [  0.10000271]]\n",
      "the iterations of  590500 the loss is 0.24409602429974556 theta= [[-12.60179918]\n",
      " [  0.10604726]\n",
      " [  0.10000975]]\n",
      "the iterations of  590600 the loss is 0.2440883192292093 theta= [[-12.60267695]\n",
      " [  0.10605422]\n",
      " [  0.10001679]]\n",
      "the iterations of  590700 the loss is 0.24408061647146337 theta= [[-12.60355458]\n",
      " [  0.10606119]\n",
      " [  0.10002383]]\n",
      "the iterations of  590800 the loss is 0.24407291602548387 theta= [[-12.60443208]\n",
      " [  0.10606815]\n",
      " [  0.10003087]]\n",
      "the iterations of  590900 the loss is 0.2440652178902477 theta= [[-12.60530946]\n",
      " [  0.10607512]\n",
      " [  0.1000379 ]]\n",
      "the iterations of  591000 the loss is 0.24405752206473205 theta= [[-12.6061867 ]\n",
      " [  0.10608208]\n",
      " [  0.10004494]]\n",
      "the iterations of  591100 the loss is 0.24404982854791524 theta= [[-12.60706381]\n",
      " [  0.10608904]\n",
      " [  0.10005197]]\n",
      "the iterations of  591200 the loss is 0.24404213733877583 theta= [[-12.60794078]\n",
      " [  0.106096  ]\n",
      " [  0.10005901]]\n",
      "the iterations of  591300 the loss is 0.244034448436293 theta= [[-12.60881763]\n",
      " [  0.10610296]\n",
      " [  0.10006604]]\n",
      "the iterations of  591400 the loss is 0.24402676183944658 theta= [[-12.60969434]\n",
      " [  0.10610991]\n",
      " [  0.10007307]]\n",
      "the iterations of  591500 the loss is 0.24401907754721713 theta= [[-12.61057093]\n",
      " [  0.10611687]\n",
      " [  0.10008011]]\n",
      "the iterations of  591600 the loss is 0.2440113955585858 theta= [[-12.61144738]\n",
      " [  0.10612383]\n",
      " [  0.10008714]]\n",
      "the iterations of  591700 the loss is 0.2440037158725341 theta= [[-12.6123237 ]\n",
      " [  0.10613078]\n",
      " [  0.10009416]]\n",
      "the iterations of  591800 the loss is 0.2439960384880445 theta= [[-12.61319989]\n",
      " [  0.10613773]\n",
      " [  0.10010119]]\n",
      "the iterations of  591900 the loss is 0.2439883634040998 theta= [[-12.61407594]\n",
      " [  0.10614469]\n",
      " [  0.10010822]]\n",
      "the iterations of  592000 the loss is 0.24398069061968347 theta= [[-12.61495187]\n",
      " [  0.10615164]\n",
      " [  0.10011524]]\n",
      "the iterations of  592100 the loss is 0.24397302013377978 theta= [[-12.61582766]\n",
      " [  0.10615859]\n",
      " [  0.10012227]]\n",
      "the iterations of  592200 the loss is 0.24396535194537333 theta= [[-12.61670333]\n",
      " [  0.10616554]\n",
      " [  0.10012929]]\n",
      "the iterations of  592300 the loss is 0.24395768605344959 theta= [[-12.61757886]\n",
      " [  0.10617249]\n",
      " [  0.10013632]]\n",
      "the iterations of  592400 the loss is 0.24395002245699438 theta= [[-12.61845426]\n",
      " [  0.10617943]\n",
      " [  0.10014334]]\n",
      "the iterations of  592500 the loss is 0.24394236115499432 theta= [[-12.61932953]\n",
      " [  0.10618638]\n",
      " [  0.10015036]]\n",
      "the iterations of  592600 the loss is 0.24393470214643626 theta= [[-12.62020467]\n",
      " [  0.10619332]\n",
      " [  0.10015738]]\n",
      "the iterations of  592700 the loss is 0.24392704543030838 theta= [[-12.62107968]\n",
      " [  0.10620027]\n",
      " [  0.1001644 ]]\n",
      "the iterations of  592800 the loss is 0.2439193910055987 theta= [[-12.62195456]\n",
      " [  0.10620721]\n",
      " [  0.10017141]]\n",
      "the iterations of  592900 the loss is 0.24391173887129647 theta= [[-12.62282931]\n",
      " [  0.10621415]\n",
      " [  0.10017843]]\n",
      "the iterations of  593000 the loss is 0.24390408902639094 theta= [[-12.62370392]\n",
      " [  0.10622109]\n",
      " [  0.10018545]]\n",
      "the iterations of  593100 the loss is 0.24389644146987222 theta= [[-12.62457841]\n",
      " [  0.10622804]\n",
      " [  0.10019246]]\n",
      "the iterations of  593200 the loss is 0.24388879620073148 theta= [[-12.62545276]\n",
      " [  0.10623497]\n",
      " [  0.10019947]]\n",
      "the iterations of  593300 the loss is 0.24388115321795972 theta= [[-12.62632698]\n",
      " [  0.10624191]\n",
      " [  0.10020649]]\n",
      "the iterations of  593400 the loss is 0.24387351252054912 theta= [[-12.62720108]\n",
      " [  0.10624885]\n",
      " [  0.1002135 ]]\n",
      "the iterations of  593500 the loss is 0.2438658741074921 theta= [[-12.62807504]\n",
      " [  0.10625579]\n",
      " [  0.10022051]]\n",
      "the iterations of  593600 the loss is 0.24385823797778172 theta= [[-12.62894887]\n",
      " [  0.10626272]\n",
      " [  0.10022752]]\n",
      "the iterations of  593700 the loss is 0.24385060413041193 theta= [[-12.62982257]\n",
      " [  0.10626965]\n",
      " [  0.10023453]]\n",
      "the iterations of  593800 the loss is 0.24384297256437698 theta= [[-12.63069614]\n",
      " [  0.10627659]\n",
      " [  0.10024153]]\n",
      "the iterations of  593900 the loss is 0.24383534327867196 theta= [[-12.63156958]\n",
      " [  0.10628352]\n",
      " [  0.10024854]]\n",
      "the iterations of  594000 the loss is 0.24382771627229224 theta= [[-12.63244289]\n",
      " [  0.10629045]\n",
      " [  0.10025554]]\n",
      "the iterations of  594100 the loss is 0.243820091544234 theta= [[-12.63331607]\n",
      " [  0.10629738]\n",
      " [  0.10026255]]\n",
      "the iterations of  594200 the loss is 0.24381246909349394 theta= [[-12.63418912]\n",
      " [  0.10630431]\n",
      " [  0.10026955]]\n",
      "the iterations of  594300 the loss is 0.24380484891906956 theta= [[-12.63506204]\n",
      " [  0.10631124]\n",
      " [  0.10027655]]\n",
      "the iterations of  594400 the loss is 0.24379723101995868 theta= [[-12.63593482]\n",
      " [  0.10631816]\n",
      " [  0.10028355]]\n",
      "the iterations of  594500 the loss is 0.24378961539515978 theta= [[-12.63680748]\n",
      " [  0.10632509]\n",
      " [  0.10029055]]\n",
      "the iterations of  594600 the loss is 0.24378200204367217 theta= [[-12.63768001]\n",
      " [  0.10633201]\n",
      " [  0.10029755]]\n",
      "the iterations of  594700 the loss is 0.24377439096449544 theta= [[-12.6385524 ]\n",
      " [  0.10633894]\n",
      " [  0.10030455]]\n",
      "the iterations of  594800 the loss is 0.2437667821566299 theta= [[-12.63942467]\n",
      " [  0.10634586]\n",
      " [  0.10031155]]\n",
      "the iterations of  594900 the loss is 0.24375917561907628 theta= [[-12.64029681]\n",
      " [  0.10635278]\n",
      " [  0.10031854]]\n",
      "the iterations of  595000 the loss is 0.24375157135083644 theta= [[-12.64116881]\n",
      " [  0.1063597 ]\n",
      " [  0.10032554]]\n",
      "the iterations of  595100 the loss is 0.24374396935091217 theta= [[-12.64204069]\n",
      " [  0.10636662]\n",
      " [  0.10033253]]\n",
      "the iterations of  595200 the loss is 0.24373636961830616 theta= [[-12.64291244]\n",
      " [  0.10637354]\n",
      " [  0.10033953]]\n",
      "the iterations of  595300 the loss is 0.243728772152022 theta= [[-12.64378405]\n",
      " [  0.10638046]\n",
      " [  0.10034652]]\n",
      "the iterations of  595400 the loss is 0.24372117695106346 theta= [[-12.64465554]\n",
      " [  0.10638737]\n",
      " [  0.10035351]]\n",
      "the iterations of  595500 the loss is 0.24371358401443477 theta= [[-12.6455269 ]\n",
      " [  0.10639429]\n",
      " [  0.1003605 ]]\n",
      "the iterations of  595600 the loss is 0.2437059933411411 theta= [[-12.64639812]\n",
      " [  0.1064012 ]\n",
      " [  0.10036749]]\n",
      "the iterations of  595700 the loss is 0.24369840493018827 theta= [[-12.64726922]\n",
      " [  0.10640812]\n",
      " [  0.10037447]]\n",
      "the iterations of  595800 the loss is 0.24369081878058235 theta= [[-12.64814019]\n",
      " [  0.10641503]\n",
      " [  0.10038146]]\n",
      "the iterations of  595900 the loss is 0.24368323489133 theta= [[-12.64901102]\n",
      " [  0.10642194]\n",
      " [  0.10038845]]\n",
      "the iterations of  596000 the loss is 0.24367565326143878 theta= [[-12.64988173]\n",
      " [  0.10642885]\n",
      " [  0.10039543]]\n",
      "the iterations of  596100 the loss is 0.24366807388991676 theta= [[-12.65075231]\n",
      " [  0.10643576]\n",
      " [  0.10040241]]\n",
      "the iterations of  596200 the loss is 0.2436604967757723 theta= [[-12.65162276]\n",
      " [  0.10644267]\n",
      " [  0.1004094 ]]\n",
      "the iterations of  596300 the loss is 0.24365292191801463 theta= [[-12.65249308]\n",
      " [  0.10644958]\n",
      " [  0.10041638]]\n",
      "the iterations of  596400 the loss is 0.24364534931565376 theta= [[-12.65336326]\n",
      " [  0.10645648]\n",
      " [  0.10042336]]\n",
      "the iterations of  596500 the loss is 0.24363777896769964 theta= [[-12.65423332]\n",
      " [  0.10646339]\n",
      " [  0.10043034]]\n",
      "the iterations of  596600 the loss is 0.24363021087316356 theta= [[-12.65510325]\n",
      " [  0.10647029]\n",
      " [  0.10043732]]\n",
      "the iterations of  596700 the loss is 0.24362264503105677 theta= [[-12.65597305]\n",
      " [  0.1064772 ]\n",
      " [  0.10044429]]\n",
      "the iterations of  596800 the loss is 0.24361508144039146 theta= [[-12.65684272]\n",
      " [  0.1064841 ]\n",
      " [  0.10045127]]\n",
      "the iterations of  596900 the loss is 0.24360752010018027 theta= [[-12.65771227]\n",
      " [  0.106491  ]\n",
      " [  0.10045825]]\n",
      "the iterations of  597000 the loss is 0.24359996100943668 theta= [[-12.65858168]\n",
      " [  0.1064979 ]\n",
      " [  0.10046522]]\n",
      "the iterations of  597100 the loss is 0.24359240416717426 theta= [[-12.65945096]\n",
      " [  0.1065048 ]\n",
      " [  0.10047219]]\n",
      "the iterations of  597200 the loss is 0.24358484957240759 theta= [[-12.66032011]\n",
      " [  0.1065117 ]\n",
      " [  0.10047917]]\n",
      "the iterations of  597300 the loss is 0.24357729722415158 theta= [[-12.66118914]\n",
      " [  0.1065186 ]\n",
      " [  0.10048614]]\n",
      "the iterations of  597400 the loss is 0.24356974712142204 theta= [[-12.66205803]\n",
      " [  0.10652549]\n",
      " [  0.10049311]]\n",
      "the iterations of  597500 the loss is 0.24356219926323505 theta= [[-12.6629268 ]\n",
      " [  0.10653239]\n",
      " [  0.10050008]]\n",
      "the iterations of  597600 the loss is 0.2435546536486072 theta= [[-12.66379544]\n",
      " [  0.10653928]\n",
      " [  0.10050704]]\n",
      "the iterations of  597700 the loss is 0.243547110276556 theta= [[-12.66466395]\n",
      " [  0.10654617]\n",
      " [  0.10051401]]\n",
      "the iterations of  597800 the loss is 0.24353956914609945 theta= [[-12.66553232]\n",
      " [  0.10655307]\n",
      " [  0.10052098]]\n",
      "the iterations of  597900 the loss is 0.2435320302562559 theta= [[-12.66640057]\n",
      " [  0.10655996]\n",
      " [  0.10052794]]\n",
      "the iterations of  598000 the loss is 0.24352449360604436 theta= [[-12.66726869]\n",
      " [  0.10656685]\n",
      " [  0.10053491]]\n",
      "the iterations of  598100 the loss is 0.2435169591944846 theta= [[-12.66813669]\n",
      " [  0.10657374]\n",
      " [  0.10054187]]\n",
      "the iterations of  598200 the loss is 0.243509427020597 theta= [[-12.66900455]\n",
      " [  0.10658062]\n",
      " [  0.10054883]]\n",
      "the iterations of  598300 the loss is 0.24350189708340225 theta= [[-12.66987228]\n",
      " [  0.10658751]\n",
      " [  0.10055579]]\n",
      "the iterations of  598400 the loss is 0.24349436938192173 theta= [[-12.67073989]\n",
      " [  0.1065944 ]\n",
      " [  0.10056275]]\n",
      "the iterations of  598500 the loss is 0.24348684391517758 theta= [[-12.67160736]\n",
      " [  0.10660128]\n",
      " [  0.10056971]]\n",
      "the iterations of  598600 the loss is 0.24347932068219208 theta= [[-12.67247471]\n",
      " [  0.10660817]\n",
      " [  0.10057667]]\n",
      "the iterations of  598700 the loss is 0.2434717996819887 theta= [[-12.67334193]\n",
      " [  0.10661505]\n",
      " [  0.10058363]]\n",
      "the iterations of  598800 the loss is 0.24346428091359085 theta= [[-12.67420902]\n",
      " [  0.10662193]\n",
      " [  0.10059058]]\n",
      "the iterations of  598900 the loss is 0.24345676437602293 theta= [[-12.67507598]\n",
      " [  0.10662881]\n",
      " [  0.10059754]]\n",
      "the iterations of  599000 the loss is 0.24344925006830972 theta= [[-12.67594282]\n",
      " [  0.10663569]\n",
      " [  0.10060449]]\n",
      "the iterations of  599100 the loss is 0.24344173798947683 theta= [[-12.67680952]\n",
      " [  0.10664257]\n",
      " [  0.10061145]]\n",
      "the iterations of  599200 the loss is 0.24343422813855017 theta= [[-12.6776761 ]\n",
      " [  0.10664945]\n",
      " [  0.1006184 ]]\n",
      "the iterations of  599300 the loss is 0.2434267205145563 theta= [[-12.67854254]\n",
      " [  0.10665633]\n",
      " [  0.10062535]]\n",
      "the iterations of  599400 the loss is 0.24341921511652237 theta= [[-12.67940886]\n",
      " [  0.1066632 ]\n",
      " [  0.1006323 ]]\n",
      "the iterations of  599500 the loss is 0.24341171194347616 theta= [[-12.68027505]\n",
      " [  0.10667008]\n",
      " [  0.10063925]]\n",
      "the iterations of  599600 the loss is 0.24340421099444595 theta= [[-12.68114112]\n",
      " [  0.10667695]\n",
      " [  0.10064619]]\n",
      "the iterations of  599700 the loss is 0.24339671226846069 theta= [[-12.68200705]\n",
      " [  0.10668383]\n",
      " [  0.10065314]]\n",
      "the iterations of  599800 the loss is 0.24338921576454975 theta= [[-12.68287285]\n",
      " [  0.1066907 ]\n",
      " [  0.10066009]]\n",
      "the iterations of  599900 the loss is 0.2433817214817432 theta= [[-12.68373853]\n",
      " [  0.10669757]\n",
      " [  0.10066703]]\n",
      "the iterations of  600000 the loss is 0.24337422941907158 theta= [[-12.68460408]\n",
      " [  0.10670444]\n",
      " [  0.10067398]]\n",
      "the iterations of  600100 the loss is 0.243366739575566 theta= [[-12.6854695 ]\n",
      " [  0.10671131]\n",
      " [  0.10068092]]\n",
      "the iterations of  600200 the loss is 0.24335925195025834 theta= [[-12.68633479]\n",
      " [  0.10671817]\n",
      " [  0.10068786]]\n",
      "the iterations of  600300 the loss is 0.24335176654218077 theta= [[-12.68719996]\n",
      " [  0.10672504]\n",
      " [  0.1006948 ]]\n",
      "the iterations of  600400 the loss is 0.2433442833503663 theta= [[-12.68806499]\n",
      " [  0.10673191]\n",
      " [  0.10070174]]\n",
      "the iterations of  600500 the loss is 0.24333680237384836 theta= [[-12.6889299 ]\n",
      " [  0.10673877]\n",
      " [  0.10070868]]\n",
      "the iterations of  600600 the loss is 0.2433293236116608 theta= [[-12.68979468]\n",
      " [  0.10674564]\n",
      " [  0.10071562]]\n",
      "the iterations of  600700 the loss is 0.2433218470628383 theta= [[-12.69065934]\n",
      " [  0.1067525 ]\n",
      " [  0.10072255]]\n",
      "the iterations of  600800 the loss is 0.2433143727264161 theta= [[-12.69152386]\n",
      " [  0.10675936]\n",
      " [  0.10072949]]\n",
      "the iterations of  600900 the loss is 0.24330690060142968 theta= [[-12.69238826]\n",
      " [  0.10676622]\n",
      " [  0.10073642]]\n",
      "the iterations of  601000 the loss is 0.24329943068691548 theta= [[-12.69325253]\n",
      " [  0.10677308]\n",
      " [  0.10074336]]\n",
      "the iterations of  601100 the loss is 0.24329196298191028 theta= [[-12.69411667]\n",
      " [  0.10677994]\n",
      " [  0.10075029]]\n",
      "the iterations of  601200 the loss is 0.24328449748545167 theta= [[-12.69498068]\n",
      " [  0.1067868 ]\n",
      " [  0.10075722]]\n",
      "the iterations of  601300 the loss is 0.2432770341965773 theta= [[-12.69584456]\n",
      " [  0.10679366]\n",
      " [  0.10076415]]\n",
      "the iterations of  601400 the loss is 0.24326957311432615 theta= [[-12.69670832]\n",
      " [  0.10680051]\n",
      " [  0.10077108]]\n",
      "the iterations of  601500 the loss is 0.2432621142377371 theta= [[-12.69757195]\n",
      " [  0.10680737]\n",
      " [  0.10077801]]\n",
      "the iterations of  601600 the loss is 0.24325465756584974 theta= [[-12.69843545]\n",
      " [  0.10681422]\n",
      " [  0.10078494]]\n",
      "the iterations of  601700 the loss is 0.24324720309770442 theta= [[-12.69929883]\n",
      " [  0.10682107]\n",
      " [  0.10079186]]\n",
      "the iterations of  601800 the loss is 0.24323975083234198 theta= [[-12.70016208]\n",
      " [  0.10682792]\n",
      " [  0.10079879]]\n",
      "the iterations of  601900 the loss is 0.24323230076880356 theta= [[-12.70102519]\n",
      " [  0.10683478]\n",
      " [  0.10080571]]\n",
      "the iterations of  602000 the loss is 0.2432248529061315 theta= [[-12.70188819]\n",
      " [  0.10684163]\n",
      " [  0.10081264]]\n",
      "the iterations of  602100 the loss is 0.24321740724336788 theta= [[-12.70275105]\n",
      " [  0.10684847]\n",
      " [  0.10081956]]\n",
      "the iterations of  602200 the loss is 0.24320996377955612 theta= [[-12.70361379]\n",
      " [  0.10685532]\n",
      " [  0.10082648]]\n",
      "the iterations of  602300 the loss is 0.24320252251373956 theta= [[-12.7044764 ]\n",
      " [  0.10686217]\n",
      " [  0.1008334 ]]\n",
      "the iterations of  602400 the loss is 0.24319508344496238 theta= [[-12.70533888]\n",
      " [  0.10686901]\n",
      " [  0.10084032]]\n",
      "the iterations of  602500 the loss is 0.24318764657226946 theta= [[-12.70620123]\n",
      " [  0.10687586]\n",
      " [  0.10084724]]\n",
      "the iterations of  602600 the loss is 0.24318021189470618 theta= [[-12.70706346]\n",
      " [  0.1068827 ]\n",
      " [  0.10085416]]\n",
      "the iterations of  602700 the loss is 0.24317277941131807 theta= [[-12.70792556]\n",
      " [  0.10688955]\n",
      " [  0.10086107]]\n",
      "the iterations of  602800 the loss is 0.24316534912115173 theta= [[-12.70878754]\n",
      " [  0.10689639]\n",
      " [  0.10086799]]\n",
      "the iterations of  602900 the loss is 0.24315792102325418 theta= [[-12.70964938]\n",
      " [  0.10690323]\n",
      " [  0.1008749 ]]\n",
      "the iterations of  603000 the loss is 0.2431504951166729 theta= [[-12.7105111 ]\n",
      " [  0.10691007]\n",
      " [  0.10088182]]\n",
      "the iterations of  603100 the loss is 0.24314307140045585 theta= [[-12.71137269]\n",
      " [  0.10691691]\n",
      " [  0.10088873]]\n",
      "the iterations of  603200 the loss is 0.2431356498736521 theta= [[-12.71223416]\n",
      " [  0.10692375]\n",
      " [  0.10089564]]\n",
      "the iterations of  603300 the loss is 0.2431282305353105 theta= [[-12.7130955 ]\n",
      " [  0.10693058]\n",
      " [  0.10090255]]\n",
      "the iterations of  603400 the loss is 0.243120813384481 theta= [[-12.71395671]\n",
      " [  0.10693742]\n",
      " [  0.10090946]]\n",
      "the iterations of  603500 the loss is 0.2431133984202138 theta= [[-12.71481779]\n",
      " [  0.10694425]\n",
      " [  0.10091637]]\n",
      "the iterations of  603600 the loss is 0.24310598564155975 theta= [[-12.71567875]\n",
      " [  0.10695109]\n",
      " [  0.10092328]]\n",
      "the iterations of  603700 the loss is 0.24309857504757043 theta= [[-12.71653958]\n",
      " [  0.10695792]\n",
      " [  0.10093018]]\n",
      "the iterations of  603800 the loss is 0.24309116663729788 theta= [[-12.71740028]\n",
      " [  0.10696475]\n",
      " [  0.10093709]]\n",
      "the iterations of  603900 the loss is 0.24308376040979435 theta= [[-12.71826086]\n",
      " [  0.10697158]\n",
      " [  0.10094399]]\n",
      "the iterations of  604000 the loss is 0.2430763563641131 theta= [[-12.71912131]\n",
      " [  0.10697841]\n",
      " [  0.1009509 ]]\n",
      "the iterations of  604100 the loss is 0.24306895449930785 theta= [[-12.71998163]\n",
      " [  0.10698524]\n",
      " [  0.1009578 ]]\n",
      "the iterations of  604200 the loss is 0.2430615548144326 theta= [[-12.72084183]\n",
      " [  0.10699207]\n",
      " [  0.1009647 ]]\n",
      "the iterations of  604300 the loss is 0.2430541573085425 theta= [[-12.72170189]\n",
      " [  0.1069989 ]\n",
      " [  0.1009716 ]]\n",
      "the iterations of  604400 the loss is 0.2430467619806925 theta= [[-12.72256184]\n",
      " [  0.10700572]\n",
      " [  0.1009785 ]]\n",
      "the iterations of  604500 the loss is 0.2430393688299386 theta= [[-12.72342165]\n",
      " [  0.10701255]\n",
      " [  0.1009854 ]]\n",
      "the iterations of  604600 the loss is 0.2430319778553371 theta= [[-12.72428134]\n",
      " [  0.10701937]\n",
      " [  0.1009923 ]]\n",
      "the iterations of  604700 the loss is 0.24302458905594532 theta= [[-12.72514091]\n",
      " [  0.10702619]\n",
      " [  0.10099919]]\n",
      "the iterations of  604800 the loss is 0.24301720243082045 theta= [[-12.72600034]\n",
      " [  0.10703302]\n",
      " [  0.10100609]]\n",
      "the iterations of  604900 the loss is 0.24300981797902055 theta= [[-12.72685965]\n",
      " [  0.10703984]\n",
      " [  0.10101298]]\n",
      "the iterations of  605000 the loss is 0.24300243569960447 theta= [[-12.72771884]\n",
      " [  0.10704666]\n",
      " [  0.10101988]]\n",
      "the iterations of  605100 the loss is 0.2429950555916313 theta= [[-12.7285779 ]\n",
      " [  0.10705348]\n",
      " [  0.10102677]]\n",
      "the iterations of  605200 the loss is 0.24298767765416063 theta= [[-12.72943683]\n",
      " [  0.10706029]\n",
      " [  0.10103366]]\n",
      "the iterations of  605300 the loss is 0.24298030188625291 theta= [[-12.73029563]\n",
      " [  0.10706711]\n",
      " [  0.10104055]]\n",
      "the iterations of  605400 the loss is 0.2429729282869689 theta= [[-12.73115431]\n",
      " [  0.10707393]\n",
      " [  0.10104744]]\n",
      "the iterations of  605500 the loss is 0.2429655568553699 theta= [[-12.73201286]\n",
      " [  0.10708074]\n",
      " [  0.10105433]]\n",
      "the iterations of  605600 the loss is 0.2429581875905179 theta= [[-12.73287129]\n",
      " [  0.10708756]\n",
      " [  0.10106122]]\n",
      "the iterations of  605700 the loss is 0.24295082049147546 theta= [[-12.73372959]\n",
      " [  0.10709437]\n",
      " [  0.1010681 ]]\n",
      "the iterations of  605800 the loss is 0.2429434555573054 theta= [[-12.73458776]\n",
      " [  0.10710118]\n",
      " [  0.10107499]]\n",
      "the iterations of  605900 the loss is 0.2429360927870713 theta= [[-12.73544581]\n",
      " [  0.10710799]\n",
      " [  0.10108187]]\n",
      "the iterations of  606000 the loss is 0.24292873217983743 theta= [[-12.73630373]\n",
      " [  0.1071148 ]\n",
      " [  0.10108875]]\n",
      "the iterations of  606100 the loss is 0.24292137373466832 theta= [[-12.73716153]\n",
      " [  0.10712161]\n",
      " [  0.10109564]]\n",
      "the iterations of  606200 the loss is 0.24291401745062915 theta= [[-12.7380192 ]\n",
      " [  0.10712842]\n",
      " [  0.10110252]]\n",
      "the iterations of  606300 the loss is 0.2429066633267858 theta= [[-12.73887674]\n",
      " [  0.10713523]\n",
      " [  0.1011094 ]]\n",
      "the iterations of  606400 the loss is 0.24289931136220447 theta= [[-12.73973416]\n",
      " [  0.10714203]\n",
      " [  0.10111628]]\n",
      "the iterations of  606500 the loss is 0.24289196155595183 theta= [[-12.74059145]\n",
      " [  0.10714884]\n",
      " [  0.10112316]]\n",
      "the iterations of  606600 the loss is 0.24288461390709556 theta= [[-12.74144862]\n",
      " [  0.10715564]\n",
      " [  0.10113003]]\n",
      "the iterations of  606700 the loss is 0.2428772684147034 theta= [[-12.74230566]\n",
      " [  0.10716244]\n",
      " [  0.10113691]]\n",
      "the iterations of  606800 the loss is 0.24286992507784383 theta= [[-12.74316257]\n",
      " [  0.10716925]\n",
      " [  0.10114379]]\n",
      "the iterations of  606900 the loss is 0.24286258389558604 theta= [[-12.74401936]\n",
      " [  0.10717605]\n",
      " [  0.10115066]]\n",
      "the iterations of  607000 the loss is 0.2428552448669993 theta= [[-12.74487603]\n",
      " [  0.10718285]\n",
      " [  0.10115753]]\n",
      "the iterations of  607100 the loss is 0.24284790799115405 theta= [[-12.74573256]\n",
      " [  0.10718965]\n",
      " [  0.10116441]]\n",
      "the iterations of  607200 the loss is 0.24284057326712044 theta= [[-12.74658897]\n",
      " [  0.10719645]\n",
      " [  0.10117128]]\n",
      "the iterations of  607300 the loss is 0.24283324069396997 theta= [[-12.74744526]\n",
      " [  0.10720324]\n",
      " [  0.10117815]]\n",
      "the iterations of  607400 the loss is 0.24282591027077424 theta= [[-12.74830142]\n",
      " [  0.10721004]\n",
      " [  0.10118502]]\n",
      "the iterations of  607500 the loss is 0.2428185819966056 theta= [[-12.74915746]\n",
      " [  0.10721683]\n",
      " [  0.10119189]]\n",
      "the iterations of  607600 the loss is 0.2428112558705369 theta= [[-12.75001337]\n",
      " [  0.10722363]\n",
      " [  0.10119875]]\n",
      "the iterations of  607700 the loss is 0.2428039318916414 theta= [[-12.75086915]\n",
      " [  0.10723042]\n",
      " [  0.10120562]]\n",
      "the iterations of  607800 the loss is 0.2427966100589929 theta= [[-12.75172481]\n",
      " [  0.10723721]\n",
      " [  0.10121249]]\n",
      "the iterations of  607900 the loss is 0.2427892903716657 theta= [[-12.75258034]\n",
      " [  0.107244  ]\n",
      " [  0.10121935]]\n",
      "the iterations of  608000 the loss is 0.24278197282873498 theta= [[-12.75343575]\n",
      " [  0.10725079]\n",
      " [  0.10122621]]\n",
      "the iterations of  608100 the loss is 0.2427746574292764 theta= [[-12.75429104]\n",
      " [  0.10725758]\n",
      " [  0.10123308]]\n",
      "the iterations of  608200 the loss is 0.24276734417236556 theta= [[-12.75514619]\n",
      " [  0.10726437]\n",
      " [  0.10123994]]\n",
      "the iterations of  608300 the loss is 0.24276003305707924 theta= [[-12.75600123]\n",
      " [  0.10727116]\n",
      " [  0.1012468 ]]\n",
      "the iterations of  608400 the loss is 0.2427527240824946 theta= [[-12.75685613]\n",
      " [  0.10727795]\n",
      " [  0.10125366]]\n",
      "the iterations of  608500 the loss is 0.24274541724768928 theta= [[-12.75771092]\n",
      " [  0.10728473]\n",
      " [  0.10126052]]\n",
      "the iterations of  608600 the loss is 0.24273811255174116 theta= [[-12.75856557]\n",
      " [  0.10729152]\n",
      " [  0.10126737]]\n",
      "the iterations of  608700 the loss is 0.24273080999372937 theta= [[-12.75942011]\n",
      " [  0.1072983 ]\n",
      " [  0.10127423]]\n",
      "the iterations of  608800 the loss is 0.2427235095727328 theta= [[-12.76027451]\n",
      " [  0.10730508]\n",
      " [  0.10128109]]\n",
      "the iterations of  608900 the loss is 0.2427162112878315 theta= [[-12.76112879]\n",
      " [  0.10731186]\n",
      " [  0.10128794]]\n",
      "the iterations of  609000 the loss is 0.24270891513810572 theta= [[-12.76198295]\n",
      " [  0.10731864]\n",
      " [  0.10129479]]\n",
      "the iterations of  609100 the loss is 0.24270162112263616 theta= [[-12.76283698]\n",
      " [  0.10732542]\n",
      " [  0.10130165]]\n",
      "the iterations of  609200 the loss is 0.24269432924050435 theta= [[-12.76369089]\n",
      " [  0.1073322 ]\n",
      " [  0.1013085 ]]\n",
      "the iterations of  609300 the loss is 0.24268703949079223 theta= [[-12.76454467]\n",
      " [  0.10733898]\n",
      " [  0.10131535]]\n",
      "the iterations of  609400 the loss is 0.2426797518725821 theta= [[-12.76539833]\n",
      " [  0.10734576]\n",
      " [  0.1013222 ]]\n",
      "the iterations of  609500 the loss is 0.24267246638495713 theta= [[-12.76625186]\n",
      " [  0.10735253]\n",
      " [  0.10132905]]\n",
      "the iterations of  609600 the loss is 0.2426651830270006 theta= [[-12.76710527]\n",
      " [  0.10735931]\n",
      " [  0.1013359 ]]\n",
      "the iterations of  609700 the loss is 0.24265790179779678 theta= [[-12.76795855]\n",
      " [  0.10736608]\n",
      " [  0.10134274]]\n",
      "the iterations of  609800 the loss is 0.24265062269643012 theta= [[-12.76881171]\n",
      " [  0.10737285]\n",
      " [  0.10134959]]\n",
      "the iterations of  609900 the loss is 0.2426433457219857 theta= [[-12.76966475]\n",
      " [  0.10737962]\n",
      " [  0.10135643]]\n",
      "the iterations of  610000 the loss is 0.24263607087354944 theta= [[-12.77051766]\n",
      " [  0.10738639]\n",
      " [  0.10136328]]\n",
      "the iterations of  610100 the loss is 0.24262879815020721 theta= [[-12.77137044]\n",
      " [  0.10739316]\n",
      " [  0.10137012]]\n",
      "the iterations of  610200 the loss is 0.24262152755104577 theta= [[-12.7722231 ]\n",
      " [  0.10739993]\n",
      " [  0.10137696]]\n",
      "the iterations of  610300 the loss is 0.2426142590751525 theta= [[-12.77307564]\n",
      " [  0.1074067 ]\n",
      " [  0.1013838 ]]\n",
      "the iterations of  610400 the loss is 0.2426069927216149 theta= [[-12.77392805]\n",
      " [  0.10741347]\n",
      " [  0.10139064]]\n",
      "the iterations of  610500 the loss is 0.24259972848952147 theta= [[-12.77478033]\n",
      " [  0.10742023]\n",
      " [  0.10139748]]\n",
      "the iterations of  610600 the loss is 0.24259246637796086 theta= [[-12.7756325 ]\n",
      " [  0.107427  ]\n",
      " [  0.10140432]]\n",
      "the iterations of  610700 the loss is 0.24258520638602263 theta= [[-12.77648453]\n",
      " [  0.10743376]\n",
      " [  0.10141115]]\n",
      "the iterations of  610800 the loss is 0.24257794851279652 theta= [[-12.77733645]\n",
      " [  0.10744052]\n",
      " [  0.10141799]]\n",
      "the iterations of  610900 the loss is 0.24257069275737295 theta= [[-12.77818824]\n",
      " [  0.10744729]\n",
      " [  0.10142483]]\n",
      "the iterations of  611000 the loss is 0.24256343911884276 theta= [[-12.7790399 ]\n",
      " [  0.10745405]\n",
      " [  0.10143166]]\n",
      "the iterations of  611100 the loss is 0.2425561875962975 theta= [[-12.77989144]\n",
      " [  0.10746081]\n",
      " [  0.10143849]]\n",
      "the iterations of  611200 the loss is 0.24254893818882906 theta= [[-12.78074286]\n",
      " [  0.10746757]\n",
      " [  0.10144532]]\n",
      "the iterations of  611300 the loss is 0.2425416908955299 theta= [[-12.78159415]\n",
      " [  0.10747432]\n",
      " [  0.10145215]]\n",
      "the iterations of  611400 the loss is 0.24253444571549337 theta= [[-12.78244532]\n",
      " [  0.10748108]\n",
      " [  0.10145898]]\n",
      "the iterations of  611500 the loss is 0.24252720264781263 theta= [[-12.78329636]\n",
      " [  0.10748784]\n",
      " [  0.10146581]]\n",
      "the iterations of  611600 the loss is 0.24251996169158205 theta= [[-12.78414728]\n",
      " [  0.10749459]\n",
      " [  0.10147264]]\n",
      "the iterations of  611700 the loss is 0.24251272284589614 theta= [[-12.78499808]\n",
      " [  0.10750135]\n",
      " [  0.10147947]]\n",
      "the iterations of  611800 the loss is 0.24250548610984982 theta= [[-12.78584875]\n",
      " [  0.1075081 ]\n",
      " [  0.10148629]]\n",
      "the iterations of  611900 the loss is 0.2424982514825389 theta= [[-12.7866993 ]\n",
      " [  0.10751485]\n",
      " [  0.10149312]]\n",
      "the iterations of  612000 the loss is 0.24249101896305963 theta= [[-12.78754972]\n",
      " [  0.1075216 ]\n",
      " [  0.10149994]]\n",
      "the iterations of  612100 the loss is 0.24248378855050867 theta= [[-12.78840002]\n",
      " [  0.10752835]\n",
      " [  0.10150677]]\n",
      "the iterations of  612200 the loss is 0.2424765602439833 theta= [[-12.7892502 ]\n",
      " [  0.1075351 ]\n",
      " [  0.10151359]]\n",
      "the iterations of  612300 the loss is 0.24246933404258114 theta= [[-12.79010025]\n",
      " [  0.10754185]\n",
      " [  0.10152041]]\n",
      "the iterations of  612400 the loss is 0.24246210994540046 theta= [[-12.79095018]\n",
      " [  0.1075486 ]\n",
      " [  0.10152723]]\n",
      "the iterations of  612500 the loss is 0.24245488795154008 theta= [[-12.79179999]\n",
      " [  0.10755534]\n",
      " [  0.10153405]]\n",
      "the iterations of  612600 the loss is 0.24244766806009935 theta= [[-12.79264967]\n",
      " [  0.10756209]\n",
      " [  0.10154087]]\n",
      "the iterations of  612700 the loss is 0.2424404502701779 theta= [[-12.79349923]\n",
      " [  0.10756883]\n",
      " [  0.10154768]]\n",
      "the iterations of  612800 the loss is 0.24243323458087634 theta= [[-12.79434866]\n",
      " [  0.10757558]\n",
      " [  0.1015545 ]]\n",
      "the iterations of  612900 the loss is 0.24242602099129537 theta= [[-12.79519797]\n",
      " [  0.10758232]\n",
      " [  0.10156132]]\n",
      "the iterations of  613000 the loss is 0.24241880950053624 theta= [[-12.79604716]\n",
      " [  0.10758906]\n",
      " [  0.10156813]]\n",
      "the iterations of  613100 the loss is 0.24241160010770105 theta= [[-12.79689622]\n",
      " [  0.1075958 ]\n",
      " [  0.10157494]]\n",
      "the iterations of  613200 the loss is 0.2424043928118921 theta= [[-12.79774516]\n",
      " [  0.10760254]\n",
      " [  0.10158176]]\n",
      "the iterations of  613300 the loss is 0.24239718761221257 theta= [[-12.79859398]\n",
      " [  0.10760928]\n",
      " [  0.10158857]]\n",
      "the iterations of  613400 the loss is 0.24238998450776564 theta= [[-12.79944267]\n",
      " [  0.10761602]\n",
      " [  0.10159538]]\n",
      "the iterations of  613500 the loss is 0.24238278349765532 theta= [[-12.80029124]\n",
      " [  0.10762275]\n",
      " [  0.10160219]]\n",
      "the iterations of  613600 the loss is 0.24237558458098626 theta= [[-12.80113969]\n",
      " [  0.10762949]\n",
      " [  0.101609  ]]\n",
      "the iterations of  613700 the loss is 0.24236838775686323 theta= [[-12.80198801]\n",
      " [  0.10763622]\n",
      " [  0.1016158 ]]\n",
      "the iterations of  613800 the loss is 0.24236119302439194 theta= [[-12.80283621]\n",
      " [  0.10764296]\n",
      " [  0.10162261]]\n",
      "the iterations of  613900 the loss is 0.24235400038267824 theta= [[-12.80368429]\n",
      " [  0.10764969]\n",
      " [  0.10162941]]\n",
      "the iterations of  614000 the loss is 0.24234680983082885 theta= [[-12.80453224]\n",
      " [  0.10765642]\n",
      " [  0.10163622]]\n",
      "the iterations of  614100 the loss is 0.24233962136795084 theta= [[-12.80538007]\n",
      " [  0.10766315]\n",
      " [  0.10164302]]\n",
      "the iterations of  614200 the loss is 0.2423324349931518 theta= [[-12.80622778]\n",
      " [  0.10766988]\n",
      " [  0.10164983]]\n",
      "the iterations of  614300 the loss is 0.24232525070553954 theta= [[-12.80707536]\n",
      " [  0.10767661]\n",
      " [  0.10165663]]\n",
      "the iterations of  614400 the loss is 0.2423180685042229 theta= [[-12.80792282]\n",
      " [  0.10768334]\n",
      " [  0.10166343]]\n",
      "the iterations of  614500 the loss is 0.24231088838831108 theta= [[-12.80877016]\n",
      " [  0.10769006]\n",
      " [  0.10167023]]\n",
      "the iterations of  614600 the loss is 0.24230371035691367 theta= [[-12.80961737]\n",
      " [  0.10769679]\n",
      " [  0.10167703]]\n",
      "the iterations of  614700 the loss is 0.2422965344091407 theta= [[-12.81046446]\n",
      " [  0.10770352]\n",
      " [  0.10168382]]\n",
      "the iterations of  614800 the loss is 0.24228936054410297 theta= [[-12.81131143]\n",
      " [  0.10771024]\n",
      " [  0.10169062]]\n",
      "the iterations of  614900 the loss is 0.24228218876091148 theta= [[-12.81215828]\n",
      " [  0.10771696]\n",
      " [  0.10169742]]\n",
      "the iterations of  615000 the loss is 0.24227501905867793 theta= [[-12.813005  ]\n",
      " [  0.10772368]\n",
      " [  0.10170421]]\n",
      "the iterations of  615100 the loss is 0.24226785143651466 theta= [[-12.8138516 ]\n",
      " [  0.10773041]\n",
      " [  0.101711  ]]\n",
      "the iterations of  615200 the loss is 0.24226068589353442 theta= [[-12.81469808]\n",
      " [  0.10773713]\n",
      " [  0.1017178 ]]\n",
      "the iterations of  615300 the loss is 0.2422535224288504 theta= [[-12.81554443]\n",
      " [  0.10774385]\n",
      " [  0.10172459]]\n",
      "the iterations of  615400 the loss is 0.24224636104157615 theta= [[-12.81639067]\n",
      " [  0.10775056]\n",
      " [  0.10173138]]\n",
      "the iterations of  615500 the loss is 0.24223920173082597 theta= [[-12.81723678]\n",
      " [  0.10775728]\n",
      " [  0.10173817]]\n",
      "the iterations of  615600 the loss is 0.24223204449571464 theta= [[-12.81808276]\n",
      " [  0.107764  ]\n",
      " [  0.10174496]]\n",
      "the iterations of  615700 the loss is 0.2422248893353575 theta= [[-12.81892863]\n",
      " [  0.10777071]\n",
      " [  0.10175175]]\n",
      "the iterations of  615800 the loss is 0.24221773624887025 theta= [[-12.81977437]\n",
      " [  0.10777743]\n",
      " [  0.10175853]]\n",
      "the iterations of  615900 the loss is 0.24221058523536898 theta= [[-12.82061999]\n",
      " [  0.10778414]\n",
      " [  0.10176532]]\n",
      "the iterations of  616000 the loss is 0.24220343629397081 theta= [[-12.82146548]\n",
      " [  0.10779085]\n",
      " [  0.1017721 ]]\n",
      "the iterations of  616100 the loss is 0.24219628942379284 theta= [[-12.82231086]\n",
      " [  0.10779756]\n",
      " [  0.10177889]]\n",
      "the iterations of  616200 the loss is 0.24218914462395283 theta= [[-12.82315611]\n",
      " [  0.10780427]\n",
      " [  0.10178567]]\n",
      "the iterations of  616300 the loss is 0.24218200189356906 theta= [[-12.82400124]\n",
      " [  0.10781098]\n",
      " [  0.10179245]]\n",
      "the iterations of  616400 the loss is 0.2421748612317605 theta= [[-12.82484624]\n",
      " [  0.10781769]\n",
      " [  0.10179924]]\n",
      "the iterations of  616500 the loss is 0.24216772263764633 theta= [[-12.82569113]\n",
      " [  0.1078244 ]\n",
      " [  0.10180602]]\n",
      "the iterations of  616600 the loss is 0.2421605861103464 theta= [[-12.82653589]\n",
      " [  0.10783111]\n",
      " [  0.10181279]]\n",
      "the iterations of  616700 the loss is 0.242153451648981 theta= [[-12.82738053]\n",
      " [  0.10783781]\n",
      " [  0.10181957]]\n",
      "the iterations of  616800 the loss is 0.2421463192526711 theta= [[-12.82822504]\n",
      " [  0.10784452]\n",
      " [  0.10182635]]\n",
      "the iterations of  616900 the loss is 0.24213918892053785 theta= [[-12.82906944]\n",
      " [  0.10785122]\n",
      " [  0.10183313]]\n",
      "the iterations of  617000 the loss is 0.24213206065170317 theta= [[-12.82991371]\n",
      " [  0.10785792]\n",
      " [  0.1018399 ]]\n",
      "the iterations of  617100 the loss is 0.24212493444528949 theta= [[-12.83075786]\n",
      " [  0.10786463]\n",
      " [  0.10184668]]\n",
      "the iterations of  617200 the loss is 0.24211781030041965 theta= [[-12.83160189]\n",
      " [  0.10787133]\n",
      " [  0.10185345]]\n",
      "the iterations of  617300 the loss is 0.24211068821621676 theta= [[-12.8324458 ]\n",
      " [  0.10787803]\n",
      " [  0.10186022]]\n",
      "the iterations of  617400 the loss is 0.2421035681918048 theta= [[-12.83328958]\n",
      " [  0.10788473]\n",
      " [  0.10186699]]\n",
      "the iterations of  617500 the loss is 0.24209645022630835 theta= [[-12.83413324]\n",
      " [  0.10789142]\n",
      " [  0.10187376]]\n",
      "the iterations of  617600 the loss is 0.242089334318852 theta= [[-12.83497679]\n",
      " [  0.10789812]\n",
      " [  0.10188053]]\n",
      "the iterations of  617700 the loss is 0.2420822204685613 theta= [[-12.8358202 ]\n",
      " [  0.10790482]\n",
      " [  0.1018873 ]]\n",
      "the iterations of  617800 the loss is 0.24207510867456175 theta= [[-12.8366635 ]\n",
      " [  0.10791151]\n",
      " [  0.10189407]]\n",
      "the iterations of  617900 the loss is 0.24206799893598016 theta= [[-12.83750667]\n",
      " [  0.10791821]\n",
      " [  0.10190084]]\n",
      "the iterations of  618000 the loss is 0.2420608912519432 theta= [[-12.83834973]\n",
      " [  0.1079249 ]\n",
      " [  0.1019076 ]]\n",
      "the iterations of  618100 the loss is 0.242053785621578 theta= [[-12.83919266]\n",
      " [  0.10793159]\n",
      " [  0.10191437]]\n",
      "the iterations of  618200 the loss is 0.24204668204401297 theta= [[-12.84003547]\n",
      " [  0.10793828]\n",
      " [  0.10192113]]\n",
      "the iterations of  618300 the loss is 0.24203958051837604 theta= [[-12.84087816]\n",
      " [  0.10794497]\n",
      " [  0.10192789]]\n",
      "the iterations of  618400 the loss is 0.2420324810437961 theta= [[-12.84172072]\n",
      " [  0.10795166]\n",
      " [  0.10193466]]\n",
      "the iterations of  618500 the loss is 0.24202538361940257 theta= [[-12.84256317]\n",
      " [  0.10795835]\n",
      " [  0.10194142]]\n",
      "the iterations of  618600 the loss is 0.24201828824432536 theta= [[-12.84340549]\n",
      " [  0.10796504]\n",
      " [  0.10194818]]\n",
      "the iterations of  618700 the loss is 0.24201119491769485 theta= [[-12.84424769]\n",
      " [  0.10797172]\n",
      " [  0.10195494]]\n",
      "the iterations of  618800 the loss is 0.24200410363864175 theta= [[-12.84508977]\n",
      " [  0.10797841]\n",
      " [  0.10196169]]\n",
      "the iterations of  618900 the loss is 0.24199701440629756 theta= [[-12.84593173]\n",
      " [  0.10798509]\n",
      " [  0.10196845]]\n",
      "the iterations of  619000 the loss is 0.241989927219794 theta= [[-12.84677356]\n",
      " [  0.10799178]\n",
      " [  0.10197521]]\n",
      "the iterations of  619100 the loss is 0.24198284207826343 theta= [[-12.84761528]\n",
      " [  0.10799846]\n",
      " [  0.10198196]]\n",
      "the iterations of  619200 the loss is 0.24197575898083865 theta= [[-12.84845687]\n",
      " [  0.10800514]\n",
      " [  0.10198872]]\n",
      "the iterations of  619300 the loss is 0.24196867792665316 theta= [[-12.84929834]\n",
      " [  0.10801182]\n",
      " [  0.10199547]]\n",
      "the iterations of  619400 the loss is 0.24196159891484065 theta= [[-12.85013969]\n",
      " [  0.1080185 ]\n",
      " [  0.10200222]]\n",
      "the iterations of  619500 the loss is 0.24195452194453554 theta= [[-12.85098092]\n",
      " [  0.10802518]\n",
      " [  0.10200897]]\n",
      "the iterations of  619600 the loss is 0.24194744701487264 theta= [[-12.85182203]\n",
      " [  0.10803186]\n",
      " [  0.10201572]]\n",
      "the iterations of  619700 the loss is 0.24194037412498726 theta= [[-12.85266302]\n",
      " [  0.10803854]\n",
      " [  0.10202247]]\n",
      "the iterations of  619800 the loss is 0.2419333032740153 theta= [[-12.85350388]\n",
      " [  0.10804521]\n",
      " [  0.10202922]]\n",
      "the iterations of  619900 the loss is 0.24192623446109296 theta= [[-12.85434463]\n",
      " [  0.10805189]\n",
      " [  0.10203597]]\n",
      "the iterations of  620000 the loss is 0.2419191676853569 theta= [[-12.85518525]\n",
      " [  0.10805856]\n",
      " [  0.10204271]]\n",
      "the iterations of  620100 the loss is 0.24191210294594467 theta= [[-12.85602575]\n",
      " [  0.10806524]\n",
      " [  0.10204946]]\n",
      "the iterations of  620200 the loss is 0.24190504024199386 theta= [[-12.85686613]\n",
      " [  0.10807191]\n",
      " [  0.1020562 ]]\n",
      "the iterations of  620300 the loss is 0.24189797957264275 theta= [[-12.85770639]\n",
      " [  0.10807858]\n",
      " [  0.10206295]]\n",
      "the iterations of  620400 the loss is 0.24189092093703035 theta= [[-12.85854653]\n",
      " [  0.10808525]\n",
      " [  0.10206969]]\n",
      "the iterations of  620500 the loss is 0.2418838643342956 theta= [[-12.85938655]\n",
      " [  0.10809192]\n",
      " [  0.10207643]]\n",
      "the iterations of  620600 the loss is 0.24187680976357848 theta= [[-12.86022645]\n",
      " [  0.10809859]\n",
      " [  0.10208317]]\n",
      "the iterations of  620700 the loss is 0.2418697572240191 theta= [[-12.86106622]\n",
      " [  0.10810525]\n",
      " [  0.10208991]]\n",
      "the iterations of  620800 the loss is 0.24186270671475832 theta= [[-12.86190588]\n",
      " [  0.10811192]\n",
      " [  0.10209665]]\n",
      "the iterations of  620900 the loss is 0.24185565823493727 theta= [[-12.86274541]\n",
      " [  0.10811859]\n",
      " [  0.10210339]]\n",
      "the iterations of  621000 the loss is 0.24184861178369757 theta= [[-12.86358483]\n",
      " [  0.10812525]\n",
      " [  0.10211013]]\n",
      "the iterations of  621100 the loss is 0.2418415673601816 theta= [[-12.86442412]\n",
      " [  0.10813191]\n",
      " [  0.10211686]]\n",
      "the iterations of  621200 the loss is 0.241834524963532 theta= [[-12.86526329]\n",
      " [  0.10813858]\n",
      " [  0.1021236 ]]\n",
      "the iterations of  621300 the loss is 0.241827484592892 theta= [[-12.86610234]\n",
      " [  0.10814524]\n",
      " [  0.10213033]]\n",
      "the iterations of  621400 the loss is 0.24182044624740523 theta= [[-12.86694127]\n",
      " [  0.1081519 ]\n",
      " [  0.10213706]]\n",
      "the iterations of  621500 the loss is 0.24181340992621586 theta= [[-12.86778008]\n",
      " [  0.10815856]\n",
      " [  0.1021438 ]]\n",
      "the iterations of  621600 the loss is 0.24180637562846832 theta= [[-12.86861877]\n",
      " [  0.10816522]\n",
      " [  0.10215053]]\n",
      "the iterations of  621700 the loss is 0.2417993433533081 theta= [[-12.86945734]\n",
      " [  0.10817188]\n",
      " [  0.10215726]]\n",
      "the iterations of  621800 the loss is 0.24179231309988047 theta= [[-12.87029579]\n",
      " [  0.10817853]\n",
      " [  0.10216399]]\n",
      "the iterations of  621900 the loss is 0.24178528486733178 theta= [[-12.87113412]\n",
      " [  0.10818519]\n",
      " [  0.10217071]]\n",
      "the iterations of  622000 the loss is 0.24177825865480856 theta= [[-12.87197232]\n",
      " [  0.10819185]\n",
      " [  0.10217744]]\n",
      "the iterations of  622100 the loss is 0.24177123446145787 theta= [[-12.87281041]\n",
      " [  0.1081985 ]\n",
      " [  0.10218417]]\n",
      "the iterations of  622200 the loss is 0.2417642122864273 theta= [[-12.87364838]\n",
      " [  0.10820515]\n",
      " [  0.10219089]]\n",
      "the iterations of  622300 the loss is 0.24175719212886493 theta= [[-12.87448622]\n",
      " [  0.1082118 ]\n",
      " [  0.10219762]]\n",
      "the iterations of  622400 the loss is 0.2417501739879192 theta= [[-12.87532395]\n",
      " [  0.10821846]\n",
      " [  0.10220434]]\n",
      "the iterations of  622500 the loss is 0.24174315786273912 theta= [[-12.87616155]\n",
      " [  0.10822511]\n",
      " [  0.10221106]]\n",
      "the iterations of  622600 the loss is 0.2417361437524743 theta= [[-12.87699904]\n",
      " [  0.10823176]\n",
      " [  0.10221779]]\n",
      "the iterations of  622700 the loss is 0.24172913165627463 theta= [[-12.8778364 ]\n",
      " [  0.1082384 ]\n",
      " [  0.10222451]]\n",
      "the iterations of  622800 the loss is 0.2417221215732906 theta= [[-12.87867365]\n",
      " [  0.10824505]\n",
      " [  0.10223123]]\n",
      "the iterations of  622900 the loss is 0.24171511350267325 theta= [[-12.87951077]\n",
      " [  0.1082517 ]\n",
      " [  0.10223795]]\n",
      "the iterations of  623000 the loss is 0.2417081074435739 theta= [[-12.88034778]\n",
      " [  0.10825834]\n",
      " [  0.10224466]]\n",
      "the iterations of  623100 the loss is 0.24170110339514445 theta= [[-12.88118466]\n",
      " [  0.10826499]\n",
      " [  0.10225138]]\n",
      "the iterations of  623200 the loss is 0.24169410135653738 theta= [[-12.88202142]\n",
      " [  0.10827163]\n",
      " [  0.1022581 ]]\n",
      "the iterations of  623300 the loss is 0.24168710132690566 theta= [[-12.88285807]\n",
      " [  0.10827828]\n",
      " [  0.10226481]]\n",
      "the iterations of  623400 the loss is 0.2416801033054026 theta= [[-12.88369459]\n",
      " [  0.10828492]\n",
      " [  0.10227152]]\n",
      "the iterations of  623500 the loss is 0.24167310729118197 theta= [[-12.884531  ]\n",
      " [  0.10829156]\n",
      " [  0.10227824]]\n",
      "the iterations of  623600 the loss is 0.24166611328339813 theta= [[-12.88536728]\n",
      " [  0.1082982 ]\n",
      " [  0.10228495]]\n",
      "the iterations of  623700 the loss is 0.24165912128120584 theta= [[-12.88620344]\n",
      " [  0.10830484]\n",
      " [  0.10229166]]\n",
      "the iterations of  623800 the loss is 0.24165213128376048 theta= [[-12.88703949]\n",
      " [  0.10831148]\n",
      " [  0.10229837]]\n",
      "the iterations of  623900 the loss is 0.2416451432902179 theta= [[-12.88787541]\n",
      " [  0.10831811]\n",
      " [  0.10230508]]\n",
      "the iterations of  624000 the loss is 0.24163815729973415 theta= [[-12.88871122]\n",
      " [  0.10832475]\n",
      " [  0.10231179]]\n",
      "the iterations of  624100 the loss is 0.24163117331146602 theta= [[-12.8895469 ]\n",
      " [  0.10833138]\n",
      " [  0.1023185 ]]\n",
      "the iterations of  624200 the loss is 0.24162419132457064 theta= [[-12.89038247]\n",
      " [  0.10833802]\n",
      " [  0.1023252 ]]\n",
      "the iterations of  624300 the loss is 0.24161721133820596 theta= [[-12.89121791]\n",
      " [  0.10834465]\n",
      " [  0.10233191]]\n",
      "the iterations of  624400 the loss is 0.24161023335152976 theta= [[-12.89205324]\n",
      " [  0.10835128]\n",
      " [  0.10233861]]\n",
      "the iterations of  624500 the loss is 0.24160325736370092 theta= [[-12.89288844]\n",
      " [  0.10835792]\n",
      " [  0.10234532]]\n",
      "the iterations of  624600 the loss is 0.24159628337387853 theta= [[-12.89372353]\n",
      " [  0.10836455]\n",
      " [  0.10235202]]\n",
      "the iterations of  624700 the loss is 0.2415893113812222 theta= [[-12.8945585 ]\n",
      " [  0.10837118]\n",
      " [  0.10235872]]\n",
      "the iterations of  624800 the loss is 0.24158234138489185 theta= [[-12.89539334]\n",
      " [  0.10837781]\n",
      " [  0.10236542]]\n",
      "the iterations of  624900 the loss is 0.24157537338404825 theta= [[-12.89622807]\n",
      " [  0.10838443]\n",
      " [  0.10237212]]\n",
      "the iterations of  625000 the loss is 0.24156840737785232 theta= [[-12.89706268]\n",
      " [  0.10839106]\n",
      " [  0.10237882]]\n",
      "the iterations of  625100 the loss is 0.24156144336546553 theta= [[-12.89789717]\n",
      " [  0.10839769]\n",
      " [  0.10238552]]\n",
      "the iterations of  625200 the loss is 0.24155448134604995 theta= [[-12.89873154]\n",
      " [  0.10840431]\n",
      " [  0.10239221]]\n",
      "the iterations of  625300 the loss is 0.24154752131876792 theta= [[-12.89956579]\n",
      " [  0.10841093]\n",
      " [  0.10239891]]\n",
      "the iterations of  625400 the loss is 0.24154056328278234 theta= [[-12.90039992]\n",
      " [  0.10841756]\n",
      " [  0.10240561]]\n",
      "the iterations of  625500 the loss is 0.24153360723725664 theta= [[-12.90123393]\n",
      " [  0.10842418]\n",
      " [  0.1024123 ]]\n",
      "the iterations of  625600 the loss is 0.24152665318135483 theta= [[-12.90206782]\n",
      " [  0.1084308 ]\n",
      " [  0.10241899]]\n",
      "the iterations of  625700 the loss is 0.24151970111424104 theta= [[-12.90290159]\n",
      " [  0.10843742]\n",
      " [  0.10242568]]\n",
      "the iterations of  625800 the loss is 0.2415127510350802 theta= [[-12.90373525]\n",
      " [  0.10844404]\n",
      " [  0.10243238]]\n",
      "the iterations of  625900 the loss is 0.2415058029430376 theta= [[-12.90456878]\n",
      " [  0.10845066]\n",
      " [  0.10243907]]\n",
      "the iterations of  626000 the loss is 0.241498856837279 theta= [[-12.9054022 ]\n",
      " [  0.10845728]\n",
      " [  0.10244576]]\n",
      "the iterations of  626100 the loss is 0.2414919127169706 theta= [[-12.90623549]\n",
      " [  0.10846389]\n",
      " [  0.10245244]]\n",
      "the iterations of  626200 the loss is 0.24148497058127924 theta= [[-12.90706867]\n",
      " [  0.10847051]\n",
      " [  0.10245913]]\n",
      "the iterations of  626300 the loss is 0.24147803042937188 theta= [[-12.90790173]\n",
      " [  0.10847712]\n",
      " [  0.10246582]]\n",
      "the iterations of  626400 the loss is 0.24147109226041613 theta= [[-12.90873467]\n",
      " [  0.10848374]\n",
      " [  0.1024725 ]]\n",
      "the iterations of  626500 the loss is 0.2414641560735805 theta= [[-12.90956749]\n",
      " [  0.10849035]\n",
      " [  0.10247919]]\n",
      "the iterations of  626600 the loss is 0.24145722186803328 theta= [[-12.91040019]\n",
      " [  0.10849696]\n",
      " [  0.10248587]]\n",
      "the iterations of  626700 the loss is 0.24145028964294366 theta= [[-12.91123277]\n",
      " [  0.10850357]\n",
      " [  0.10249255]]\n",
      "the iterations of  626800 the loss is 0.241443359397481 theta= [[-12.91206523]\n",
      " [  0.10851018]\n",
      " [  0.10249924]]\n",
      "the iterations of  626900 the loss is 0.24143643113081553 theta= [[-12.91289758]\n",
      " [  0.10851679]\n",
      " [  0.10250592]]\n",
      "the iterations of  627000 the loss is 0.24142950484211745 theta= [[-12.9137298]\n",
      " [  0.1085234]\n",
      " [  0.1025126]]\n",
      "the iterations of  627100 the loss is 0.241422580530558 theta= [[-12.91456191]\n",
      " [  0.10853001]\n",
      " [  0.10251928]]\n",
      "the iterations of  627200 the loss is 0.24141565819530844 theta= [[-12.9153939 ]\n",
      " [  0.10853661]\n",
      " [  0.10252595]]\n",
      "the iterations of  627300 the loss is 0.24140873783554062 theta= [[-12.91622576]\n",
      " [  0.10854322]\n",
      " [  0.10253263]]\n",
      "the iterations of  627400 the loss is 0.2414018194504269 theta= [[-12.91705751]\n",
      " [  0.10854982]\n",
      " [  0.10253931]]\n",
      "the iterations of  627500 the loss is 0.24139490303914013 theta= [[-12.91788915]\n",
      " [  0.10855643]\n",
      " [  0.10254598]]\n",
      "the iterations of  627600 the loss is 0.2413879886008536 theta= [[-12.91872066]\n",
      " [  0.10856303]\n",
      " [  0.10255266]]\n",
      "the iterations of  627700 the loss is 0.2413810761347409 theta= [[-12.91955205]\n",
      " [  0.10856963]\n",
      " [  0.10255933]]\n",
      "the iterations of  627800 the loss is 0.2413741656399765 theta= [[-12.92038333]\n",
      " [  0.10857623]\n",
      " [  0.102566  ]]\n",
      "the iterations of  627900 the loss is 0.24136725711573478 theta= [[-12.92121449]\n",
      " [  0.10858283]\n",
      " [  0.10257267]]\n",
      "the iterations of  628000 the loss is 0.24136035056119132 theta= [[-12.92204553]\n",
      " [  0.10858943]\n",
      " [  0.10257935]]\n",
      "the iterations of  628100 the loss is 0.2413534459755213 theta= [[-12.92287645]\n",
      " [  0.10859603]\n",
      " [  0.10258601]]\n",
      "the iterations of  628200 the loss is 0.24134654335790093 theta= [[-12.92370725]\n",
      " [  0.10860262]\n",
      " [  0.10259268]]\n",
      "the iterations of  628300 the loss is 0.24133964270750688 theta= [[-12.92453793]\n",
      " [  0.10860922]\n",
      " [  0.10259935]]\n",
      "the iterations of  628400 the loss is 0.24133274402351595 theta= [[-12.9253685 ]\n",
      " [  0.10861582]\n",
      " [  0.10260602]]\n",
      "the iterations of  628500 the loss is 0.24132584730510578 theta= [[-12.92619894]\n",
      " [  0.10862241]\n",
      " [  0.10261268]]\n",
      "the iterations of  628600 the loss is 0.24131895255145422 theta= [[-12.92702927]\n",
      " [  0.108629  ]\n",
      " [  0.10261935]]\n",
      "the iterations of  628700 the loss is 0.2413120597617397 theta= [[-12.92785948]\n",
      " [  0.10863559]\n",
      " [  0.10262601]]\n",
      "the iterations of  628800 the loss is 0.24130516893514112 theta= [[-12.92868958]\n",
      " [  0.10864219]\n",
      " [  0.10263268]]\n",
      "the iterations of  628900 the loss is 0.24129828007083756 theta= [[-12.92951955]\n",
      " [  0.10864878]\n",
      " [  0.10263934]]\n",
      "the iterations of  629000 the loss is 0.24129139316800913 theta= [[-12.93034941]\n",
      " [  0.10865537]\n",
      " [  0.102646  ]]\n",
      "the iterations of  629100 the loss is 0.2412845082258359 theta= [[-12.93117914]\n",
      " [  0.10866195]\n",
      " [  0.10265266]]\n",
      "the iterations of  629200 the loss is 0.2412776252434986 theta= [[-12.93200876]\n",
      " [  0.10866854]\n",
      " [  0.10265932]]\n",
      "the iterations of  629300 the loss is 0.2412707442201784 theta= [[-12.93283826]\n",
      " [  0.10867513]\n",
      " [  0.10266598]]\n",
      "the iterations of  629400 the loss is 0.24126386515505707 theta= [[-12.93366765]\n",
      " [  0.10868171]\n",
      " [  0.10267263]]\n",
      "the iterations of  629500 the loss is 0.2412569880473166 theta= [[-12.93449691]\n",
      " [  0.1086883 ]\n",
      " [  0.10267929]]\n",
      "the iterations of  629600 the loss is 0.24125011289613935 theta= [[-12.93532606]\n",
      " [  0.10869488]\n",
      " [  0.10268595]]\n",
      "the iterations of  629700 the loss is 0.24124323970070866 theta= [[-12.93615509]\n",
      " [  0.10870147]\n",
      " [  0.1026926 ]]\n",
      "the iterations of  629800 the loss is 0.2412363684602078 theta= [[-12.936984  ]\n",
      " [  0.10870805]\n",
      " [  0.10269926]]\n",
      "the iterations of  629900 the loss is 0.24122949917382083 theta= [[-12.9378128 ]\n",
      " [  0.10871463]\n",
      " [  0.10270591]]\n",
      "the iterations of  630000 the loss is 0.2412226318407322 theta= [[-12.93864147]\n",
      " [  0.10872121]\n",
      " [  0.10271256]]\n",
      "the iterations of  630100 the loss is 0.24121576646012652 theta= [[-12.93947003]\n",
      " [  0.10872779]\n",
      " [  0.10271921]]\n",
      "the iterations of  630200 the loss is 0.24120890303118928 theta= [[-12.94029847]\n",
      " [  0.10873437]\n",
      " [  0.10272586]]\n",
      "the iterations of  630300 the loss is 0.24120204155310607 theta= [[-12.94112679]\n",
      " [  0.10874094]\n",
      " [  0.10273251]]\n",
      "the iterations of  630400 the loss is 0.24119518202506326 theta= [[-12.941955  ]\n",
      " [  0.10874752]\n",
      " [  0.10273916]]\n",
      "the iterations of  630500 the loss is 0.2411883244462477 theta= [[-12.94278308]\n",
      " [  0.1087541 ]\n",
      " [  0.1027458 ]]\n",
      "the iterations of  630600 the loss is 0.2411814688158463 theta= [[-12.94361105]\n",
      " [  0.10876067]\n",
      " [  0.10275245]]\n",
      "the iterations of  630700 the loss is 0.2411746151330466 theta= [[-12.94443891]\n",
      " [  0.10876725]\n",
      " [  0.1027591 ]]\n",
      "the iterations of  630800 the loss is 0.2411677633970369 theta= [[-12.94526664]\n",
      " [  0.10877382]\n",
      " [  0.10276574]]\n",
      "the iterations of  630900 the loss is 0.2411609136070055 theta= [[-12.94609426]\n",
      " [  0.10878039]\n",
      " [  0.10277238]]\n",
      "the iterations of  631000 the loss is 0.2411540657621415 theta= [[-12.94692176]\n",
      " [  0.10878696]\n",
      " [  0.10277903]]\n",
      "the iterations of  631100 the loss is 0.2411472198616342 theta= [[-12.94774914]\n",
      " [  0.10879353]\n",
      " [  0.10278567]]\n",
      "the iterations of  631200 the loss is 0.24114037590467371 theta= [[-12.9485764 ]\n",
      " [  0.1088001 ]\n",
      " [  0.10279231]]\n",
      "the iterations of  631300 the loss is 0.24113353389045017 theta= [[-12.94940355]\n",
      " [  0.10880667]\n",
      " [  0.10279895]]\n",
      "the iterations of  631400 the loss is 0.24112669381815435 theta= [[-12.95023058]\n",
      " [  0.10881324]\n",
      " [  0.10280559]]\n",
      "the iterations of  631500 the loss is 0.24111985568697772 theta= [[-12.95105749]\n",
      " [  0.1088198 ]\n",
      " [  0.10281223]]\n",
      "the iterations of  631600 the loss is 0.24111301949611166 theta= [[-12.95188428]\n",
      " [  0.10882637]\n",
      " [  0.10281886]]\n",
      "the iterations of  631700 the loss is 0.24110618524474853 theta= [[-12.95271096]\n",
      " [  0.10883293]\n",
      " [  0.1028255 ]]\n",
      "the iterations of  631800 the loss is 0.2410993529320809 theta= [[-12.95353752]\n",
      " [  0.1088395 ]\n",
      " [  0.10283213]]\n",
      "the iterations of  631900 the loss is 0.2410925225573018 theta= [[-12.95436397]\n",
      " [  0.10884606]\n",
      " [  0.10283877]]\n",
      "the iterations of  632000 the loss is 0.24108569411960473 theta= [[-12.95519029]\n",
      " [  0.10885262]\n",
      " [  0.1028454 ]]\n",
      "the iterations of  632100 the loss is 0.24107886761818376 theta= [[-12.9560165 ]\n",
      " [  0.10885918]\n",
      " [  0.10285203]]\n",
      "the iterations of  632200 the loss is 0.24107204305223318 theta= [[-12.95684259]\n",
      " [  0.10886574]\n",
      " [  0.10285866]]\n",
      "the iterations of  632300 the loss is 0.2410652204209478 theta= [[-12.95766857]\n",
      " [  0.1088723 ]\n",
      " [  0.10286529]]\n",
      "the iterations of  632400 the loss is 0.24105839972352325 theta= [[-12.95849442]\n",
      " [  0.10887886]\n",
      " [  0.10287192]]\n",
      "the iterations of  632500 the loss is 0.2410515809591551 theta= [[-12.95932016]\n",
      " [  0.10888541]\n",
      " [  0.10287855]]\n",
      "the iterations of  632600 the loss is 0.24104476412703954 theta= [[-12.96014579]\n",
      " [  0.10889197]\n",
      " [  0.10288518]]\n",
      "the iterations of  632700 the loss is 0.24103794922637323 theta= [[-12.96097129]\n",
      " [  0.10889853]\n",
      " [  0.10289181]]\n",
      "the iterations of  632800 the loss is 0.2410311362563533 theta= [[-12.96179668]\n",
      " [  0.10890508]\n",
      " [  0.10289843]]\n",
      "the iterations of  632900 the loss is 0.24102432521617737 theta= [[-12.96262195]\n",
      " [  0.10891163]\n",
      " [  0.10290506]]\n",
      "the iterations of  633000 the loss is 0.24101751610504338 theta= [[-12.96344711]\n",
      " [  0.10891819]\n",
      " [  0.10291168]]\n",
      "the iterations of  633100 the loss is 0.24101070892215 theta= [[-12.96427215]\n",
      " [  0.10892474]\n",
      " [  0.1029183 ]]\n",
      "the iterations of  633200 the loss is 0.241003903666696 theta= [[-12.96509707]\n",
      " [  0.10893129]\n",
      " [  0.10292493]]\n",
      "the iterations of  633300 the loss is 0.24099710033788083 theta= [[-12.96592188]\n",
      " [  0.10893784]\n",
      " [  0.10293155]]\n",
      "the iterations of  633400 the loss is 0.24099029893490426 theta= [[-12.96674656]\n",
      " [  0.10894439]\n",
      " [  0.10293817]]\n",
      "the iterations of  633500 the loss is 0.2409834994569665 theta= [[-12.96757114]\n",
      " [  0.10895093]\n",
      " [  0.10294479]]\n",
      "the iterations of  633600 the loss is 0.24097670190326836 theta= [[-12.96839559]\n",
      " [  0.10895748]\n",
      " [  0.10295141]]\n",
      "the iterations of  633700 the loss is 0.240969906273011 theta= [[-12.96921993]\n",
      " [  0.10896403]\n",
      " [  0.10295802]]\n",
      "the iterations of  633800 the loss is 0.24096311256539596 theta= [[-12.97004415]\n",
      " [  0.10897057]\n",
      " [  0.10296464]]\n",
      "the iterations of  633900 the loss is 0.2409563207796253 theta= [[-12.97086826]\n",
      " [  0.10897712]\n",
      " [  0.10297126]]\n",
      "the iterations of  634000 the loss is 0.24094953091490154 theta= [[-12.97169224]\n",
      " [  0.10898366]\n",
      " [  0.10297787]]\n",
      "the iterations of  634100 the loss is 0.24094274297042761 theta= [[-12.97251612]\n",
      " [  0.1089902 ]\n",
      " [  0.10298448]]\n",
      "the iterations of  634200 the loss is 0.24093595694540706 theta= [[-12.97333987]\n",
      " [  0.10899674]\n",
      " [  0.1029911 ]]\n",
      "the iterations of  634300 the loss is 0.24092917283904344 theta= [[-12.97416351]\n",
      " [  0.10900329]\n",
      " [  0.10299771]]\n",
      "the iterations of  634400 the loss is 0.2409223906505413 theta= [[-12.97498703]\n",
      " [  0.10900982]\n",
      " [  0.10300432]]\n",
      "the iterations of  634500 the loss is 0.24091561037910522 theta= [[-12.97581044]\n",
      " [  0.10901636]\n",
      " [  0.10301093]]\n",
      "the iterations of  634600 the loss is 0.24090883202394042 theta= [[-12.97663373]\n",
      " [  0.1090229 ]\n",
      " [  0.10301754]]\n",
      "the iterations of  634700 the loss is 0.24090205558425246 theta= [[-12.9774569 ]\n",
      " [  0.10902944]\n",
      " [  0.10302415]]\n",
      "the iterations of  634800 the loss is 0.24089528105924754 theta= [[-12.97827996]\n",
      " [  0.10903597]\n",
      " [  0.10303075]]\n",
      "the iterations of  634900 the loss is 0.24088850844813206 theta= [[-12.9791029 ]\n",
      " [  0.10904251]\n",
      " [  0.10303736]]\n",
      "the iterations of  635000 the loss is 0.24088173775011296 theta= [[-12.97992573]\n",
      " [  0.10904904]\n",
      " [  0.10304397]]\n",
      "the iterations of  635100 the loss is 0.24087496896439775 theta= [[-12.98074843]\n",
      " [  0.10905558]\n",
      " [  0.10305057]]\n",
      "the iterations of  635200 the loss is 0.24086820209019397 theta= [[-12.98157103]\n",
      " [  0.10906211]\n",
      " [  0.10305717]]\n",
      "the iterations of  635300 the loss is 0.24086143712671032 theta= [[-12.9823935 ]\n",
      " [  0.10906864]\n",
      " [  0.10306378]]\n",
      "the iterations of  635400 the loss is 0.2408546740731552 theta= [[-12.98321586]\n",
      " [  0.10907517]\n",
      " [  0.10307038]]\n",
      "the iterations of  635500 the loss is 0.24084791292873794 theta= [[-12.98403811]\n",
      " [  0.1090817 ]\n",
      " [  0.10307698]]\n",
      "the iterations of  635600 the loss is 0.24084115369266804 theta= [[-12.98486023]\n",
      " [  0.10908823]\n",
      " [  0.10308358]]\n",
      "the iterations of  635700 the loss is 0.24083439636415568 theta= [[-12.98568225]\n",
      " [  0.10909476]\n",
      " [  0.10309018]]\n",
      "the iterations of  635800 the loss is 0.24082764094241116 theta= [[-12.98650414]\n",
      " [  0.10910128]\n",
      " [  0.10309678]]\n",
      "the iterations of  635900 the loss is 0.24082088742664556 theta= [[-12.98732592]\n",
      " [  0.10910781]\n",
      " [  0.10310337]]\n",
      "the iterations of  636000 the loss is 0.24081413581607017 theta= [[-12.98814759]\n",
      " [  0.10911434]\n",
      " [  0.10310997]]\n",
      "the iterations of  636100 the loss is 0.24080738610989674 theta= [[-12.98896913]\n",
      " [  0.10912086]\n",
      " [  0.10311657]]\n",
      "the iterations of  636200 the loss is 0.2408006383073376 theta= [[-12.98979057]\n",
      " [  0.10912738]\n",
      " [  0.10312316]]\n",
      "the iterations of  636300 the loss is 0.24079389240760535 theta= [[-12.99061188]\n",
      " [  0.10913391]\n",
      " [  0.10312975]]\n",
      "the iterations of  636400 the loss is 0.24078714840991328 theta= [[-12.99143308]\n",
      " [  0.10914043]\n",
      " [  0.10313635]]\n",
      "the iterations of  636500 the loss is 0.24078040631347478 theta= [[-12.99225417]\n",
      " [  0.10914695]\n",
      " [  0.10314294]]\n",
      "the iterations of  636600 the loss is 0.24077366611750395 theta= [[-12.99307514]\n",
      " [  0.10915347]\n",
      " [  0.10314953]]\n",
      "the iterations of  636700 the loss is 0.24076692782121506 theta= [[-12.99389599]\n",
      " [  0.10915999]\n",
      " [  0.10315612]]\n",
      "the iterations of  636800 the loss is 0.24076019142382335 theta= [[-12.99471673]\n",
      " [  0.1091665 ]\n",
      " [  0.10316271]]\n",
      "the iterations of  636900 the loss is 0.24075345692454386 theta= [[-12.99553735]\n",
      " [  0.10917302]\n",
      " [  0.1031693 ]]\n",
      "the iterations of  637000 the loss is 0.2407467243225923 theta= [[-12.99635786]\n",
      " [  0.10917954]\n",
      " [  0.10317588]]\n",
      "the iterations of  637100 the loss is 0.24073999361718507 theta= [[-12.99717825]\n",
      " [  0.10918605]\n",
      " [  0.10318247]]\n",
      "the iterations of  637200 the loss is 0.2407332648075387 theta= [[-12.99799852]\n",
      " [  0.10919257]\n",
      " [  0.10318905]]\n",
      "the iterations of  637300 the loss is 0.2407265378928702 theta= [[-12.99881868]\n",
      " [  0.10919908]\n",
      " [  0.10319564]]\n",
      "the iterations of  637400 the loss is 0.24071981287239708 theta= [[-12.99963873]\n",
      " [  0.10920559]\n",
      " [  0.10320222]]\n",
      "the iterations of  637500 the loss is 0.24071308974533723 theta= [[-13.00045866]\n",
      " [  0.1092121 ]\n",
      " [  0.10320881]]\n",
      "the iterations of  637600 the loss is 0.24070636851090904 theta= [[-13.00127847]\n",
      " [  0.10921861]\n",
      " [  0.10321539]]\n",
      "the iterations of  637700 the loss is 0.24069964916833148 theta= [[-13.00209817]\n",
      " [  0.10922512]\n",
      " [  0.10322197]]\n",
      "the iterations of  637800 the loss is 0.2406929317168236 theta= [[-13.00291775]\n",
      " [  0.10923163]\n",
      " [  0.10322855]]\n",
      "the iterations of  637900 the loss is 0.24068621615560504 theta= [[-13.00373722]\n",
      " [  0.10923814]\n",
      " [  0.10323513]]\n",
      "the iterations of  638000 the loss is 0.24067950248389589 theta= [[-13.00455657]\n",
      " [  0.10924465]\n",
      " [  0.1032417 ]]\n",
      "the iterations of  638100 the loss is 0.24067279070091685 theta= [[-13.00537581]\n",
      " [  0.10925115]\n",
      " [  0.10324828]]\n",
      "the iterations of  638200 the loss is 0.240666080805889 theta= [[-13.00619493]\n",
      " [  0.10925766]\n",
      " [  0.10325486]]\n",
      "the iterations of  638300 the loss is 0.24065937279803337 theta= [[-13.00701394]\n",
      " [  0.10926416]\n",
      " [  0.10326143]]\n",
      "the iterations of  638400 the loss is 0.240652666676572 theta= [[-13.00783283]\n",
      " [  0.10927066]\n",
      " [  0.10326801]]\n",
      "the iterations of  638500 the loss is 0.24064596244072717 theta= [[-13.0086516 ]\n",
      " [  0.10927717]\n",
      " [  0.10327458]]\n",
      "the iterations of  638600 the loss is 0.24063926008972167 theta= [[-13.00947026]\n",
      " [  0.10928367]\n",
      " [  0.10328115]]\n",
      "the iterations of  638700 the loss is 0.24063255962277846 theta= [[-13.01028881]\n",
      " [  0.10929017]\n",
      " [  0.10328772]]\n",
      "the iterations of  638800 the loss is 0.2406258610391212 theta= [[-13.01110724]\n",
      " [  0.10929667]\n",
      " [  0.10329429]]\n",
      "the iterations of  638900 the loss is 0.2406191643379738 theta= [[-13.01192556]\n",
      " [  0.10930317]\n",
      " [  0.10330086]]\n",
      "the iterations of  639000 the loss is 0.24061246951856094 theta= [[-13.01274376]\n",
      " [  0.10930966]\n",
      " [  0.10330743]]\n",
      "the iterations of  639100 the loss is 0.24060577658010718 theta= [[-13.01356184]\n",
      " [  0.10931616]\n",
      " [  0.103314  ]]\n",
      "the iterations of  639200 the loss is 0.24059908552183795 theta= [[-13.01437982]\n",
      " [  0.10932266]\n",
      " [  0.10332057]]\n",
      "the iterations of  639300 the loss is 0.240592396342979 theta= [[-13.01519767]\n",
      " [  0.10932915]\n",
      " [  0.10332713]]\n",
      "the iterations of  639400 the loss is 0.24058570904275645 theta= [[-13.01601541]\n",
      " [  0.10933565]\n",
      " [  0.1033337 ]]\n",
      "the iterations of  639500 the loss is 0.2405790236203967 theta= [[-13.01683304]\n",
      " [  0.10934214]\n",
      " [  0.10334026]]\n",
      "the iterations of  639600 the loss is 0.24057234007512698 theta= [[-13.01765055]\n",
      " [  0.10934863]\n",
      " [  0.10334683]]\n",
      "the iterations of  639700 the loss is 0.24056565840617458 theta= [[-13.01846795]\n",
      " [  0.10935512]\n",
      " [  0.10335339]]\n",
      "the iterations of  639800 the loss is 0.24055897861276754 theta= [[-13.01928523]\n",
      " [  0.10936161]\n",
      " [  0.10335995]]\n",
      "the iterations of  639900 the loss is 0.24055230069413405 theta= [[-13.0201024 ]\n",
      " [  0.1093681 ]\n",
      " [  0.10336651]]\n",
      "the iterations of  640000 the loss is 0.2405456246495029 theta= [[-13.02091945]\n",
      " [  0.10937459]\n",
      " [  0.10337307]]\n",
      "the iterations of  640100 the loss is 0.24053895047810314 theta= [[-13.02173639]\n",
      " [  0.10938108]\n",
      " [  0.10337963]]\n",
      "the iterations of  640200 the loss is 0.24053227817916445 theta= [[-13.02255321]\n",
      " [  0.10938757]\n",
      " [  0.10338619]]\n",
      "the iterations of  640300 the loss is 0.24052560775191664 theta= [[-13.02336992]\n",
      " [  0.10939405]\n",
      " [  0.10339275]]\n",
      "the iterations of  640400 the loss is 0.2405189391955905 theta= [[-13.02418652]\n",
      " [  0.10940054]\n",
      " [  0.1033993 ]]\n",
      "the iterations of  640500 the loss is 0.24051227250941662 theta= [[-13.025003  ]\n",
      " [  0.10940702]\n",
      " [  0.10340586]]\n",
      "the iterations of  640600 the loss is 0.24050560769262624 theta= [[-13.02581936]\n",
      " [  0.10941351]\n",
      " [  0.10341241]]\n",
      "the iterations of  640700 the loss is 0.24049894474445135 theta= [[-13.02663561]\n",
      " [  0.10941999]\n",
      " [  0.10341896]]\n",
      "the iterations of  640800 the loss is 0.24049228366412367 theta= [[-13.02745175]\n",
      " [  0.10942647]\n",
      " [  0.10342552]]\n",
      "the iterations of  640900 the loss is 0.24048562445087615 theta= [[-13.02826777]\n",
      " [  0.10943295]\n",
      " [  0.10343207]]\n",
      "the iterations of  641000 the loss is 0.2404789671039417 theta= [[-13.02908368]\n",
      " [  0.10943943]\n",
      " [  0.10343862]]\n",
      "the iterations of  641100 the loss is 0.24047231162255348 theta= [[-13.02989947]\n",
      " [  0.10944591]\n",
      " [  0.10344517]]\n",
      "the iterations of  641200 the loss is 0.2404656580059456 theta= [[-13.03071515]\n",
      " [  0.10945239]\n",
      " [  0.10345172]]\n",
      "the iterations of  641300 the loss is 0.24045900625335215 theta= [[-13.03153072]\n",
      " [  0.10945887]\n",
      " [  0.10345827]]\n",
      "the iterations of  641400 the loss is 0.24045235636400794 theta= [[-13.03234617]\n",
      " [  0.10946534]\n",
      " [  0.10346481]]\n",
      "the iterations of  641500 the loss is 0.24044570833714793 theta= [[-13.03316151]\n",
      " [  0.10947182]\n",
      " [  0.10347136]]\n",
      "the iterations of  641600 the loss is 0.24043906217200778 theta= [[-13.03397673]\n",
      " [  0.10947829]\n",
      " [  0.1034779 ]]\n",
      "the iterations of  641700 the loss is 0.24043241786782338 theta= [[-13.03479184]\n",
      " [  0.10948476]\n",
      " [  0.10348445]]\n",
      "the iterations of  641800 the loss is 0.24042577542383112 theta= [[-13.03560683]\n",
      " [  0.10949124]\n",
      " [  0.10349099]]\n",
      "the iterations of  641900 the loss is 0.240419134839268 theta= [[-13.03642171]\n",
      " [  0.10949771]\n",
      " [  0.10349753]]\n",
      "the iterations of  642000 the loss is 0.2404124961133709 theta= [[-13.03723648]\n",
      " [  0.10950418]\n",
      " [  0.10350408]]\n",
      "the iterations of  642100 the loss is 0.24040585924537758 theta= [[-13.03805113]\n",
      " [  0.10951065]\n",
      " [  0.10351062]]\n",
      "the iterations of  642200 the loss is 0.2403992242345262 theta= [[-13.03886567]\n",
      " [  0.10951712]\n",
      " [  0.10351716]]\n",
      "the iterations of  642300 the loss is 0.2403925910800551 theta= [[-13.03968009]\n",
      " [  0.10952359]\n",
      " [  0.1035237 ]]\n",
      "the iterations of  642400 the loss is 0.24038595978120342 theta= [[-13.0404944 ]\n",
      " [  0.10953005]\n",
      " [  0.10353023]]\n",
      "the iterations of  642500 the loss is 0.24037933033721026 theta= [[-13.0413086 ]\n",
      " [  0.10953652]\n",
      " [  0.10353677]]\n",
      "the iterations of  642600 the loss is 0.24037270274731534 theta= [[-13.04212268]\n",
      " [  0.10954299]\n",
      " [  0.10354331]]\n",
      "the iterations of  642700 the loss is 0.24036607701075893 theta= [[-13.04293665]\n",
      " [  0.10954945]\n",
      " [  0.10354984]]\n",
      "the iterations of  642800 the loss is 0.24035945312678175 theta= [[-13.0437505 ]\n",
      " [  0.10955591]\n",
      " [  0.10355638]]\n",
      "the iterations of  642900 the loss is 0.24035283109462457 theta= [[-13.04456424]\n",
      " [  0.10956238]\n",
      " [  0.10356291]]\n",
      "the iterations of  643000 the loss is 0.2403462109135288 theta= [[-13.04537787]\n",
      " [  0.10956884]\n",
      " [  0.10356944]]\n",
      "the iterations of  643100 the loss is 0.24033959258273652 theta= [[-13.04619138]\n",
      " [  0.1095753 ]\n",
      " [  0.10357597]]\n",
      "the iterations of  643200 the loss is 0.24033297610148982 theta= [[-13.04700478]\n",
      " [  0.10958176]\n",
      " [  0.10358251]]\n",
      "the iterations of  643300 the loss is 0.24032636146903147 theta= [[-13.04781807]\n",
      " [  0.10958822]\n",
      " [  0.10358904]]\n",
      "the iterations of  643400 the loss is 0.2403197486846045 theta= [[-13.04863124]\n",
      " [  0.10959468]\n",
      " [  0.10359556]]\n",
      "the iterations of  643500 the loss is 0.2403131377474526 theta= [[-13.0494443 ]\n",
      " [  0.10960113]\n",
      " [  0.10360209]]\n",
      "the iterations of  643600 the loss is 0.2403065286568194 theta= [[-13.05025725]\n",
      " [  0.10960759]\n",
      " [  0.10360862]]\n",
      "the iterations of  643700 the loss is 0.24029992141194945 theta= [[-13.05107008]\n",
      " [  0.10961405]\n",
      " [  0.10361515]]\n",
      "the iterations of  643800 the loss is 0.2402933160120875 theta= [[-13.0518828 ]\n",
      " [  0.1096205 ]\n",
      " [  0.10362167]]\n",
      "the iterations of  643900 the loss is 0.2402867124564786 theta= [[-13.0526954 ]\n",
      " [  0.10962696]\n",
      " [  0.1036282 ]]\n",
      "the iterations of  644000 the loss is 0.24028011074436847 theta= [[-13.05350789]\n",
      " [  0.10963341]\n",
      " [  0.10363472]]\n",
      "the iterations of  644100 the loss is 0.2402735108750031 theta= [[-13.05432027]\n",
      " [  0.10963986]\n",
      " [  0.10364124]]\n",
      "the iterations of  644200 the loss is 0.24026691284762897 theta= [[-13.05513254]\n",
      " [  0.10964631]\n",
      " [  0.10364776]]\n",
      "the iterations of  644300 the loss is 0.24026031666149295 theta= [[-13.05594469]\n",
      " [  0.10965276]\n",
      " [  0.10365428]]\n",
      "the iterations of  644400 the loss is 0.2402537223158421 theta= [[-13.05675672]\n",
      " [  0.10965921]\n",
      " [  0.1036608 ]]\n",
      "the iterations of  644500 the loss is 0.2402471298099244 theta= [[-13.05756865]\n",
      " [  0.10966566]\n",
      " [  0.10366732]]\n",
      "the iterations of  644600 the loss is 0.24024053914298782 theta= [[-13.05838046]\n",
      " [  0.10967211]\n",
      " [  0.10367384]]\n",
      "the iterations of  644700 the loss is 0.2402339503142807 theta= [[-13.05919216]\n",
      " [  0.10967855]\n",
      " [  0.10368036]]\n",
      "the iterations of  644800 the loss is 0.24022736332305217 theta= [[-13.06000374]\n",
      " [  0.109685  ]\n",
      " [  0.10368688]]\n",
      "the iterations of  644900 the loss is 0.24022077816855142 theta= [[-13.06081522]\n",
      " [  0.10969144]\n",
      " [  0.10369339]]\n",
      "the iterations of  645000 the loss is 0.24021419485002832 theta= [[-13.06162657]\n",
      " [  0.10969789]\n",
      " [  0.10369991]]\n",
      "the iterations of  645100 the loss is 0.24020761336673288 theta= [[-13.06243782]\n",
      " [  0.10970433]\n",
      " [  0.10370642]]\n",
      "the iterations of  645200 the loss is 0.2402010337179158 theta= [[-13.06324895]\n",
      " [  0.10971077]\n",
      " [  0.10371293]]\n",
      "the iterations of  645300 the loss is 0.24019445590282804 theta= [[-13.06405997]\n",
      " [  0.10971721]\n",
      " [  0.10371944]]\n",
      "the iterations of  645400 the loss is 0.2401878799207208 theta= [[-13.06487088]\n",
      " [  0.10972366]\n",
      " [  0.10372596]]\n",
      "the iterations of  645500 the loss is 0.24018130577084634 theta= [[-13.06568167]\n",
      " [  0.10973009]\n",
      " [  0.10373247]]\n",
      "the iterations of  645600 the loss is 0.24017473345245655 theta= [[-13.06649235]\n",
      " [  0.10973653]\n",
      " [  0.10373897]]\n",
      "the iterations of  645700 the loss is 0.240168162964804 theta= [[-13.06730292]\n",
      " [  0.10974297]\n",
      " [  0.10374548]]\n",
      "the iterations of  645800 the loss is 0.24016159430714207 theta= [[-13.06811338]\n",
      " [  0.10974941]\n",
      " [  0.10375199]]\n",
      "the iterations of  645900 the loss is 0.24015502747872383 theta= [[-13.06892372]\n",
      " [  0.10975584]\n",
      " [  0.1037585 ]]\n",
      "the iterations of  646000 the loss is 0.2401484624788033 theta= [[-13.06973395]\n",
      " [  0.10976228]\n",
      " [  0.103765  ]]\n",
      "the iterations of  646100 the loss is 0.24014189930663485 theta= [[-13.07054406]\n",
      " [  0.10976871]\n",
      " [  0.10377151]]\n",
      "the iterations of  646200 the loss is 0.24013533796147304 theta= [[-13.07135407]\n",
      " [  0.10977515]\n",
      " [  0.10377801]]\n",
      "the iterations of  646300 the loss is 0.24012877844257297 theta= [[-13.07216396]\n",
      " [  0.10978158]\n",
      " [  0.10378451]]\n",
      "the iterations of  646400 the loss is 0.24012222074919026 theta= [[-13.07297374]\n",
      " [  0.10978801]\n",
      " [  0.10379102]]\n",
      "the iterations of  646500 the loss is 0.2401156648805807 theta= [[-13.0737834 ]\n",
      " [  0.10979444]\n",
      " [  0.10379752]]\n",
      "the iterations of  646600 the loss is 0.24010911083600064 theta= [[-13.07459295]\n",
      " [  0.10980087]\n",
      " [  0.10380402]]\n",
      "the iterations of  646700 the loss is 0.24010255861470678 theta= [[-13.07540239]\n",
      " [  0.1098073 ]\n",
      " [  0.10381052]]\n",
      "the iterations of  646800 the loss is 0.24009600821595622 theta= [[-13.07621172]\n",
      " [  0.10981373]\n",
      " [  0.10381702]]\n",
      "the iterations of  646900 the loss is 0.24008945963900674 theta= [[-13.07702094]\n",
      " [  0.10982016]\n",
      " [  0.10382351]]\n",
      "the iterations of  647000 the loss is 0.24008291288311615 theta= [[-13.07783004]\n",
      " [  0.10982658]\n",
      " [  0.10383001]]\n",
      "the iterations of  647100 the loss is 0.24007636794754267 theta= [[-13.07863903]\n",
      " [  0.10983301]\n",
      " [  0.10383651]]\n",
      "the iterations of  647200 the loss is 0.24006982483154524 theta= [[-13.07944791]\n",
      " [  0.10983943]\n",
      " [  0.103843  ]]\n",
      "the iterations of  647300 the loss is 0.240063283534383 theta= [[-13.08025667]\n",
      " [  0.10984586]\n",
      " [  0.1038495 ]]\n",
      "the iterations of  647400 the loss is 0.24005674405531546 theta= [[-13.08106532]\n",
      " [  0.10985228]\n",
      " [  0.10385599]]\n",
      "the iterations of  647500 the loss is 0.24005020639360264 theta= [[-13.08187386]\n",
      " [  0.1098587 ]\n",
      " [  0.10386248]]\n",
      "the iterations of  647600 the loss is 0.24004367054850498 theta= [[-13.08268229]\n",
      " [  0.10986512]\n",
      " [  0.10386897]]\n",
      "the iterations of  647700 the loss is 0.24003713651928327 theta= [[-13.08349061]\n",
      " [  0.10987154]\n",
      " [  0.10387546]]\n",
      "the iterations of  647800 the loss is 0.24003060430519874 theta= [[-13.08429881]\n",
      " [  0.10987796]\n",
      " [  0.10388195]]\n",
      "the iterations of  647900 the loss is 0.24002407390551292 theta= [[-13.0851069 ]\n",
      " [  0.10988438]\n",
      " [  0.10388844]]\n",
      "the iterations of  648000 the loss is 0.24001754531948788 theta= [[-13.08591488]\n",
      " [  0.1098908 ]\n",
      " [  0.10389493]]\n",
      "the iterations of  648100 the loss is 0.240011018546386 theta= [[-13.08672275]\n",
      " [  0.10989721]\n",
      " [  0.10390142]]\n",
      "the iterations of  648200 the loss is 0.24000449358547016 theta= [[-13.0875305 ]\n",
      " [  0.10990363]\n",
      " [  0.1039079 ]]\n",
      "the iterations of  648300 the loss is 0.23999797043600346 theta= [[-13.08833815]\n",
      " [  0.10991004]\n",
      " [  0.10391439]]\n",
      "the iterations of  648400 the loss is 0.23999144909724968 theta= [[-13.08914568]\n",
      " [  0.10991646]\n",
      " [  0.10392087]]\n",
      "the iterations of  648500 the loss is 0.23998492956847287 theta= [[-13.08995309]\n",
      " [  0.10992287]\n",
      " [  0.10392735]]\n",
      "the iterations of  648600 the loss is 0.23997841184893734 theta= [[-13.0907604 ]\n",
      " [  0.10992928]\n",
      " [  0.10393384]]\n",
      "the iterations of  648700 the loss is 0.2399718959379079 theta= [[-13.09156759]\n",
      " [  0.10993569]\n",
      " [  0.10394032]]\n",
      "the iterations of  648800 the loss is 0.23996538183464988 theta= [[-13.09237468]\n",
      " [  0.1099421 ]\n",
      " [  0.1039468 ]]\n",
      "the iterations of  648900 the loss is 0.23995886953842896 theta= [[-13.09318165]\n",
      " [  0.10994851]\n",
      " [  0.10395328]]\n",
      "the iterations of  649000 the loss is 0.23995235904851106 theta= [[-13.09398851]\n",
      " [  0.10995492]\n",
      " [  0.10395976]]\n",
      "the iterations of  649100 the loss is 0.23994585036416283 theta= [[-13.09479525]\n",
      " [  0.10996133]\n",
      " [  0.10396624]]\n",
      "the iterations of  649200 the loss is 0.23993934348465096 theta= [[-13.09560189]\n",
      " [  0.10996774]\n",
      " [  0.10397271]]\n",
      "the iterations of  649300 the loss is 0.23993283840924262 theta= [[-13.09640841]\n",
      " [  0.10997414]\n",
      " [  0.10397919]]\n",
      "the iterations of  649400 the loss is 0.23992633513720565 theta= [[-13.09721482]\n",
      " [  0.10998055]\n",
      " [  0.10398567]]\n",
      "the iterations of  649500 the loss is 0.23991983366780809 theta= [[-13.09802112]\n",
      " [  0.10998695]\n",
      " [  0.10399214]]\n",
      "the iterations of  649600 the loss is 0.23991333400031828 theta= [[-13.09882731]\n",
      " [  0.10999335]\n",
      " [  0.10399861]]\n",
      "the iterations of  649700 the loss is 0.23990683613400518 theta= [[-13.09963338]\n",
      " [  0.10999976]\n",
      " [  0.10400509]]\n",
      "the iterations of  649800 the loss is 0.23990034006813787 theta= [[-13.10043935]\n",
      " [  0.11000616]\n",
      " [  0.10401156]]\n",
      "the iterations of  649900 the loss is 0.23989384580198625 theta= [[-13.1012452 ]\n",
      " [  0.11001256]\n",
      " [  0.10401803]]\n",
      "the iterations of  650000 the loss is 0.23988735333482025 theta= [[-13.10205094]\n",
      " [  0.11001896]\n",
      " [  0.1040245 ]]\n",
      "the iterations of  650100 the loss is 0.23988086266591022 theta= [[-13.10285657]\n",
      " [  0.11002536]\n",
      " [  0.10403097]]\n",
      "the iterations of  650200 the loss is 0.23987437379452714 theta= [[-13.10366209]\n",
      " [  0.11003176]\n",
      " [  0.10403744]]\n",
      "the iterations of  650300 the loss is 0.23986788671994227 theta= [[-13.10446749]\n",
      " [  0.11003815]\n",
      " [  0.1040439 ]]\n",
      "the iterations of  650400 the loss is 0.2398614014414271 theta= [[-13.10527279]\n",
      " [  0.11004455]\n",
      " [  0.10405037]]\n",
      "the iterations of  650500 the loss is 0.2398549179582538 theta= [[-13.10607797]\n",
      " [  0.11005094]\n",
      " [  0.10405684]]\n",
      "the iterations of  650600 the loss is 0.23984843626969482 theta= [[-13.10688304]\n",
      " [  0.11005734]\n",
      " [  0.1040633 ]]\n",
      "the iterations of  650700 the loss is 0.23984195637502306 theta= [[-13.107688  ]\n",
      " [  0.11006373]\n",
      " [  0.10406976]]\n",
      "the iterations of  650800 the loss is 0.23983547827351157 theta= [[-13.10849285]\n",
      " [  0.11007013]\n",
      " [  0.10407623]]\n",
      "the iterations of  650900 the loss is 0.23982900196443413 theta= [[-13.10929759]\n",
      " [  0.11007652]\n",
      " [  0.10408269]]\n",
      "the iterations of  651000 the loss is 0.23982252744706478 theta= [[-13.11010221]\n",
      " [  0.11008291]\n",
      " [  0.10408915]]\n",
      "the iterations of  651100 the loss is 0.23981605472067785 theta= [[-13.11090673]\n",
      " [  0.1100893 ]\n",
      " [  0.10409561]]\n",
      "the iterations of  651200 the loss is 0.23980958378454814 theta= [[-13.11171113]\n",
      " [  0.11009569]\n",
      " [  0.10410207]]\n",
      "the iterations of  651300 the loss is 0.239803114637951 theta= [[-13.11251543]\n",
      " [  0.11010208]\n",
      " [  0.10410853]]\n",
      "the iterations of  651400 the loss is 0.23979664728016195 theta= [[-13.11331961]\n",
      " [  0.11010846]\n",
      " [  0.10411499]]\n",
      "the iterations of  651500 the loss is 0.23979018171045696 theta= [[-13.11412368]\n",
      " [  0.11011485]\n",
      " [  0.10412144]]\n",
      "the iterations of  651600 the loss is 0.23978371792811248 theta= [[-13.11492764]\n",
      " [  0.11012124]\n",
      " [  0.1041279 ]]\n",
      "the iterations of  651700 the loss is 0.23977725593240545 theta= [[-13.11573148]\n",
      " [  0.11012762]\n",
      " [  0.10413436]]\n",
      "the iterations of  651800 the loss is 0.23977079572261267 theta= [[-13.11653522]\n",
      " [  0.11013401]\n",
      " [  0.10414081]]\n",
      "the iterations of  651900 the loss is 0.23976433729801222 theta= [[-13.11733885]\n",
      " [  0.11014039]\n",
      " [  0.10414726]]\n",
      "the iterations of  652000 the loss is 0.2397578806578817 theta= [[-13.11814236]\n",
      " [  0.11014677]\n",
      " [  0.10415372]]\n",
      "the iterations of  652100 the loss is 0.2397514258014996 theta= [[-13.11894576]\n",
      " [  0.11015315]\n",
      " [  0.10416017]]\n",
      "the iterations of  652200 the loss is 0.23974497272814474 theta= [[-13.11974906]\n",
      " [  0.11015953]\n",
      " [  0.10416662]]\n",
      "the iterations of  652300 the loss is 0.23973852143709626 theta= [[-13.12055224]\n",
      " [  0.11016591]\n",
      " [  0.10417307]]\n",
      "the iterations of  652400 the loss is 0.23973207192763357 theta= [[-13.12135531]\n",
      " [  0.11017229]\n",
      " [  0.10417952]]\n",
      "the iterations of  652500 the loss is 0.23972562419903679 theta= [[-13.12215827]\n",
      " [  0.11017867]\n",
      " [  0.10418596]]\n",
      "the iterations of  652600 the loss is 0.23971917825058633 theta= [[-13.12296112]\n",
      " [  0.11018505]\n",
      " [  0.10419241]]\n",
      "the iterations of  652700 the loss is 0.23971273408156266 theta= [[-13.12376385]\n",
      " [  0.11019142]\n",
      " [  0.10419886]]\n",
      "the iterations of  652800 the loss is 0.239706291691247 theta= [[-13.12456648]\n",
      " [  0.1101978 ]\n",
      " [  0.1042053 ]]\n",
      "the iterations of  652900 the loss is 0.239699851078921 theta= [[-13.125369  ]\n",
      " [  0.11020417]\n",
      " [  0.10421175]]\n",
      "the iterations of  653000 the loss is 0.23969341224386645 theta= [[-13.1261714 ]\n",
      " [  0.11021055]\n",
      " [  0.10421819]]\n",
      "the iterations of  653100 the loss is 0.23968697518536558 theta= [[-13.1269737 ]\n",
      " [  0.11021692]\n",
      " [  0.10422463]]\n",
      "the iterations of  653200 the loss is 0.23968053990270113 theta= [[-13.12777588]\n",
      " [  0.11022329]\n",
      " [  0.10423108]]\n",
      "the iterations of  653300 the loss is 0.23967410639515627 theta= [[-13.12857796]\n",
      " [  0.11022966]\n",
      " [  0.10423752]]\n",
      "the iterations of  653400 the loss is 0.23966767466201436 theta= [[-13.12937992]\n",
      " [  0.11023603]\n",
      " [  0.10424396]]\n",
      "the iterations of  653500 the loss is 0.2396612447025592 theta= [[-13.13018177]\n",
      " [  0.1102424 ]\n",
      " [  0.1042504 ]]\n",
      "the iterations of  653600 the loss is 0.2396548165160751 theta= [[-13.13098352]\n",
      " [  0.11024877]\n",
      " [  0.10425683]]\n",
      "the iterations of  653700 the loss is 0.23964839010184677 theta= [[-13.13178515]\n",
      " [  0.11025514]\n",
      " [  0.10426327]]\n",
      "the iterations of  653800 the loss is 0.23964196545915917 theta= [[-13.13258667]\n",
      " [  0.1102615 ]\n",
      " [  0.10426971]]\n",
      "the iterations of  653900 the loss is 0.23963554258729755 theta= [[-13.13338808]\n",
      " [  0.11026787]\n",
      " [  0.10427614]]\n",
      "the iterations of  654000 the loss is 0.23962912148554788 theta= [[-13.13418938]\n",
      " [  0.11027423]\n",
      " [  0.10428258]]\n",
      "the iterations of  654100 the loss is 0.23962270215319642 theta= [[-13.13499057]\n",
      " [  0.1102806 ]\n",
      " [  0.10428901]]\n",
      "the iterations of  654200 the loss is 0.23961628458952958 theta= [[-13.13579165]\n",
      " [  0.11028696]\n",
      " [  0.10429545]]\n",
      "the iterations of  654300 the loss is 0.23960986879383436 theta= [[-13.13659262]\n",
      " [  0.11029332]\n",
      " [  0.10430188]]\n",
      "the iterations of  654400 the loss is 0.23960345476539818 theta= [[-13.13739348]\n",
      " [  0.11029968]\n",
      " [  0.10430831]]\n",
      "the iterations of  654500 the loss is 0.23959704250350877 theta= [[-13.13819422]\n",
      " [  0.11030604]\n",
      " [  0.10431474]]\n",
      "the iterations of  654600 the loss is 0.2395906320074541 theta= [[-13.13899486]\n",
      " [  0.1103124 ]\n",
      " [  0.10432117]]\n",
      "the iterations of  654700 the loss is 0.23958422327652287 theta= [[-13.13979539]\n",
      " [  0.11031876]\n",
      " [  0.1043276 ]]\n",
      "the iterations of  654800 the loss is 0.2395778163100039 theta= [[-13.14059581]\n",
      " [  0.11032512]\n",
      " [  0.10433403]]\n",
      "the iterations of  654900 the loss is 0.2395714111071864 theta= [[-13.14139611]\n",
      " [  0.11033148]\n",
      " [  0.10434045]]\n",
      "the iterations of  655000 the loss is 0.2395650076673601 theta= [[-13.14219631]\n",
      " [  0.11033783]\n",
      " [  0.10434688]]\n",
      "the iterations of  655100 the loss is 0.239558605989815 theta= [[-13.1429964 ]\n",
      " [  0.11034419]\n",
      " [  0.10435331]]\n",
      "the iterations of  655200 the loss is 0.2395522060738417 theta= [[-13.14379638]\n",
      " [  0.11035054]\n",
      " [  0.10435973]]\n",
      "the iterations of  655300 the loss is 0.23954580791873084 theta= [[-13.14459624]\n",
      " [  0.1103569 ]\n",
      " [  0.10436615]]\n",
      "the iterations of  655400 the loss is 0.23953941152377364 theta= [[-13.145396  ]\n",
      " [  0.11036325]\n",
      " [  0.10437258]]\n",
      "the iterations of  655500 the loss is 0.2395330168882619 theta= [[-13.14619565]\n",
      " [  0.1103696 ]\n",
      " [  0.104379  ]]\n",
      "the iterations of  655600 the loss is 0.23952662401148747 theta= [[-13.14699518]\n",
      " [  0.11037595]\n",
      " [  0.10438542]]\n",
      "the iterations of  655700 the loss is 0.23952023289274266 theta= [[-13.14779461]\n",
      " [  0.1103823 ]\n",
      " [  0.10439184]]\n",
      "the iterations of  655800 the loss is 0.23951384353132013 theta= [[-13.14859393]\n",
      " [  0.11038865]\n",
      " [  0.10439826]]\n",
      "the iterations of  655900 the loss is 0.23950745592651324 theta= [[-13.14939314]\n",
      " [  0.110395  ]\n",
      " [  0.10440468]]\n",
      "the iterations of  656000 the loss is 0.23950107007761537 theta= [[-13.15019223]\n",
      " [  0.11040135]\n",
      " [  0.10441109]]\n",
      "the iterations of  656100 the loss is 0.2394946859839204 theta= [[-13.15099122]\n",
      " [  0.1104077 ]\n",
      " [  0.10441751]]\n",
      "the iterations of  656200 the loss is 0.23948830364472268 theta= [[-13.1517901 ]\n",
      " [  0.11041404]\n",
      " [  0.10442393]]\n",
      "the iterations of  656300 the loss is 0.23948192305931662 theta= [[-13.15258887]\n",
      " [  0.11042039]\n",
      " [  0.10443034]]\n",
      "the iterations of  656400 the loss is 0.2394755442269976 theta= [[-13.15338752]\n",
      " [  0.11042673]\n",
      " [  0.10443675]]\n",
      "the iterations of  656500 the loss is 0.23946916714706099 theta= [[-13.15418607]\n",
      " [  0.11043307]\n",
      " [  0.10444317]]\n",
      "the iterations of  656600 the loss is 0.23946279181880237 theta= [[-13.15498451]\n",
      " [  0.11043942]\n",
      " [  0.10444958]]\n",
      "the iterations of  656700 the loss is 0.23945641824151806 theta= [[-13.15578284]\n",
      " [  0.11044576]\n",
      " [  0.10445599]]\n",
      "the iterations of  656800 the loss is 0.2394500464145047 theta= [[-13.15658106]\n",
      " [  0.1104521 ]\n",
      " [  0.1044624 ]]\n",
      "the iterations of  656900 the loss is 0.23944367633705912 theta= [[-13.15737917]\n",
      " [  0.11045844]\n",
      " [  0.10446881]]\n",
      "the iterations of  657000 the loss is 0.2394373080084787 theta= [[-13.15817717]\n",
      " [  0.11046478]\n",
      " [  0.10447522]]\n",
      "the iterations of  657100 the loss is 0.2394309414280611 theta= [[-13.15897506]\n",
      " [  0.11047111]\n",
      " [  0.10448163]]\n",
      "the iterations of  657200 the loss is 0.23942457659510438 theta= [[-13.15977284]\n",
      " [  0.11047745]\n",
      " [  0.10448803]]\n",
      "the iterations of  657300 the loss is 0.23941821350890716 theta= [[-13.16057052]\n",
      " [  0.11048379]\n",
      " [  0.10449444]]\n",
      "the iterations of  657400 the loss is 0.2394118521687682 theta= [[-13.16136808]\n",
      " [  0.11049012]\n",
      " [  0.10450085]]\n",
      "the iterations of  657500 the loss is 0.2394054925739868 theta= [[-13.16216553]\n",
      " [  0.11049646]\n",
      " [  0.10450725]]\n",
      "the iterations of  657600 the loss is 0.23939913472386234 theta= [[-13.16296287]\n",
      " [  0.11050279]\n",
      " [  0.10451365]]\n",
      "the iterations of  657700 the loss is 0.239392778617695 theta= [[-13.16376011]\n",
      " [  0.11050912]\n",
      " [  0.10452006]]\n",
      "the iterations of  657800 the loss is 0.2393864242547851 theta= [[-13.16455723]\n",
      " [  0.11051546]\n",
      " [  0.10452646]]\n",
      "the iterations of  657900 the loss is 0.23938007163443337 theta= [[-13.16535425]\n",
      " [  0.11052179]\n",
      " [  0.10453286]]\n",
      "the iterations of  658000 the loss is 0.2393737207559409 theta= [[-13.16615116]\n",
      " [  0.11052812]\n",
      " [  0.10453926]]\n",
      "the iterations of  658100 the loss is 0.23936737161860924 theta= [[-13.16694795]\n",
      " [  0.11053445]\n",
      " [  0.10454566]]\n",
      "the iterations of  658200 the loss is 0.23936102422174024 theta= [[-13.16774464]\n",
      " [  0.11054078]\n",
      " [  0.10455206]]\n",
      "the iterations of  658300 the loss is 0.23935467856463613 theta= [[-13.16854122]\n",
      " [  0.1105471 ]\n",
      " [  0.10455845]]\n",
      "the iterations of  658400 the loss is 0.2393483346465996 theta= [[-13.16933769]\n",
      " [  0.11055343]\n",
      " [  0.10456485]]\n",
      "the iterations of  658500 the loss is 0.23934199246693347 theta= [[-13.17013405]\n",
      " [  0.11055976]\n",
      " [  0.10457125]]\n",
      "the iterations of  658600 the loss is 0.23933565202494134 theta= [[-13.1709303 ]\n",
      " [  0.11056608]\n",
      " [  0.10457764]]\n",
      "the iterations of  658700 the loss is 0.23932931331992693 theta= [[-13.17172644]\n",
      " [  0.11057241]\n",
      " [  0.10458404]]\n",
      "the iterations of  658800 the loss is 0.23932297635119418 theta= [[-13.17252248]\n",
      " [  0.11057873]\n",
      " [  0.10459043]]\n",
      "the iterations of  658900 the loss is 0.2393166411180479 theta= [[-13.1733184 ]\n",
      " [  0.11058505]\n",
      " [  0.10459682]]\n",
      "the iterations of  659000 the loss is 0.2393103076197928 theta= [[-13.17411421]\n",
      " [  0.11059137]\n",
      " [  0.10460321]]\n",
      "the iterations of  659100 the loss is 0.23930397585573407 theta= [[-13.17490992]\n",
      " [  0.11059769]\n",
      " [  0.1046096 ]]\n",
      "the iterations of  659200 the loss is 0.23929764582517749 theta= [[-13.17570552]\n",
      " [  0.11060401]\n",
      " [  0.10461599]]\n",
      "the iterations of  659300 the loss is 0.23929131752742902 theta= [[-13.17650101]\n",
      " [  0.11061033]\n",
      " [  0.10462238]]\n",
      "the iterations of  659400 the loss is 0.23928499096179487 theta= [[-13.17729639]\n",
      " [  0.11061665]\n",
      " [  0.10462877]]\n",
      "the iterations of  659500 the loss is 0.239278666127582 theta= [[-13.17809166]\n",
      " [  0.11062297]\n",
      " [  0.10463516]]\n",
      "the iterations of  659600 the loss is 0.23927234302409756 theta= [[-13.17888682]\n",
      " [  0.11062929]\n",
      " [  0.10464154]]\n",
      "the iterations of  659700 the loss is 0.23926602165064903 theta= [[-13.17968187]\n",
      " [  0.1106356 ]\n",
      " [  0.10464793]]\n",
      "the iterations of  659800 the loss is 0.239259702006544 theta= [[-13.18047682]\n",
      " [  0.11064192]\n",
      " [  0.10465431]]\n",
      "the iterations of  659900 the loss is 0.23925338409109118 theta= [[-13.18127165]\n",
      " [  0.11064823]\n",
      " [  0.1046607 ]]\n",
      "the iterations of  660000 the loss is 0.23924706790359893 theta= [[-13.18206638]\n",
      " [  0.11065454]\n",
      " [  0.10466708]]\n",
      "the iterations of  660100 the loss is 0.2392407534433762 theta= [[-13.182861  ]\n",
      " [  0.11066086]\n",
      " [  0.10467346]]\n",
      "the iterations of  660200 the loss is 0.23923444070973243 theta= [[-13.18365551]\n",
      " [  0.11066717]\n",
      " [  0.10467984]]\n",
      "the iterations of  660300 the loss is 0.23922812970197735 theta= [[-13.18444991]\n",
      " [  0.11067348]\n",
      " [  0.10468622]]\n",
      "the iterations of  660400 the loss is 0.23922182041942108 theta= [[-13.1852442 ]\n",
      " [  0.11067979]\n",
      " [  0.1046926 ]]\n",
      "the iterations of  660500 the loss is 0.23921551286137419 theta= [[-13.18603838]\n",
      " [  0.1106861 ]\n",
      " [  0.10469898]]\n",
      "the iterations of  660600 the loss is 0.23920920702714749 theta= [[-13.18683246]\n",
      " [  0.1106924 ]\n",
      " [  0.10470536]]\n",
      "the iterations of  660700 the loss is 0.2392029029160522 theta= [[-13.18762642]\n",
      " [  0.11069871]\n",
      " [  0.10471173]]\n",
      "the iterations of  660800 the loss is 0.23919660052739966 theta= [[-13.18842028]\n",
      " [  0.11070502]\n",
      " [  0.10471811]]\n",
      "the iterations of  660900 the loss is 0.2391902998605022 theta= [[-13.18921403]\n",
      " [  0.11071132]\n",
      " [  0.10472448]]\n",
      "the iterations of  661000 the loss is 0.2391840009146719 theta= [[-13.19000767]\n",
      " [  0.11071763]\n",
      " [  0.10473086]]\n",
      "the iterations of  661100 the loss is 0.2391777036892217 theta= [[-13.19080121]\n",
      " [  0.11072393]\n",
      " [  0.10473723]]\n",
      "the iterations of  661200 the loss is 0.23917140818346447 theta= [[-13.19159463]\n",
      " [  0.11073023]\n",
      " [  0.1047436 ]]\n",
      "the iterations of  661300 the loss is 0.2391651143967138 theta= [[-13.19238795]\n",
      " [  0.11073654]\n",
      " [  0.10474998]]\n",
      "the iterations of  661400 the loss is 0.23915882232828342 theta= [[-13.19318116]\n",
      " [  0.11074284]\n",
      " [  0.10475635]]\n",
      "the iterations of  661500 the loss is 0.23915253197748756 theta= [[-13.19397426]\n",
      " [  0.11074914]\n",
      " [  0.10476272]]\n",
      "the iterations of  661600 the loss is 0.23914624334364068 theta= [[-13.19476725]\n",
      " [  0.11075544]\n",
      " [  0.10476909]]\n",
      "the iterations of  661700 the loss is 0.23913995642605787 theta= [[-13.19556013]\n",
      " [  0.11076174]\n",
      " [  0.10477545]]\n",
      "the iterations of  661800 the loss is 0.23913367122405418 theta= [[-13.19635291]\n",
      " [  0.11076803]\n",
      " [  0.10478182]]\n",
      "the iterations of  661900 the loss is 0.2391273877369455 theta= [[-13.19714557]\n",
      " [  0.11077433]\n",
      " [  0.10478819]]\n",
      "the iterations of  662000 the loss is 0.23912110596404776 theta= [[-13.19793813]\n",
      " [  0.11078063]\n",
      " [  0.10479455]]\n",
      "the iterations of  662100 the loss is 0.23911482590467728 theta= [[-13.19873058]\n",
      " [  0.11078692]\n",
      " [  0.10480092]]\n",
      "the iterations of  662200 the loss is 0.23910854755815092 theta= [[-13.19952292]\n",
      " [  0.11079322]\n",
      " [  0.10480728]]\n",
      "the iterations of  662300 the loss is 0.23910227092378572 theta= [[-13.20031516]\n",
      " [  0.11079951]\n",
      " [  0.10481364]]\n",
      "the iterations of  662400 the loss is 0.23909599600089945 theta= [[-13.20110728]\n",
      " [  0.1108058 ]\n",
      " [  0.10482001]]\n",
      "the iterations of  662500 the loss is 0.23908972278880955 theta= [[-13.2018993 ]\n",
      " [  0.11081209]\n",
      " [  0.10482637]]\n",
      "the iterations of  662600 the loss is 0.23908345128683453 theta= [[-13.20269121]\n",
      " [  0.11081839]\n",
      " [  0.10483273]]\n",
      "the iterations of  662700 the loss is 0.2390771814942928 theta= [[-13.20348302]\n",
      " [  0.11082468]\n",
      " [  0.10483909]]\n",
      "the iterations of  662800 the loss is 0.23907091341050354 theta= [[-13.20427471]\n",
      " [  0.11083096]\n",
      " [  0.10484545]]\n",
      "the iterations of  662900 the loss is 0.23906464703478592 theta= [[-13.2050663 ]\n",
      " [  0.11083725]\n",
      " [  0.1048518 ]]\n",
      "the iterations of  663000 the loss is 0.23905838236645963 theta= [[-13.20585778]\n",
      " [  0.11084354]\n",
      " [  0.10485816]]\n",
      "the iterations of  663100 the loss is 0.23905211940484478 theta= [[-13.20664915]\n",
      " [  0.11084983]\n",
      " [  0.10486452]]\n",
      "the iterations of  663200 the loss is 0.2390458581492617 theta= [[-13.20744041]\n",
      " [  0.11085611]\n",
      " [  0.10487087]]\n",
      "the iterations of  663300 the loss is 0.23903959859903115 theta= [[-13.20823157]\n",
      " [  0.1108624 ]\n",
      " [  0.10487723]]\n",
      "the iterations of  663400 the loss is 0.2390333407534744 theta= [[-13.20902261]\n",
      " [  0.11086868]\n",
      " [  0.10488358]]\n",
      "the iterations of  663500 the loss is 0.2390270846119131 theta= [[-13.20981355]\n",
      " [  0.11087497]\n",
      " [  0.10488993]]\n",
      "the iterations of  663600 the loss is 0.23902083017366885 theta= [[-13.21060439]\n",
      " [  0.11088125]\n",
      " [  0.10489629]]\n",
      "the iterations of  663700 the loss is 0.23901457743806392 theta= [[-13.21139511]\n",
      " [  0.11088753]\n",
      " [  0.10490264]]\n",
      "the iterations of  663800 the loss is 0.2390083264044211 theta= [[-13.21218573]\n",
      " [  0.11089381]\n",
      " [  0.10490899]]\n",
      "the iterations of  663900 the loss is 0.23900207707206317 theta= [[-13.21297624]\n",
      " [  0.11090009]\n",
      " [  0.10491534]]\n",
      "the iterations of  664000 the loss is 0.23899582944031336 theta= [[-13.21376664]\n",
      " [  0.11090637]\n",
      " [  0.10492168]]\n",
      "the iterations of  664100 the loss is 0.2389895835084957 theta= [[-13.21455694]\n",
      " [  0.11091265]\n",
      " [  0.10492803]]\n",
      "the iterations of  664200 the loss is 0.23898333927593396 theta= [[-13.21534712]\n",
      " [  0.11091893]\n",
      " [  0.10493438]]\n",
      "the iterations of  664300 the loss is 0.23897709674195278 theta= [[-13.2161372 ]\n",
      " [  0.1109252 ]\n",
      " [  0.10494073]]\n",
      "the iterations of  664400 the loss is 0.2389708559058767 theta= [[-13.21692717]\n",
      " [  0.11093148]\n",
      " [  0.10494707]]\n",
      "the iterations of  664500 the loss is 0.23896461676703115 theta= [[-13.21771704]\n",
      " [  0.11093775]\n",
      " [  0.10495341]]\n",
      "the iterations of  664600 the loss is 0.2389583793247413 theta= [[-13.2185068 ]\n",
      " [  0.11094403]\n",
      " [  0.10495976]]\n",
      "the iterations of  664700 the loss is 0.238952143578333 theta= [[-13.21929645]\n",
      " [  0.1109503 ]\n",
      " [  0.1049661 ]]\n",
      "the iterations of  664800 the loss is 0.23894590952713282 theta= [[-13.22008599]\n",
      " [  0.11095657]\n",
      " [  0.10497244]]\n",
      "the iterations of  664900 the loss is 0.238939677170467 theta= [[-13.22087542]\n",
      " [  0.11096284]\n",
      " [  0.10497878]]\n",
      "the iterations of  665000 the loss is 0.23893344650766288 theta= [[-13.22166475]\n",
      " [  0.11096911]\n",
      " [  0.10498512]]\n",
      "the iterations of  665100 the loss is 0.23892721753804733 theta= [[-13.22245397]\n",
      " [  0.11097538]\n",
      " [  0.10499146]]\n",
      "the iterations of  665200 the loss is 0.23892099026094826 theta= [[-13.22324309]\n",
      " [  0.11098165]\n",
      " [  0.1049978 ]]\n",
      "the iterations of  665300 the loss is 0.23891476467569372 theta= [[-13.22403209]\n",
      " [  0.11098792]\n",
      " [  0.10500414]]\n",
      "the iterations of  665400 the loss is 0.23890854078161197 theta= [[-13.22482099]\n",
      " [  0.11099419]\n",
      " [  0.10501047]]\n",
      "the iterations of  665500 the loss is 0.23890231857803182 theta= [[-13.22560978]\n",
      " [  0.11100045]\n",
      " [  0.10501681]]\n",
      "the iterations of  665600 the loss is 0.2388960980642823 theta= [[-13.22639847]\n",
      " [  0.11100672]\n",
      " [  0.10502314]]\n",
      "the iterations of  665700 the loss is 0.23888987923969282 theta= [[-13.22718704]\n",
      " [  0.11101298]\n",
      " [  0.10502948]]\n",
      "the iterations of  665800 the loss is 0.2388836621035933 theta= [[-13.22797552]\n",
      " [  0.11101925]\n",
      " [  0.10503581]]\n",
      "the iterations of  665900 the loss is 0.2388774466553139 theta= [[-13.22876388]\n",
      " [  0.11102551]\n",
      " [  0.10504214]]\n",
      "the iterations of  666000 the loss is 0.23887123289418516 theta= [[-13.22955214]\n",
      " [  0.11103177]\n",
      " [  0.10504848]]\n",
      "the iterations of  666100 the loss is 0.23886502081953798 theta= [[-13.23034028]\n",
      " [  0.11103804]\n",
      " [  0.10505481]]\n",
      "the iterations of  666200 the loss is 0.23885881043070348 theta= [[-13.23112833]\n",
      " [  0.1110443 ]\n",
      " [  0.10506114]]\n",
      "the iterations of  666300 the loss is 0.2388526017270134 theta= [[-13.23191626]\n",
      " [  0.11105056]\n",
      " [  0.10506746]]\n",
      "the iterations of  666400 the loss is 0.23884639470779967 theta= [[-13.23270409]\n",
      " [  0.11105681]\n",
      " [  0.10507379]]\n",
      "the iterations of  666500 the loss is 0.23884018937239454 theta= [[-13.23349181]\n",
      " [  0.11106307]\n",
      " [  0.10508012]]\n",
      "the iterations of  666600 the loss is 0.23883398572013084 theta= [[-13.23427943]\n",
      " [  0.11106933]\n",
      " [  0.10508645]]\n",
      "the iterations of  666700 the loss is 0.23882778375034153 theta= [[-13.23506694]\n",
      " [  0.11107559]\n",
      " [  0.10509277]]\n",
      "the iterations of  666800 the loss is 0.2388215834623599 theta= [[-13.23585434]\n",
      " [  0.11108184]\n",
      " [  0.1050991 ]]\n",
      "the iterations of  666900 the loss is 0.2388153848555198 theta= [[-13.23664163]\n",
      " [  0.1110881 ]\n",
      " [  0.10510542]]\n",
      "the iterations of  667000 the loss is 0.2388091879291553 theta= [[-13.23742882]\n",
      " [  0.11109435]\n",
      " [  0.10511174]]\n",
      "the iterations of  667100 the loss is 0.23880299268260086 theta= [[-13.2382159 ]\n",
      " [  0.1111006 ]\n",
      " [  0.10511807]]\n",
      "the iterations of  667200 the loss is 0.23879679911519133 theta= [[-13.23900288]\n",
      " [  0.11110685]\n",
      " [  0.10512439]]\n",
      "the iterations of  667300 the loss is 0.23879060722626172 theta= [[-13.23978975]\n",
      " [  0.11111311]\n",
      " [  0.10513071]]\n",
      "the iterations of  667400 the loss is 0.23878441701514766 theta= [[-13.24057651]\n",
      " [  0.11111936]\n",
      " [  0.10513703]]\n",
      "the iterations of  667500 the loss is 0.23877822848118505 theta= [[-13.24136316]\n",
      " [  0.11112561]\n",
      " [  0.10514335]]\n",
      "the iterations of  667600 the loss is 0.23877204162371016 theta= [[-13.24214971]\n",
      " [  0.11113185]\n",
      " [  0.10514966]]\n",
      "the iterations of  667700 the loss is 0.23876585644205928 theta= [[-13.24293615]\n",
      " [  0.1111381 ]\n",
      " [  0.10515598]]\n",
      "the iterations of  667800 the loss is 0.23875967293556977 theta= [[-13.24372249]\n",
      " [  0.11114435]\n",
      " [  0.1051623 ]]\n",
      "the iterations of  667900 the loss is 0.23875349110357857 theta= [[-13.24450872]\n",
      " [  0.1111506 ]\n",
      " [  0.10516861]]\n",
      "the iterations of  668000 the loss is 0.23874731094542348 theta= [[-13.24529484]\n",
      " [  0.11115684]\n",
      " [  0.10517493]]\n",
      "the iterations of  668100 the loss is 0.23874113246044248 theta= [[-13.24608085]\n",
      " [  0.11116309]\n",
      " [  0.10518124]]\n",
      "the iterations of  668200 the loss is 0.23873495564797392 theta= [[-13.24686676]\n",
      " [  0.11116933]\n",
      " [  0.10518755]]\n",
      "the iterations of  668300 the loss is 0.2387287805073565 theta= [[-13.24765257]\n",
      " [  0.11117557]\n",
      " [  0.10519386]]\n",
      "the iterations of  668400 the loss is 0.23872260703792908 theta= [[-13.24843826]\n",
      " [  0.11118181]\n",
      " [  0.10520018]]\n",
      "the iterations of  668500 the loss is 0.23871643523903138 theta= [[-13.24922385]\n",
      " [  0.11118806]\n",
      " [  0.10520649]]\n",
      "the iterations of  668600 the loss is 0.23871026511000298 theta= [[-13.25000934]\n",
      " [  0.1111943 ]\n",
      " [  0.1052128 ]]\n",
      "the iterations of  668700 the loss is 0.23870409665018397 theta= [[-13.25079471]\n",
      " [  0.11120054]\n",
      " [  0.1052191 ]]\n",
      "the iterations of  668800 the loss is 0.2386979298589149 theta= [[-13.25157999]\n",
      " [  0.11120677]\n",
      " [  0.10522541]]\n",
      "the iterations of  668900 the loss is 0.23869176473553655 theta= [[-13.25236515]\n",
      " [  0.11121301]\n",
      " [  0.10523172]]\n",
      "the iterations of  669000 the loss is 0.23868560127939 theta= [[-13.25315021]\n",
      " [  0.11121925]\n",
      " [  0.10523803]]\n",
      "the iterations of  669100 the loss is 0.23867943948981682 theta= [[-13.25393516]\n",
      " [  0.11122549]\n",
      " [  0.10524433]]\n",
      "the iterations of  669200 the loss is 0.23867327936615884 theta= [[-13.25472001]\n",
      " [  0.11123172]\n",
      " [  0.10525063]]\n",
      "the iterations of  669300 the loss is 0.23866712090775827 theta= [[-13.25550475]\n",
      " [  0.11123795]\n",
      " [  0.10525694]]\n",
      "the iterations of  669400 the loss is 0.23866096411395776 theta= [[-13.25628939]\n",
      " [  0.11124419]\n",
      " [  0.10526324]]\n",
      "the iterations of  669500 the loss is 0.2386548089841001 theta= [[-13.25707392]\n",
      " [  0.11125042]\n",
      " [  0.10526954]]\n",
      "the iterations of  669600 the loss is 0.23864865551752865 theta= [[-13.25785834]\n",
      " [  0.11125665]\n",
      " [  0.10527584]]\n",
      "the iterations of  669700 the loss is 0.23864250371358703 theta= [[-13.25864266]\n",
      " [  0.11126288]\n",
      " [  0.10528214]]\n",
      "the iterations of  669800 the loss is 0.23863635357161905 theta= [[-13.25942687]\n",
      " [  0.11126911]\n",
      " [  0.10528844]]\n",
      "the iterations of  669900 the loss is 0.23863020509096913 theta= [[-13.26021097]\n",
      " [  0.11127534]\n",
      " [  0.10529474]]\n",
      "the iterations of  670000 the loss is 0.23862405827098193 theta= [[-13.26099497]\n",
      " [  0.11128157]\n",
      " [  0.10530104]]\n",
      "the iterations of  670100 the loss is 0.23861791311100236 theta= [[-13.26177886]\n",
      " [  0.1112878 ]\n",
      " [  0.10530734]]\n",
      "the iterations of  670200 the loss is 0.23861176961037586 theta= [[-13.26256265]\n",
      " [  0.11129403]\n",
      " [  0.10531363]]\n",
      "the iterations of  670300 the loss is 0.23860562776844813 theta= [[-13.26334633]\n",
      " [  0.11130025]\n",
      " [  0.10531993]]\n",
      "the iterations of  670400 the loss is 0.23859948758456515 theta= [[-13.26412991]\n",
      " [  0.11130648]\n",
      " [  0.10532622]]\n",
      "the iterations of  670500 the loss is 0.23859334905807333 theta= [[-13.26491338]\n",
      " [  0.1113127 ]\n",
      " [  0.10533252]]\n",
      "the iterations of  670600 the loss is 0.23858721218831933 theta= [[-13.26569674]\n",
      " [  0.11131893]\n",
      " [  0.10533881]]\n",
      "the iterations of  670700 the loss is 0.23858107697465042 theta= [[-13.26648   ]\n",
      " [  0.11132515]\n",
      " [  0.1053451 ]]\n",
      "the iterations of  670800 the loss is 0.23857494341641383 theta= [[-13.26726315]\n",
      " [  0.11133137]\n",
      " [  0.10535139]]\n",
      "the iterations of  670900 the loss is 0.2385688115129575 theta= [[-13.2680462 ]\n",
      " [  0.11133759]\n",
      " [  0.10535768]]\n",
      "the iterations of  671000 the loss is 0.23856268126362937 theta= [[-13.26882914]\n",
      " [  0.11134381]\n",
      " [  0.10536397]]\n",
      "the iterations of  671100 the loss is 0.23855655266777812 theta= [[-13.26961198]\n",
      " [  0.11135003]\n",
      " [  0.10537026]]\n",
      "the iterations of  671200 the loss is 0.23855042572475263 theta= [[-13.27039471]\n",
      " [  0.11135625]\n",
      " [  0.10537655]]\n",
      "the iterations of  671300 the loss is 0.23854430043390187 theta= [[-13.27117733]\n",
      " [  0.11136247]\n",
      " [  0.10538283]]\n",
      "the iterations of  671400 the loss is 0.23853817679457534 theta= [[-13.27195985]\n",
      " [  0.11136869]\n",
      " [  0.10538912]]\n",
      "the iterations of  671500 the loss is 0.23853205480612288 theta= [[-13.27274227]\n",
      " [  0.1113749 ]\n",
      " [  0.1053954 ]]\n",
      "the iterations of  671600 the loss is 0.2385259344678948 theta= [[-13.27352457]\n",
      " [  0.11138112]\n",
      " [  0.10540169]]\n",
      "the iterations of  671700 the loss is 0.23851981577924153 theta= [[-13.27430678]\n",
      " [  0.11138733]\n",
      " [  0.10540797]]\n",
      "the iterations of  671800 the loss is 0.23851369873951417 theta= [[-13.27508887]\n",
      " [  0.11139355]\n",
      " [  0.10541425]]\n",
      "the iterations of  671900 the loss is 0.23850758334806382 theta= [[-13.27587087]\n",
      " [  0.11139976]\n",
      " [  0.10542054]]\n",
      "the iterations of  672000 the loss is 0.2385014696042419 theta= [[-13.27665275]\n",
      " [  0.11140597]\n",
      " [  0.10542682]]\n",
      "the iterations of  672100 the loss is 0.23849535750740053 theta= [[-13.27743454]\n",
      " [  0.11141218]\n",
      " [  0.1054331 ]]\n",
      "the iterations of  672200 the loss is 0.2384892470568918 theta= [[-13.27821621]\n",
      " [  0.11141839]\n",
      " [  0.10543938]]\n",
      "the iterations of  672300 the loss is 0.23848313825206852 theta= [[-13.27899778]\n",
      " [  0.1114246 ]\n",
      " [  0.10544565]]\n",
      "the iterations of  672400 the loss is 0.23847703109228344 theta= [[-13.27977925]\n",
      " [  0.11143081]\n",
      " [  0.10545193]]\n",
      "the iterations of  672500 the loss is 0.23847092557688995 theta= [[-13.28056061]\n",
      " [  0.11143702]\n",
      " [  0.10545821]]\n",
      "the iterations of  672600 the loss is 0.23846482170524172 theta= [[-13.28134186]\n",
      " [  0.11144323]\n",
      " [  0.10546449]]\n",
      "the iterations of  672700 the loss is 0.23845871947669256 theta= [[-13.28212301]\n",
      " [  0.11144943]\n",
      " [  0.10547076]]\n",
      "the iterations of  672800 the loss is 0.238452618890597 theta= [[-13.28290406]\n",
      " [  0.11145564]\n",
      " [  0.10547703]]\n",
      "the iterations of  672900 the loss is 0.2384465199463095 theta= [[-13.283685  ]\n",
      " [  0.11146184]\n",
      " [  0.10548331]]\n",
      "the iterations of  673000 the loss is 0.23844042264318524 theta= [[-13.28446583]\n",
      " [  0.11146805]\n",
      " [  0.10548958]]\n",
      "the iterations of  673100 the loss is 0.23843432698057945 theta= [[-13.28524656]\n",
      " [  0.11147425]\n",
      " [  0.10549585]]\n",
      "the iterations of  673200 the loss is 0.23842823295784768 theta= [[-13.28602719]\n",
      " [  0.11148045]\n",
      " [  0.10550212]]\n",
      "the iterations of  673300 the loss is 0.23842214057434627 theta= [[-13.28680771]\n",
      " [  0.11148665]\n",
      " [  0.10550839]]\n",
      "the iterations of  673400 the loss is 0.23841604982943138 theta= [[-13.28758812]\n",
      " [  0.11149285]\n",
      " [  0.10551466]]\n",
      "the iterations of  673500 the loss is 0.23840996072245968 theta= [[-13.28836843]\n",
      " [  0.11149905]\n",
      " [  0.10552093]]\n",
      "the iterations of  673600 the loss is 0.23840387325278833 theta= [[-13.28914864]\n",
      " [  0.11150525]\n",
      " [  0.1055272 ]]\n",
      "the iterations of  673700 the loss is 0.23839778741977471 theta= [[-13.28992874]\n",
      " [  0.11151145]\n",
      " [  0.10553347]]\n",
      "the iterations of  673800 the loss is 0.2383917032227764 theta= [[-13.29070873]\n",
      " [  0.11151765]\n",
      " [  0.10553973]]\n",
      "the iterations of  673900 the loss is 0.23838562066115154 theta= [[-13.29148862]\n",
      " [  0.11152384]\n",
      " [  0.105546  ]]\n",
      "the iterations of  674000 the loss is 0.23837953973425868 theta= [[-13.29226841]\n",
      " [  0.11153004]\n",
      " [  0.10555226]]\n",
      "the iterations of  674100 the loss is 0.23837346044145627 theta= [[-13.29304809]\n",
      " [  0.11153623]\n",
      " [  0.10555852]]\n",
      "the iterations of  674200 the loss is 0.23836738278210362 theta= [[-13.29382766]\n",
      " [  0.11154243]\n",
      " [  0.10556479]]\n",
      "the iterations of  674300 the loss is 0.23836130675556 theta= [[-13.29460714]\n",
      " [  0.11154862]\n",
      " [  0.10557105]]\n",
      "the iterations of  674400 the loss is 0.2383552323611852 theta= [[-13.2953865 ]\n",
      " [  0.11155481]\n",
      " [  0.10557731]]\n",
      "the iterations of  674500 the loss is 0.23834915959833936 theta= [[-13.29616576]\n",
      " [  0.111561  ]\n",
      " [  0.10558357]]\n",
      "the iterations of  674600 the loss is 0.23834308846638297 theta= [[-13.29694492]\n",
      " [  0.11156719]\n",
      " [  0.10558983]]\n",
      "the iterations of  674700 the loss is 0.23833701896467663 theta= [[-13.29772397]\n",
      " [  0.11157338]\n",
      " [  0.10559609]]\n",
      "the iterations of  674800 the loss is 0.23833095109258162 theta= [[-13.29850292]\n",
      " [  0.11157957]\n",
      " [  0.10560234]]\n",
      "the iterations of  674900 the loss is 0.2383248848494593 theta= [[-13.29928176]\n",
      " [  0.11158576]\n",
      " [  0.1056086 ]]\n",
      "the iterations of  675000 the loss is 0.23831882023467144 theta= [[-13.3000605 ]\n",
      " [  0.11159195]\n",
      " [  0.10561486]]\n",
      "the iterations of  675100 the loss is 0.23831275724758022 theta= [[-13.30083914]\n",
      " [  0.11159813]\n",
      " [  0.10562111]]\n",
      "the iterations of  675200 the loss is 0.23830669588754802 theta= [[-13.30161767]\n",
      " [  0.11160432]\n",
      " [  0.10562737]]\n",
      "the iterations of  675300 the loss is 0.23830063615393765 theta= [[-13.30239609]\n",
      " [  0.11161051]\n",
      " [  0.10563362]]\n",
      "the iterations of  675400 the loss is 0.2382945780461122 theta= [[-13.30317441]\n",
      " [  0.11161669]\n",
      " [  0.10563987]]\n",
      "the iterations of  675500 the loss is 0.23828852156343502 theta= [[-13.30395263]\n",
      " [  0.11162287]\n",
      " [  0.10564612]]\n",
      "the iterations of  675600 the loss is 0.23828246670527026 theta= [[-13.30473074]\n",
      " [  0.11162905]\n",
      " [  0.10565238]]\n",
      "the iterations of  675700 the loss is 0.23827641347098166 theta= [[-13.30550875]\n",
      " [  0.11163524]\n",
      " [  0.10565863]]\n",
      "the iterations of  675800 the loss is 0.2382703618599339 theta= [[-13.30628665]\n",
      " [  0.11164142]\n",
      " [  0.10566487]]\n",
      "the iterations of  675900 the loss is 0.23826431187149175 theta= [[-13.30706445]\n",
      " [  0.1116476 ]\n",
      " [  0.10567112]]\n",
      "the iterations of  676000 the loss is 0.23825826350502044 theta= [[-13.30784215]\n",
      " [  0.11165378]\n",
      " [  0.10567737]]\n",
      "the iterations of  676100 the loss is 0.2382522167598853 theta= [[-13.30861974]\n",
      " [  0.11165995]\n",
      " [  0.10568362]]\n",
      "the iterations of  676200 the loss is 0.2382461716354522 theta= [[-13.30939723]\n",
      " [  0.11166613]\n",
      " [  0.10568986]]\n",
      "the iterations of  676300 the loss is 0.23824012813108722 theta= [[-13.31017461]\n",
      " [  0.11167231]\n",
      " [  0.10569611]]\n",
      "the iterations of  676400 the loss is 0.23823408624615694 theta= [[-13.31095189]\n",
      " [  0.11167848]\n",
      " [  0.10570235]]\n",
      "the iterations of  676500 the loss is 0.23822804598002814 theta= [[-13.31172906]\n",
      " [  0.11168466]\n",
      " [  0.1057086 ]]\n",
      "the iterations of  676600 the loss is 0.2382220073320679 theta= [[-13.31250613]\n",
      " [  0.11169083]\n",
      " [  0.10571484]]\n",
      "the iterations of  676700 the loss is 0.23821597030164376 theta= [[-13.3132831 ]\n",
      " [  0.11169701]\n",
      " [  0.10572108]]\n",
      "the iterations of  676800 the loss is 0.23820993488812353 theta= [[-13.31405996]\n",
      " [  0.11170318]\n",
      " [  0.10572732]]\n",
      "the iterations of  676900 the loss is 0.23820390109087522 theta= [[-13.31483671]\n",
      " [  0.11170935]\n",
      " [  0.10573356]]\n",
      "the iterations of  677000 the loss is 0.23819786890926756 theta= [[-13.31561337]\n",
      " [  0.11171552]\n",
      " [  0.1057398 ]]\n",
      "the iterations of  677100 the loss is 0.23819183834266888 theta= [[-13.31638992]\n",
      " [  0.11172169]\n",
      " [  0.10574604]]\n",
      "the iterations of  677200 the loss is 0.2381858093904489 theta= [[-13.31716636]\n",
      " [  0.11172786]\n",
      " [  0.10575228]]\n",
      "the iterations of  677300 the loss is 0.23817978205197665 theta= [[-13.31794271]\n",
      " [  0.11173403]\n",
      " [  0.10575852]]\n",
      "the iterations of  677400 the loss is 0.23817375632662224 theta= [[-13.31871894]\n",
      " [  0.1117402 ]\n",
      " [  0.10576475]]\n",
      "the iterations of  677500 the loss is 0.23816773221375556 theta= [[-13.31949508]\n",
      " [  0.11174636]\n",
      " [  0.10577099]]\n",
      "the iterations of  677600 the loss is 0.23816170971274722 theta= [[-13.32027111]\n",
      " [  0.11175253]\n",
      " [  0.10577722]]\n",
      "the iterations of  677700 the loss is 0.23815568882296786 theta= [[-13.32104703]\n",
      " [  0.11175869]\n",
      " [  0.10578345]]\n",
      "the iterations of  677800 the loss is 0.23814966954378877 theta= [[-13.32182286]\n",
      " [  0.11176486]\n",
      " [  0.10578969]]\n",
      "the iterations of  677900 the loss is 0.23814365187458136 theta= [[-13.32259858]\n",
      " [  0.11177102]\n",
      " [  0.10579592]]\n",
      "the iterations of  678000 the loss is 0.2381376358147174 theta= [[-13.32337419]\n",
      " [  0.11177718]\n",
      " [  0.10580215]]\n",
      "the iterations of  678100 the loss is 0.23813162136356886 theta= [[-13.3241497 ]\n",
      " [  0.11178335]\n",
      " [  0.10580838]]\n",
      "the iterations of  678200 the loss is 0.23812560852050854 theta= [[-13.32492511]\n",
      " [  0.11178951]\n",
      " [  0.10581461]]\n",
      "the iterations of  678300 the loss is 0.23811959728490886 theta= [[-13.32570041]\n",
      " [  0.11179567]\n",
      " [  0.10582084]]\n",
      "the iterations of  678400 the loss is 0.23811358765614327 theta= [[-13.32647562]\n",
      " [  0.11180183]\n",
      " [  0.10582707]]\n",
      "the iterations of  678500 the loss is 0.2381075796335848 theta= [[-13.32725071]\n",
      " [  0.11180799]\n",
      " [  0.10583329]]\n",
      "the iterations of  678600 the loss is 0.2381015732166075 theta= [[-13.32802571]\n",
      " [  0.11181414]\n",
      " [  0.10583952]]\n",
      "the iterations of  678700 the loss is 0.23809556840458534 theta= [[-13.32880059]\n",
      " [  0.1118203 ]\n",
      " [  0.10584575]]\n",
      "the iterations of  678800 the loss is 0.23808956519689267 theta= [[-13.32957538]\n",
      " [  0.11182646]\n",
      " [  0.10585197]]\n",
      "the iterations of  678900 the loss is 0.2380835635929045 theta= [[-13.33035006]\n",
      " [  0.11183261]\n",
      " [  0.10585819]]\n",
      "the iterations of  679000 the loss is 0.23807756359199567 theta= [[-13.33112464]\n",
      " [  0.11183877]\n",
      " [  0.10586442]]\n",
      "the iterations of  679100 the loss is 0.2380715651935416 theta= [[-13.33189912]\n",
      " [  0.11184492]\n",
      " [  0.10587064]]\n",
      "the iterations of  679200 the loss is 0.23806556839691814 theta= [[-13.33267349]\n",
      " [  0.11185107]\n",
      " [  0.10587686]]\n",
      "the iterations of  679300 the loss is 0.2380595732015011 theta= [[-13.33344776]\n",
      " [  0.11185722]\n",
      " [  0.10588308]]\n",
      "the iterations of  679400 the loss is 0.23805357960666731 theta= [[-13.33422193]\n",
      " [  0.11186338]\n",
      " [  0.1058893 ]]\n",
      "the iterations of  679500 the loss is 0.23804758761179312 theta= [[-13.33499599]\n",
      " [  0.11186953]\n",
      " [  0.10589552]]\n",
      "the iterations of  679600 the loss is 0.2380415972162557 theta= [[-13.33576995]\n",
      " [  0.11187568]\n",
      " [  0.10590174]]\n",
      "the iterations of  679700 the loss is 0.23803560841943242 theta= [[-13.3365438 ]\n",
      " [  0.11188182]\n",
      " [  0.10590795]]\n",
      "the iterations of  679800 the loss is 0.23802962122070087 theta= [[-13.33731756]\n",
      " [  0.11188797]\n",
      " [  0.10591417]]\n",
      "the iterations of  679900 the loss is 0.2380236356194393 theta= [[-13.3380912 ]\n",
      " [  0.11189412]\n",
      " [  0.10592039]]\n",
      "the iterations of  680000 the loss is 0.2380176516150258 theta= [[-13.33886475]\n",
      " [  0.11190027]\n",
      " [  0.1059266 ]]\n",
      "the iterations of  680100 the loss is 0.2380116692068392 theta= [[-13.33963819]\n",
      " [  0.11190641]\n",
      " [  0.10593282]]\n",
      "the iterations of  680200 the loss is 0.23800568839425845 theta= [[-13.34041153]\n",
      " [  0.11191256]\n",
      " [  0.10593903]]\n",
      "the iterations of  680300 the loss is 0.23799970917666283 theta= [[-13.34118477]\n",
      " [  0.1119187 ]\n",
      " [  0.10594524]]\n",
      "the iterations of  680400 the loss is 0.237993731553432 theta= [[-13.3419579 ]\n",
      " [  0.11192484]\n",
      " [  0.10595145]]\n",
      "the iterations of  680500 the loss is 0.237987755523946 theta= [[-13.34273093]\n",
      " [  0.11193099]\n",
      " [  0.10595766]]\n",
      "the iterations of  680600 the loss is 0.23798178108758505 theta= [[-13.34350386]\n",
      " [  0.11193713]\n",
      " [  0.10596387]]\n",
      "the iterations of  680700 the loss is 0.23797580824372982 theta= [[-13.34427668]\n",
      " [  0.11194327]\n",
      " [  0.10597008]]\n",
      "the iterations of  680800 the loss is 0.23796983699176122 theta= [[-13.34504941]\n",
      " [  0.11194941]\n",
      " [  0.10597629]]\n",
      "the iterations of  680900 the loss is 0.23796386733106048 theta= [[-13.34582202]\n",
      " [  0.11195555]\n",
      " [  0.1059825 ]]\n",
      "the iterations of  681000 the loss is 0.23795789926100927 theta= [[-13.34659454]\n",
      " [  0.11196168]\n",
      " [  0.1059887 ]]\n",
      "the iterations of  681100 the loss is 0.23795193278098956 theta= [[-13.34736695]\n",
      " [  0.11196782]\n",
      " [  0.10599491]]\n",
      "the iterations of  681200 the loss is 0.23794596789038344 theta= [[-13.34813926]\n",
      " [  0.11197396]\n",
      " [  0.10600111]]\n",
      "the iterations of  681300 the loss is 0.23794000458857353 theta= [[-13.34891147]\n",
      " [  0.11198009]\n",
      " [  0.10600732]]\n",
      "the iterations of  681400 the loss is 0.23793404287494269 theta= [[-13.34968357]\n",
      " [  0.11198623]\n",
      " [  0.10601352]]\n",
      "the iterations of  681500 the loss is 0.23792808274887428 theta= [[-13.35045557]\n",
      " [  0.11199236]\n",
      " [  0.10601972]]\n",
      "the iterations of  681600 the loss is 0.23792212420975153 theta= [[-13.35122747]\n",
      " [  0.1119985 ]\n",
      " [  0.10602593]]\n",
      "the iterations of  681700 the loss is 0.2379161672569586 theta= [[-13.35199927]\n",
      " [  0.11200463]\n",
      " [  0.10603213]]\n",
      "the iterations of  681800 the loss is 0.23791021188987954 theta= [[-13.35277096]\n",
      " [  0.11201076]\n",
      " [  0.10603833]]\n",
      "the iterations of  681900 the loss is 0.23790425810789886 theta= [[-13.35354255]\n",
      " [  0.11201689]\n",
      " [  0.10604453]]\n",
      "the iterations of  682000 the loss is 0.23789830591040131 theta= [[-13.35431404]\n",
      " [  0.11202302]\n",
      " [  0.10605072]]\n",
      "the iterations of  682100 the loss is 0.23789235529677222 theta= [[-13.35508542]\n",
      " [  0.11202915]\n",
      " [  0.10605692]]\n",
      "the iterations of  682200 the loss is 0.23788640626639676 theta= [[-13.3558567 ]\n",
      " [  0.11203528]\n",
      " [  0.10606312]]\n",
      "the iterations of  682300 the loss is 0.23788045881866104 theta= [[-13.35662788]\n",
      " [  0.11204141]\n",
      " [  0.10606932]]\n",
      "the iterations of  682400 the loss is 0.23787451295295092 theta= [[-13.35739896]\n",
      " [  0.11204754]\n",
      " [  0.10607551]]\n",
      "the iterations of  682500 the loss is 0.23786856866865289 theta= [[-13.35816994]\n",
      " [  0.11205366]\n",
      " [  0.1060817 ]]\n",
      "the iterations of  682600 the loss is 0.2378626259651537 theta= [[-13.35894081]\n",
      " [  0.11205979]\n",
      " [  0.1060879 ]]\n",
      "the iterations of  682700 the loss is 0.23785668484184047 theta= [[-13.35971158]\n",
      " [  0.11206591]\n",
      " [  0.10609409]]\n",
      "the iterations of  682800 the loss is 0.23785074529810055 theta= [[-13.36048224]\n",
      " [  0.11207203]\n",
      " [  0.10610028]]\n",
      "the iterations of  682900 the loss is 0.23784480733332164 theta= [[-13.36125281]\n",
      " [  0.11207816]\n",
      " [  0.10610647]]\n",
      "the iterations of  683000 the loss is 0.23783887094689174 theta= [[-13.36202327]\n",
      " [  0.11208428]\n",
      " [  0.10611266]]\n",
      "the iterations of  683100 the loss is 0.23783293613819914 theta= [[-13.36279363]\n",
      " [  0.1120904 ]\n",
      " [  0.10611885]]\n",
      "the iterations of  683200 the loss is 0.23782700290663267 theta= [[-13.36356389]\n",
      " [  0.11209652]\n",
      " [  0.10612504]]\n",
      "the iterations of  683300 the loss is 0.2378210712515812 theta= [[-13.36433404]\n",
      " [  0.11210264]\n",
      " [  0.10613123]]\n",
      "the iterations of  683400 the loss is 0.2378151411724341 theta= [[-13.36510409]\n",
      " [  0.11210876]\n",
      " [  0.10613742]]\n",
      "the iterations of  683500 the loss is 0.23780921266858088 theta= [[-13.36587404]\n",
      " [  0.11211488]\n",
      " [  0.1061436 ]]\n",
      "the iterations of  683600 the loss is 0.2378032857394114 theta= [[-13.36664389]\n",
      " [  0.112121  ]\n",
      " [  0.10614979]]\n",
      "the iterations of  683700 the loss is 0.23779736038431626 theta= [[-13.36741364]\n",
      " [  0.11212711]\n",
      " [  0.10615597]]\n",
      "the iterations of  683800 the loss is 0.23779143660268584 theta= [[-13.36818328]\n",
      " [  0.11213323]\n",
      " [  0.10616216]]\n",
      "the iterations of  683900 the loss is 0.23778551439391069 theta= [[-13.36895282]\n",
      " [  0.11213934]\n",
      " [  0.10616834]]\n",
      "the iterations of  684000 the loss is 0.23777959375738253 theta= [[-13.36972226]\n",
      " [  0.11214546]\n",
      " [  0.10617452]]\n",
      "the iterations of  684100 the loss is 0.23777367469249266 theta= [[-13.3704916 ]\n",
      " [  0.11215157]\n",
      " [  0.1061807 ]]\n",
      "the iterations of  684200 the loss is 0.237767757198633 theta= [[-13.37126084]\n",
      " [  0.11215768]\n",
      " [  0.10618688]]\n",
      "the iterations of  684300 the loss is 0.23776184127519556 theta= [[-13.37202997]\n",
      " [  0.11216379]\n",
      " [  0.10619306]]\n",
      "the iterations of  684400 the loss is 0.2377559269215732 theta= [[-13.372799  ]\n",
      " [  0.1121699 ]\n",
      " [  0.10619924]]\n",
      "the iterations of  684500 the loss is 0.2377500141371582 theta= [[-13.37356793]\n",
      " [  0.11217601]\n",
      " [  0.10620542]]\n",
      "the iterations of  684600 the loss is 0.2377441029213439 theta= [[-13.37433675]\n",
      " [  0.11218212]\n",
      " [  0.1062116 ]]\n",
      "the iterations of  684700 the loss is 0.23773819327352377 theta= [[-13.37510548]\n",
      " [  0.11218823]\n",
      " [  0.10621777]]\n",
      "the iterations of  684800 the loss is 0.2377322851930913 theta= [[-13.3758741 ]\n",
      " [  0.11219434]\n",
      " [  0.10622395]]\n",
      "the iterations of  684900 the loss is 0.237726378679441 theta= [[-13.37664262]\n",
      " [  0.11220045]\n",
      " [  0.10623012]]\n",
      "the iterations of  685000 the loss is 0.2377204737319669 theta= [[-13.37741104]\n",
      " [  0.11220655]\n",
      " [  0.1062363 ]]\n",
      "the iterations of  685100 the loss is 0.23771457035006374 theta= [[-13.37817936]\n",
      " [  0.11221266]\n",
      " [  0.10624247]]\n",
      "the iterations of  685200 the loss is 0.2377086685331266 theta= [[-13.37894758]\n",
      " [  0.11221876]\n",
      " [  0.10624864]]\n",
      "the iterations of  685300 the loss is 0.23770276828055079 theta= [[-13.37971569]\n",
      " [  0.11222486]\n",
      " [  0.10625482]]\n",
      "the iterations of  685400 the loss is 0.23769686959173186 theta= [[-13.3804837 ]\n",
      " [  0.11223097]\n",
      " [  0.10626099]]\n",
      "the iterations of  685500 the loss is 0.23769097246606585 theta= [[-13.38125161]\n",
      " [  0.11223707]\n",
      " [  0.10626716]]\n",
      "the iterations of  685600 the loss is 0.23768507690294885 theta= [[-13.38201942]\n",
      " [  0.11224317]\n",
      " [  0.10627333]]\n",
      "the iterations of  685700 the loss is 0.2376791829017776 theta= [[-13.38278713]\n",
      " [  0.11224927]\n",
      " [  0.10627949]]\n",
      "the iterations of  685800 the loss is 0.23767329046194896 theta= [[-13.38355473]\n",
      " [  0.11225537]\n",
      " [  0.10628566]]\n",
      "the iterations of  685900 the loss is 0.23766739958286007 theta= [[-13.38432223]\n",
      " [  0.11226147]\n",
      " [  0.10629183]]\n",
      "the iterations of  686000 the loss is 0.23766151026390833 theta= [[-13.38508964]\n",
      " [  0.11226757]\n",
      " [  0.10629799]]\n",
      "the iterations of  686100 the loss is 0.23765562250449196 theta= [[-13.38585694]\n",
      " [  0.11227366]\n",
      " [  0.10630416]]\n",
      "the iterations of  686200 the loss is 0.23764973630400868 theta= [[-13.38662413]\n",
      " [  0.11227976]\n",
      " [  0.10631032]]\n",
      "the iterations of  686300 the loss is 0.2376438516618572 theta= [[-13.38739123]\n",
      " [  0.11228586]\n",
      " [  0.10631649]]\n",
      "the iterations of  686400 the loss is 0.23763796857743621 theta= [[-13.38815823]\n",
      " [  0.11229195]\n",
      " [  0.10632265]]\n",
      "the iterations of  686500 the loss is 0.23763208705014474 theta= [[-13.38892512]\n",
      " [  0.11229805]\n",
      " [  0.10632881]]\n",
      "the iterations of  686600 the loss is 0.23762620707938217 theta= [[-13.38969191]\n",
      " [  0.11230414]\n",
      " [  0.10633497]]\n",
      "the iterations of  686700 the loss is 0.2376203286645482 theta= [[-13.3904586 ]\n",
      " [  0.11231023]\n",
      " [  0.10634113]]\n",
      "the iterations of  686800 the loss is 0.23761445180504295 theta= [[-13.39122519]\n",
      " [  0.11231632]\n",
      " [  0.10634729]]\n",
      "the iterations of  686900 the loss is 0.23760857650026668 theta= [[-13.39199168]\n",
      " [  0.11232241]\n",
      " [  0.10635345]]\n",
      "the iterations of  687000 the loss is 0.23760270274961998 theta= [[-13.39275807]\n",
      " [  0.1123285 ]\n",
      " [  0.10635961]]\n",
      "the iterations of  687100 the loss is 0.2375968305525039 theta= [[-13.39352435]\n",
      " [  0.11233459]\n",
      " [  0.10636577]]\n",
      "the iterations of  687200 the loss is 0.23759095990831955 theta= [[-13.39429054]\n",
      " [  0.11234068]\n",
      " [  0.10637192]]\n",
      "the iterations of  687300 the loss is 0.23758509081646864 theta= [[-13.39505662]\n",
      " [  0.11234677]\n",
      " [  0.10637808]]\n",
      "the iterations of  687400 the loss is 0.23757922327635306 theta= [[-13.3958226 ]\n",
      " [  0.11235285]\n",
      " [  0.10638423]]\n",
      "the iterations of  687500 the loss is 0.2375733572873749 theta= [[-13.39658848]\n",
      " [  0.11235894]\n",
      " [  0.10639039]]\n",
      "the iterations of  687600 the loss is 0.2375674928489366 theta= [[-13.39735426]\n",
      " [  0.11236503]\n",
      " [  0.10639654]]\n",
      "the iterations of  687700 the loss is 0.23756162996044114 theta= [[-13.39811994]\n",
      " [  0.11237111]\n",
      " [  0.10640269]]\n",
      "the iterations of  687800 the loss is 0.23755576862129146 theta= [[-13.39888551]\n",
      " [  0.11237719]\n",
      " [  0.10640884]]\n",
      "the iterations of  687900 the loss is 0.23754990883089114 theta= [[-13.39965099]\n",
      " [  0.11238328]\n",
      " [  0.106415  ]]\n",
      "the iterations of  688000 the loss is 0.23754405058864378 theta= [[-13.40041636]\n",
      " [  0.11238936]\n",
      " [  0.10642115]]\n",
      "the iterations of  688100 the loss is 0.23753819389395361 theta= [[-13.40118163]\n",
      " [  0.11239544]\n",
      " [  0.10642729]]\n",
      "the iterations of  688200 the loss is 0.23753233874622492 theta= [[-13.40194681]\n",
      " [  0.11240152]\n",
      " [  0.10643344]]\n",
      "the iterations of  688300 the loss is 0.2375264851448622 theta= [[-13.40271188]\n",
      " [  0.1124076 ]\n",
      " [  0.10643959]]\n",
      "the iterations of  688400 the loss is 0.23752063308927052 theta= [[-13.40347685]\n",
      " [  0.11241368]\n",
      " [  0.10644574]]\n",
      "the iterations of  688500 the loss is 0.23751478257885522 theta= [[-13.40424172]\n",
      " [  0.11241976]\n",
      " [  0.10645188]]\n",
      "the iterations of  688600 the loss is 0.23750893361302194 theta= [[-13.40500648]\n",
      " [  0.11242583]\n",
      " [  0.10645803]]\n",
      "the iterations of  688700 the loss is 0.23750308619117647 theta= [[-13.40577115]\n",
      " [  0.11243191]\n",
      " [  0.10646417]]\n",
      "the iterations of  688800 the loss is 0.237497240312725 theta= [[-13.40653572]\n",
      " [  0.11243799]\n",
      " [  0.10647032]]\n",
      "the iterations of  688900 the loss is 0.23749139597707397 theta= [[-13.40730018]\n",
      " [  0.11244406]\n",
      " [  0.10647646]]\n",
      "the iterations of  689000 the loss is 0.23748555318363046 theta= [[-13.40806455]\n",
      " [  0.11245013]\n",
      " [  0.1064826 ]]\n",
      "the iterations of  689100 the loss is 0.2374797119318014 theta= [[-13.40882881]\n",
      " [  0.11245621]\n",
      " [  0.10648874]]\n",
      "the iterations of  689200 the loss is 0.23747387222099423 theta= [[-13.40959297]\n",
      " [  0.11246228]\n",
      " [  0.10649488]]\n",
      "the iterations of  689300 the loss is 0.23746803405061673 theta= [[-13.41035703]\n",
      " [  0.11246835]\n",
      " [  0.10650102]]\n",
      "the iterations of  689400 the loss is 0.23746219742007696 theta= [[-13.411121  ]\n",
      " [  0.11247442]\n",
      " [  0.10650716]]\n",
      "the iterations of  689500 the loss is 0.2374563623287832 theta= [[-13.41188486]\n",
      " [  0.11248049]\n",
      " [  0.1065133 ]]\n",
      "the iterations of  689600 the loss is 0.23745052877614412 theta= [[-13.41264862]\n",
      " [  0.11248656]\n",
      " [  0.10651943]]\n",
      "the iterations of  689700 the loss is 0.23744469676156876 theta= [[-13.41341227]\n",
      " [  0.11249263]\n",
      " [  0.10652557]]\n",
      "the iterations of  689800 the loss is 0.23743886628446625 theta= [[-13.41417583]\n",
      " [  0.1124987 ]\n",
      " [  0.10653171]]\n",
      "the iterations of  689900 the loss is 0.23743303734424628 theta= [[-13.41493929]\n",
      " [  0.11250477]\n",
      " [  0.10653784]]\n",
      "the iterations of  690000 the loss is 0.23742720994031863 theta= [[-13.41570265]\n",
      " [  0.11251083]\n",
      " [  0.10654397]]\n",
      "the iterations of  690100 the loss is 0.2374213840720936 theta= [[-13.4164659 ]\n",
      " [  0.1125169 ]\n",
      " [  0.10655011]]\n",
      "the iterations of  690200 the loss is 0.23741555973898165 theta= [[-13.41722906]\n",
      " [  0.11252296]\n",
      " [  0.10655624]]\n",
      "the iterations of  690300 the loss is 0.2374097369403936 theta= [[-13.41799211]\n",
      " [  0.11252902]\n",
      " [  0.10656237]]\n",
      "the iterations of  690400 the loss is 0.2374039156757404 theta= [[-13.41875507]\n",
      " [  0.11253509]\n",
      " [  0.1065685 ]]\n",
      "the iterations of  690500 the loss is 0.23739809594443353 theta= [[-13.41951792]\n",
      " [  0.11254115]\n",
      " [  0.10657463]]\n",
      "the iterations of  690600 the loss is 0.23739227774588487 theta= [[-13.42028068]\n",
      " [  0.11254721]\n",
      " [  0.10658076]]\n",
      "the iterations of  690700 the loss is 0.23738646107950617 theta= [[-13.42104333]\n",
      " [  0.11255327]\n",
      " [  0.10658689]]\n",
      "the iterations of  690800 the loss is 0.23738064594470976 theta= [[-13.42180588]\n",
      " [  0.11255933]\n",
      " [  0.10659302]]\n",
      "the iterations of  690900 the loss is 0.23737483234090842 theta= [[-13.42256834]\n",
      " [  0.11256539]\n",
      " [  0.10659914]]\n",
      "the iterations of  691000 the loss is 0.23736902026751494 theta= [[-13.42333069]\n",
      " [  0.11257145]\n",
      " [  0.10660527]]\n",
      "the iterations of  691100 the loss is 0.23736320972394265 theta= [[-13.42409294]\n",
      " [  0.11257751]\n",
      " [  0.10661139]]\n",
      "the iterations of  691200 the loss is 0.23735740070960493 theta= [[-13.42485509]\n",
      " [  0.11258356]\n",
      " [  0.10661752]]\n",
      "the iterations of  691300 the loss is 0.23735159322391566 theta= [[-13.42561714]\n",
      " [  0.11258962]\n",
      " [  0.10662364]]\n",
      "the iterations of  691400 the loss is 0.23734578726628913 theta= [[-13.42637909]\n",
      " [  0.11259567]\n",
      " [  0.10662977]]\n",
      "the iterations of  691500 the loss is 0.2373399828361395 theta= [[-13.42714094]\n",
      " [  0.11260173]\n",
      " [  0.10663589]]\n",
      "the iterations of  691600 the loss is 0.23733417993288178 theta= [[-13.42790269]\n",
      " [  0.11260778]\n",
      " [  0.10664201]]\n",
      "the iterations of  691700 the loss is 0.23732837855593084 theta= [[-13.42866434]\n",
      " [  0.11261383]\n",
      " [  0.10664813]]\n",
      "the iterations of  691800 the loss is 0.23732257870470203 theta= [[-13.4294259 ]\n",
      " [  0.11261989]\n",
      " [  0.10665425]]\n",
      "the iterations of  691900 the loss is 0.2373167803786111 theta= [[-13.43018735]\n",
      " [  0.11262594]\n",
      " [  0.10666037]]\n",
      "the iterations of  692000 the loss is 0.23731098357707378 theta= [[-13.4309487 ]\n",
      " [  0.11263199]\n",
      " [  0.10666648]]\n",
      "the iterations of  692100 the loss is 0.23730518829950648 theta= [[-13.43170994]\n",
      " [  0.11263804]\n",
      " [  0.1066726 ]]\n",
      "the iterations of  692200 the loss is 0.2372993945453257 theta= [[-13.43247109]\n",
      " [  0.11264409]\n",
      " [  0.10667872]]\n",
      "the iterations of  692300 the loss is 0.23729360231394833 theta= [[-13.43323214]\n",
      " [  0.11265013]\n",
      " [  0.10668483]]\n",
      "the iterations of  692400 the loss is 0.23728781160479148 theta= [[-13.43399309]\n",
      " [  0.11265618]\n",
      " [  0.10669095]]\n",
      "the iterations of  692500 the loss is 0.23728202241727242 theta= [[-13.43475394]\n",
      " [  0.11266223]\n",
      " [  0.10669706]]\n",
      "the iterations of  692600 the loss is 0.237276234750809 theta= [[-13.43551469]\n",
      " [  0.11266827]\n",
      " [  0.10670318]]\n",
      "the iterations of  692700 the loss is 0.23727044860481947 theta= [[-13.43627534]\n",
      " [  0.11267432]\n",
      " [  0.10670929]]\n",
      "the iterations of  692800 the loss is 0.23726466397872187 theta= [[-13.43703589]\n",
      " [  0.11268036]\n",
      " [  0.1067154 ]]\n",
      "the iterations of  692900 the loss is 0.23725888087193522 theta= [[-13.43779634]\n",
      " [  0.1126864 ]\n",
      " [  0.10672151]]\n",
      "the iterations of  693000 the loss is 0.23725309928387803 theta= [[-13.43855669]\n",
      " [  0.11269245]\n",
      " [  0.10672762]]\n",
      "the iterations of  693100 the loss is 0.2372473192139699 theta= [[-13.43931694]\n",
      " [  0.11269849]\n",
      " [  0.10673373]]\n",
      "the iterations of  693200 the loss is 0.23724154066163014 theta= [[-13.44007709]\n",
      " [  0.11270453]\n",
      " [  0.10673984]]\n",
      "the iterations of  693300 the loss is 0.2372357636262786 theta= [[-13.44083715]\n",
      " [  0.11271057]\n",
      " [  0.10674595]]\n",
      "the iterations of  693400 the loss is 0.23722998810733553 theta= [[-13.4415971 ]\n",
      " [  0.11271661]\n",
      " [  0.10675205]]\n",
      "the iterations of  693500 the loss is 0.23722421410422137 theta= [[-13.44235695]\n",
      " [  0.11272265]\n",
      " [  0.10675816]]\n",
      "the iterations of  693600 the loss is 0.23721844161635677 theta= [[-13.4431167 ]\n",
      " [  0.11272868]\n",
      " [  0.10676426]]\n",
      "the iterations of  693700 the loss is 0.23721267064316287 theta= [[-13.44387635]\n",
      " [  0.11273472]\n",
      " [  0.10677037]]\n",
      "the iterations of  693800 the loss is 0.23720690118406085 theta= [[-13.4446359 ]\n",
      " [  0.11274076]\n",
      " [  0.10677647]]\n",
      "the iterations of  693900 the loss is 0.2372011332384725 theta= [[-13.44539536]\n",
      " [  0.11274679]\n",
      " [  0.10678257]]\n",
      "the iterations of  694000 the loss is 0.23719536680581965 theta= [[-13.44615471]\n",
      " [  0.11275283]\n",
      " [  0.10678868]]\n",
      "the iterations of  694100 the loss is 0.23718960188552438 theta= [[-13.44691396]\n",
      " [  0.11275886]\n",
      " [  0.10679478]]\n",
      "the iterations of  694200 the loss is 0.23718383847700944 theta= [[-13.44767312]\n",
      " [  0.11276489]\n",
      " [  0.10680088]]\n",
      "the iterations of  694300 the loss is 0.23717807657969753 theta= [[-13.44843217]\n",
      " [  0.11277093]\n",
      " [  0.10680698]]\n",
      "the iterations of  694400 the loss is 0.23717231619301188 theta= [[-13.44919112]\n",
      " [  0.11277696]\n",
      " [  0.10681308]]\n",
      "the iterations of  694500 the loss is 0.2371665573163757 theta= [[-13.44994998]\n",
      " [  0.11278299]\n",
      " [  0.10681918]]\n",
      "the iterations of  694600 the loss is 0.2371607999492129 theta= [[-13.45070874]\n",
      " [  0.11278902]\n",
      " [  0.10682527]]\n",
      "the iterations of  694700 the loss is 0.23715504409094731 theta= [[-13.45146739]\n",
      " [  0.11279505]\n",
      " [  0.10683137]]\n",
      "the iterations of  694800 the loss is 0.23714928974100327 theta= [[-13.45222595]\n",
      " [  0.11280108]\n",
      " [  0.10683747]]\n",
      "the iterations of  694900 the loss is 0.23714353689880543 theta= [[-13.45298441]\n",
      " [  0.1128071 ]\n",
      " [  0.10684356]]\n",
      "the iterations of  695000 the loss is 0.23713778556377865 theta= [[-13.45374276]\n",
      " [  0.11281313]\n",
      " [  0.10684965]]\n",
      "the iterations of  695100 the loss is 0.23713203573534808 theta= [[-13.45450102]\n",
      " [  0.11281916]\n",
      " [  0.10685575]]\n",
      "the iterations of  695200 the loss is 0.23712628741293923 theta= [[-13.45525918]\n",
      " [  0.11282518]\n",
      " [  0.10686184]]\n",
      "the iterations of  695300 the loss is 0.2371205405959779 theta= [[-13.45601724]\n",
      " [  0.1128312 ]\n",
      " [  0.10686793]]\n",
      "the iterations of  695400 the loss is 0.23711479528389018 theta= [[-13.4567752 ]\n",
      " [  0.11283723]\n",
      " [  0.10687402]]\n",
      "the iterations of  695500 the loss is 0.2371090514761022 theta= [[-13.45753307]\n",
      " [  0.11284325]\n",
      " [  0.10688011]]\n",
      "the iterations of  695600 the loss is 0.23710330917204098 theta= [[-13.45829083]\n",
      " [  0.11284927]\n",
      " [  0.1068862 ]]\n",
      "the iterations of  695700 the loss is 0.2370975683711331 theta= [[-13.45904849]\n",
      " [  0.11285529]\n",
      " [  0.10689229]]\n",
      "the iterations of  695800 the loss is 0.23709182907280613 theta= [[-13.45980606]\n",
      " [  0.11286131]\n",
      " [  0.10689838]]\n",
      "the iterations of  695900 the loss is 0.2370860912764875 theta= [[-13.46056352]\n",
      " [  0.11286733]\n",
      " [  0.10690447]]\n",
      "the iterations of  696000 the loss is 0.23708035498160498 theta= [[-13.46132089]\n",
      " [  0.11287335]\n",
      " [  0.10691055]]\n",
      "the iterations of  696100 the loss is 0.23707462018758665 theta= [[-13.46207815]\n",
      " [  0.11287937]\n",
      " [  0.10691664]]\n",
      "the iterations of  696200 the loss is 0.23706888689386105 theta= [[-13.46283532]\n",
      " [  0.11288539]\n",
      " [  0.10692272]]\n",
      "the iterations of  696300 the loss is 0.23706315509985687 theta= [[-13.46359239]\n",
      " [  0.11289141]\n",
      " [  0.10692881]]\n",
      "the iterations of  696400 the loss is 0.23705742480500308 theta= [[-13.46434936]\n",
      " [  0.11289742]\n",
      " [  0.10693489]]\n",
      "the iterations of  696500 the loss is 0.23705169600872897 theta= [[-13.46510623]\n",
      " [  0.11290344]\n",
      " [  0.10694097]]\n",
      "the iterations of  696600 the loss is 0.23704596871046416 theta= [[-13.465863  ]\n",
      " [  0.11290945]\n",
      " [  0.10694705]]\n",
      "the iterations of  696700 the loss is 0.23704024290963854 theta= [[-13.46661968]\n",
      " [  0.11291546]\n",
      " [  0.10695314]]\n",
      "the iterations of  696800 the loss is 0.23703451860568214 theta= [[-13.46737625]\n",
      " [  0.11292148]\n",
      " [  0.10695922]]\n",
      "the iterations of  696900 the loss is 0.23702879579802574 theta= [[-13.46813273]\n",
      " [  0.11292749]\n",
      " [  0.10696529]]\n",
      "the iterations of  697000 the loss is 0.23702307448609963 theta= [[-13.4688891 ]\n",
      " [  0.1129335 ]\n",
      " [  0.10697137]]\n",
      "the iterations of  697100 the loss is 0.23701735466933516 theta= [[-13.46964538]\n",
      " [  0.11293951]\n",
      " [  0.10697745]]\n",
      "the iterations of  697200 the loss is 0.23701163634716374 theta= [[-13.47040156]\n",
      " [  0.11294552]\n",
      " [  0.10698353]]\n",
      "the iterations of  697300 the loss is 0.23700591951901687 theta= [[-13.47115764]\n",
      " [  0.11295153]\n",
      " [  0.1069896 ]]\n",
      "the iterations of  697400 the loss is 0.2370002041843264 theta= [[-13.47191362]\n",
      " [  0.11295754]\n",
      " [  0.10699568]]\n",
      "the iterations of  697500 the loss is 0.2369944903425247 theta= [[-13.4726695 ]\n",
      " [  0.11296354]\n",
      " [  0.10700175]]\n",
      "the iterations of  697600 the loss is 0.23698877799304416 theta= [[-13.47342528]\n",
      " [  0.11296955]\n",
      " [  0.10700783]]\n",
      "the iterations of  697700 the loss is 0.23698306713531767 theta= [[-13.47418097]\n",
      " [  0.11297556]\n",
      " [  0.1070139 ]]\n",
      "the iterations of  697800 the loss is 0.23697735776877835 theta= [[-13.47493655]\n",
      " [  0.11298156]\n",
      " [  0.10701997]]\n",
      "the iterations of  697900 the loss is 0.2369716498928593 theta= [[-13.47569204]\n",
      " [  0.11298756]\n",
      " [  0.10702604]]\n",
      "the iterations of  698000 the loss is 0.23696594350699438 theta= [[-13.47644743]\n",
      " [  0.11299357]\n",
      " [  0.10703211]]\n",
      "the iterations of  698100 the loss is 0.23696023861061755 theta= [[-13.47720272]\n",
      " [  0.11299957]\n",
      " [  0.10703818]]\n",
      "the iterations of  698200 the loss is 0.23695453520316304 theta= [[-13.47795791]\n",
      " [  0.11300557]\n",
      " [  0.10704425]]\n",
      "the iterations of  698300 the loss is 0.23694883328406535 theta= [[-13.47871301]\n",
      " [  0.11301157]\n",
      " [  0.10705032]]\n",
      "the iterations of  698400 the loss is 0.23694313285275925 theta= [[-13.479468  ]\n",
      " [  0.11301757]\n",
      " [  0.10705639]]\n",
      "the iterations of  698500 the loss is 0.23693743390867997 theta= [[-13.4802229 ]\n",
      " [  0.11302357]\n",
      " [  0.10706246]]\n",
      "the iterations of  698600 the loss is 0.23693173645126278 theta= [[-13.4809777 ]\n",
      " [  0.11302957]\n",
      " [  0.10706852]]\n",
      "the iterations of  698700 the loss is 0.2369260404799435 theta= [[-13.48173239]\n",
      " [  0.11303557]\n",
      " [  0.10707459]]\n",
      "the iterations of  698800 the loss is 0.23692034599415798 theta= [[-13.482487  ]\n",
      " [  0.11304157]\n",
      " [  0.10708065]]\n",
      "the iterations of  698900 the loss is 0.2369146529933426 theta= [[-13.4832415 ]\n",
      " [  0.11304756]\n",
      " [  0.10708671]]\n",
      "the iterations of  699000 the loss is 0.2369089614769338 theta= [[-13.4839959 ]\n",
      " [  0.11305356]\n",
      " [  0.10709278]]\n",
      "the iterations of  699100 the loss is 0.23690327144436843 theta= [[-13.48475021]\n",
      " [  0.11305955]\n",
      " [  0.10709884]]\n",
      "the iterations of  699200 the loss is 0.23689758289508364 theta= [[-13.48550442]\n",
      " [  0.11306555]\n",
      " [  0.1071049 ]]\n",
      "the iterations of  699300 the loss is 0.23689189582851686 theta= [[-13.48625852]\n",
      " [  0.11307154]\n",
      " [  0.10711096]]\n",
      "the iterations of  699400 the loss is 0.23688621024410583 theta= [[-13.48701253]\n",
      " [  0.11307753]\n",
      " [  0.10711702]]\n",
      "the iterations of  699500 the loss is 0.2368805261412885 theta= [[-13.48776645]\n",
      " [  0.11308352]\n",
      " [  0.10712308]]\n",
      "the iterations of  699600 the loss is 0.23687484351950297 theta= [[-13.48852026]\n",
      " [  0.11308951]\n",
      " [  0.10712914]]\n",
      "the iterations of  699700 the loss is 0.23686916237818803 theta= [[-13.48927398]\n",
      " [  0.1130955 ]\n",
      " [  0.10713519]]\n",
      "the iterations of  699800 the loss is 0.23686348271678237 theta= [[-13.4900276 ]\n",
      " [  0.11310149]\n",
      " [  0.10714125]]\n",
      "the iterations of  699900 the loss is 0.23685780453472527 theta= [[-13.49078112]\n",
      " [  0.11310748]\n",
      " [  0.10714731]]\n",
      "the iterations of  700000 the loss is 0.23685212783145615 theta= [[-13.49153454]\n",
      " [  0.11311347]\n",
      " [  0.10715336]]\n",
      "the iterations of  700100 the loss is 0.2368464526064145 theta= [[-13.49228786]\n",
      " [  0.11311946]\n",
      " [  0.10715942]]\n",
      "the iterations of  700200 the loss is 0.2368407788590404 theta= [[-13.49304109]\n",
      " [  0.11312544]\n",
      " [  0.10716547]]\n",
      "the iterations of  700300 the loss is 0.2368351065887741 theta= [[-13.49379421]\n",
      " [  0.11313143]\n",
      " [  0.10717152]]\n",
      "the iterations of  700400 the loss is 0.23682943579505636 theta= [[-13.49454724]\n",
      " [  0.11313741]\n",
      " [  0.10717757]]\n",
      "the iterations of  700500 the loss is 0.23682376647732784 theta= [[-13.49530017]\n",
      " [  0.1131434 ]\n",
      " [  0.10718363]]\n",
      "the iterations of  700600 the loss is 0.23681809863502973 theta= [[-13.49605301]\n",
      " [  0.11314938]\n",
      " [  0.10718968]]\n",
      "the iterations of  700700 the loss is 0.23681243226760348 theta= [[-13.49680574]\n",
      " [  0.11315536]\n",
      " [  0.10719573]]\n",
      "the iterations of  700800 the loss is 0.2368067673744908 theta= [[-13.49755838]\n",
      " [  0.11316135]\n",
      " [  0.10720177]]\n",
      "the iterations of  700900 the loss is 0.23680110395513349 theta= [[-13.49831092]\n",
      " [  0.11316733]\n",
      " [  0.10720782]]\n",
      "the iterations of  701000 the loss is 0.23679544200897407 theta= [[-13.49906336]\n",
      " [  0.11317331]\n",
      " [  0.10721387]]\n",
      "the iterations of  701100 the loss is 0.23678978153545482 theta= [[-13.49981571]\n",
      " [  0.11317929]\n",
      " [  0.10721992]]\n",
      "the iterations of  701200 the loss is 0.23678412253401884 theta= [[-13.50056795]\n",
      " [  0.11318526]\n",
      " [  0.10722596]]\n",
      "the iterations of  701300 the loss is 0.23677846500410907 theta= [[-13.5013201 ]\n",
      " [  0.11319124]\n",
      " [  0.10723201]]\n",
      "the iterations of  701400 the loss is 0.23677280894516908 theta= [[-13.50207215]\n",
      " [  0.11319722]\n",
      " [  0.10723805]]\n",
      "the iterations of  701500 the loss is 0.23676715435664236 theta= [[-13.5028241 ]\n",
      " [  0.11320319]\n",
      " [  0.10724409]]\n",
      "the iterations of  701600 the loss is 0.23676150123797296 theta= [[-13.50357596]\n",
      " [  0.11320917]\n",
      " [  0.10725014]]\n",
      "the iterations of  701700 the loss is 0.23675584958860516 theta= [[-13.50432772]\n",
      " [  0.11321514]\n",
      " [  0.10725618]]\n",
      "the iterations of  701800 the loss is 0.23675019940798347 theta= [[-13.50507938]\n",
      " [  0.11322112]\n",
      " [  0.10726222]]\n",
      "the iterations of  701900 the loss is 0.23674455069555272 theta= [[-13.50583094]\n",
      " [  0.11322709]\n",
      " [  0.10726826]]\n",
      "the iterations of  702000 the loss is 0.23673890345075804 theta= [[-13.5065824 ]\n",
      " [  0.11323306]\n",
      " [  0.1072743 ]]\n",
      "the iterations of  702100 the loss is 0.23673325767304473 theta= [[-13.50733377]\n",
      " [  0.11323904]\n",
      " [  0.10728034]]\n",
      "the iterations of  702200 the loss is 0.23672761336185846 theta= [[-13.50808504]\n",
      " [  0.11324501]\n",
      " [  0.10728638]]\n",
      "the iterations of  702300 the loss is 0.23672197051664526 theta= [[-13.50883621]\n",
      " [  0.11325098]\n",
      " [  0.10729241]]\n",
      "the iterations of  702400 the loss is 0.23671632913685126 theta= [[-13.50958728]\n",
      " [  0.11325695]\n",
      " [  0.10729845]]\n",
      "the iterations of  702500 the loss is 0.23671068922192312 theta= [[-13.51033826]\n",
      " [  0.11326291]\n",
      " [  0.10730448]]\n",
      "the iterations of  702600 the loss is 0.2367050507713074 theta= [[-13.51108914]\n",
      " [  0.11326888]\n",
      " [  0.10731052]]\n"
     ]
    }
   ],
   "source": [
    "def predict(X):\n",
    "    # get the result\n",
    "    h = hypothesis(X,theta)\n",
    "    #classify according to the result\n",
    "    if(h>=0.5):\n",
    "        h = 1\n",
    "    else:\n",
    "        h = 0\n",
    "    return h\n",
    "\n",
    "c = np.ones(X.shape[0]).transpose()\n",
    "X = np.insert(X, 0, values=c, axis=1)\n",
    "n = X.shape[1]   # the number of features\n",
    "num = X.shape[0]\n",
    "theta = np.zeros(n).reshape(-1,1)\n",
    "y = y.reshape(-1,1)   \n",
    "\n",
    "theta_temp = np.zeros(n).reshape(-1,1)\n",
    "\n",
    "iterations = 1000000\n",
    "alpha = 0.00001\n",
    "\n",
    "theta, loss= gradientDescent(X,y,theta,iterations,alpha)\n",
    "print('theta=\\n',theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "28f31e57-4b14-463d-a61c-3f04c9985635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#def plotDescisionBoundary(X,y,theta):\n",
    "cm_dark = mpl.colors.ListedColormap(['g', 'r'])\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.scatter(data[:,0],data[:,1],c=np.array(y).squeeze(),cmap=cm_dark,s=30)\n",
    "#补充画决策边界代码；\n",
    "x1 = np.linspace(np.min(data[:, 0]), np.max(data[:, 0]), 100)\n",
    "x2 = -(theta[0] + theta[1]*x1)/theta[2]\n",
    "plt.plot(x1,x2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "addd6020-e7e1-4777-90fc-f573de4d0f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0, iterations), loss)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.yscale('log')  # 使用对数刻度更清晰显示下降趋势\n",
    "plt.title('Loss During Gradient Descent')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7f213ae7-df36-49c7-a0a9-4934e8381619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_p = [1,34.62365962451697, 43.89499752400101]\n",
    "predict(X_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6cd66e-ad71-489f-b37b-678b5b9de82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
